{"fragment_id": "F_R1_1_6", "source_id": "R1", "locator": "tradingresource.md:L1-L6", "text": "# Awesome Quant\n\nA curated list of insanely awesome libraries, packages and resources for Quants (Quantitative Finance).\n\n[![](https://awesome.re/badge.svg)](https://awesome.re)", "tags": []}
{"fragment_id": "F_R1_7_25", "source_id": "R1", "locator": "tradingresource.md:L7-L25", "text": "## Languages\n\n- [Python](#python)\n- [R](#r)\n- [Matlab](#matlab)\n- [Julia](#julia)\n- [Java](#java)\n- [JavaScript](#javascript)\n- [Haskell](#haskell)\n- [Scala](#scala)\n- [Ruby](#ruby)\n- [Elixir/Erlang](#elixirerlang)\n- [Golang](#golang)\n- [CPP](#cpp)\n- [CSharp](#csharp)\n- [Rust](#rust)\n- [Frameworks](#frameworks)\n- [Reproducing Works, Training & Books](#reproducing-works-training--books)", "tags": []}
{"fragment_id": "F_R1_26_27", "source_id": "R1", "locator": "tradingresource.md:L26-L27", "text": "## Python", "tags": []}
{"fragment_id": "F_R1_28_40", "source_id": "R1", "locator": "tradingresource.md:L28-L40", "text": "### Numerical Libraries & Data Structures\n\n- [numpy](https://www.numpy.org) - NumPy is the fundamental package for scientific computing with Python.\n- [scipy](https://www.scipy.org) - SciPy (pronounced “Sigh Pie”) is a Python-based ecosystem of open-source software for mathematics, science, and engineering.\n- [pandas](https://pandas.pydata.org) - pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n- [polars](https://docs.pola.rs/) - Polars is a blazingly fast DataFrame library for manipulating structured data.\n- [quantdsl](https://github.com/johnbywater/quantdsl) - Domain specific language for quantitative analytics in finance and trading.\n- [statistics](https://docs.python.org/3/library/statistics.html) - Builtin Python library for all basic statistical calculations.\n- [sympy](https://www.sympy.org/) - SymPy is a Python library for symbolic mathematics.\n- [pymc3](https://docs.pymc.io/) - Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano.\n- [modelx](https://docs.modelx.io/) - Python reimagination of spreadsheets as formula-centric objects that are interoperable with pandas.\n- [ArcticDB](https://github.com/man-group/ArcticDB) - High performance datastore for time series and tick data.", "tags": []}
{"fragment_id": "F_R1_41_55", "source_id": "R1", "locator": "tradingresource.md:L41-L55", "text": "### Financial Instruments and Pricing\n\n- [OpenBB Terminal](https://github.com/OpenBB-finance/OpenBBTerminal) - Terminal for investment research for everyone.\n- [Fincept Terminal](https://github.com/Fincept-Corporation/FinceptTerminal) - Advance Data Based A.I Terminal for all Types of Financial Asset Research.\n- [PyQL](https://github.com/enthought/pyql) - QuantLib's Python port.\n- [pyfin](https://github.com/opendoor-labs/pyfin) - Basic options pricing in Python. *ARCHIVED*\n- [vollib](https://github.com/vollib/vollib) - vollib is a python library for calculating option prices, implied volatility and greeks.\n- [QuantPy](https://github.com/jsmidt/QuantPy) - A framework for quantitative finance In python.\n- [Finance-Python](https://github.com/alpha-miner/Finance-Python) - Python tools for Finance.\n- [ffn](https://github.com/pmorissette/ffn) - A financial function library for Python.\n- [pynance](https://github.com/GriffinAustin/pynance) - Lightweight Python library for assembling and analyzing financial data.\n- [tia](https://github.com/bpsmith/tia) - Toolkit for integration and analysis.\n- [hasura/base-python-dash](https://platform.hasura.io/hub/projects/hasura/base-python-dash) - Hasura quick start to deploy Dash framework. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python.\n- [hasura/base-python-bokeh](https://platform.hasura.io/hub/projects/hasura/base-python-bokeh) - Hasura quick start to visualize data with bokeh library.\n- [pysabr](https://github.com/ynouri/pysabr) - SABR model Python implementation.", "tags": []}
{"fragment_id": "F_R1_56_66", "source_id": "R1", "locator": "tradingresource.md:L56-L66", "text": "- [FinancePy](https://github.com/domokane/FinancePy) - A Python Finance Library that focuses on the pricing and risk-management of Financial Derivatives, including fixed-income, equity, FX and credit derivatives.\n- [gs-quant](https://github.com/goldmansachs/gs-quant) - Python toolkit for quantitative finance\n- [willowtree](https://github.com/federicomariamassari/willowtree) - Robust and flexible Python implementation of the willow tree lattice for derivatives pricing.\n- [financial-engineering](https://github.com/federicomariamassari/financial-engineering) - Applications of Monte Carlo methods to financial engineering projects, in Python.\n- [optlib](https://github.com/dbrojas/optlib) - A library for financial options pricing written in Python.\n- [tf-quant-finance](https://github.com/google/tf-quant-finance) - High-performance TensorFlow library for quantitative finance.\n- [Q-Fin](https://github.com/RomanMichaelPaolucci/Q-Fin) - A Python library for mathematical finance.\n- [Quantsbin](https://github.com/quantsbin/Quantsbin) - Tools for pricing and plotting of vanilla option prices, greeks and various other analysis around them.\n- [finoptions](https://github.com/bbcho/finoptions-dev) - Complete python implementation of R package fOptions with partial implementation of fExoticOptions for pricing various options.\n- [pypme](https://github.com/ymyke/pypme) - PME (Public Market Equivalent) calculation.\n- [AbsBox](https://github.com/yellowbean/AbsBox) - A Python based library to model cashflow for structured product like Asset-backed securities (ABS) and Mortgage-backed securities (MBS).", "tags": []}
{"fragment_id": "F_R1_67_71", "source_id": "R1", "locator": "tradingresource.md:L67-L71", "text": "- [Intrinsic-Value-Calculator](https://github.com/akashaero/Intrinsic-Value-Calculator) - A Python tool for quick calculations of a stock's fair value using Discounted Cash Flow analysis.\n- [Kelly-Criterion](https://github.com/deltaray-io/kelly-criterion) - Kelly Criterion implemented in Python to size portfolios based on J. L. Kelly Jr's formula.\n- [rateslib](https://github.com/attack68/rateslib) - A fixed income library for pricing bonds and bond futures, and derivatives such as IRS, cross-currency and FX swaps.\n- [fypy](https://github.com/jkirkby3/fypy) - Vanilla and exotic option pricing library to support quantitative R&D. Focus on pricing interesting/useful models and contracts (including and beyond Black-Scholes), as well as calibration of financial models to market data.", "tags": []}
{"fragment_id": "F_R1_72_80", "source_id": "R1", "locator": "tradingresource.md:L72-L80", "text": "### Indicators\n\n- [pandas_talib](https://github.com/femtotrader/pandas_talib) - A Python Pandas implementation of technical analysis indicators.\n- [finta](https://github.com/peerchemist/finta) - Common financial technical analysis indicators implemented in Pandas.\n- [Tulipy](https://github.com/cirla/tulipy) - Financial Technical Analysis Indicator Library (Python bindings for [tulipindicators](https://github.com/TulipCharts/tulipindicators))\n- [lppls](https://github.com/Boulder-Investment-Technologies/lppls) - A Python module for fitting the [Log-Periodic Power Law Singularity (LPPLS)](https://en.wikipedia.org/wiki/Didier_Sornette#The_JLS_and_LPPLS_models) model.\n- [talipp](https://github.com/nardew/talipp) - Incremental technical analysis library for Python.\n- [streaming_indicators](https://github.com/mr-easy/streaming_indicators) - A python library for computing technical analysis indicators on streaming data.", "tags": []}
{"fragment_id": "F_R1_81_94", "source_id": "R1", "locator": "tradingresource.md:L81-L94", "text": "### Trading & Backtesting\n- [skfolio](https://github.com/skfolio/skfolio) - Python library for portfolio optimization built on top of scikit-learn. It provides a unified interface and sklearn compatible tools to build, tune and cross-validate portfolio models.\n- [Investing algorithm framework](https://github.com/coding-kitties/investing-algorithm-framework) - Framework for developing, backtesting, and deploying automated trading algorithms.\n- [QSTrader](https://github.com/mhallsmoore/qstrader) - QSTrader backtesting simulation engine.\n- [Blankly](https://github.com/Blankly-Finance/Blankly) - Fully integrated backtesting, paper trading, and live deployment.\n- [TA-Lib](https://github.com/mrjbq7/ta-lib) - Python wrapper for TA-Lib (<http://ta-lib.org/>).\n- [zipline](https://github.com/quantopian/zipline) - Pythonic algorithmic trading library.\n- [zipline-reloaded](https://github.com/stefan-jansen/zipline-reloaded) - Zipline, a Pythonic Algorithmic Trading Library.\n- [QuantSoftware Toolkit](https://github.com/QuantSoftware/QuantSoftwareToolkit) - Python-based open source software framework designed to support portfolio construction and management.\n- [quantitative](https://github.com/jeffrey-liang/quantitative) - Quantitative finance, and backtesting library.\n- [analyzer](https://github.com/llazzaro/analyzer) - Python framework for real-time financial and backtesting trading strategies.\n- [bt](https://github.com/pmorissette/bt) - Flexible Backtesting for Python.\n- [backtrader](https://github.com/backtrader/backtrader) - Python Backtesting library for trading strategies.\n- [pythalesians](https://github.com/thalesians/pythalesians) - Python library to backtest trading strategies, plot charts, seamlessly download market data, analyze market patterns etc.", "tags": []}
{"fragment_id": "F_R1_95_106", "source_id": "R1", "locator": "tradingresource.md:L95-L106", "text": "- [pybacktest](https://github.com/ematvey/pybacktest) - Vectorized backtesting framework in Python / pandas, designed to make your backtesting easier.\n- [pyalgotrade](https://github.com/gbeced/pyalgotrade) - Python Algorithmic Trading Library.\n- [basana](https://github.com/gbeced/basana) - A Python async and event driven framework for algorithmic trading, with a focus on crypto currencies.\n- [tradingWithPython](https://pypi.org/project/tradingWithPython/) - A collection of functions and classes for Quantitative trading.\n- [Pandas TA](https://github.com/twopirllc/pandas-ta) - Pandas TA is an easy to use Python 3 Pandas Extension with 115+ Indicators. Easily build Custom Strategies.\n- [ta](https://github.com/bukosabino/ta) - Technical Analysis Library using Pandas (Python)\n- [algobroker](https://github.com/joequant/algobroker) - This is an execution engine for algo trading.\n- [pysentosa](https://pypi.org/project/pysentosa/) - Python API for sentosa trading system.\n- [finmarketpy](https://github.com/cuemacro/finmarketpy) - Python library for backtesting trading strategies and analyzing financial markets.\n- [binary-martingale](https://github.com/metaperl/binary-martingale) - Computer program to automatically trade binary options martingale style.\n- [fooltrader](https://github.com/foolcage/fooltrader) - the project using big-data technology to provide an uniform way to analyze the whole market.\n- [zvt](https://github.com/zvtvz/zvt) - the project using sql, pandas to provide an uniform and extendable way to record data, computing factors, select securities, backtesting, realtime trading and it could show all of them in clearly charts in realtime.", "tags": []}
{"fragment_id": "F_R1_107_116", "source_id": "R1", "locator": "tradingresource.md:L107-L116", "text": "- [pylivetrader](https://github.com/alpacahq/pylivetrader) - zipline-compatible live trading library.\n- [pipeline-live](https://github.com/alpacahq/pipeline-live) - zipline's pipeline capability with IEX for live trading.\n- [zipline-extensions](https://github.com/quantrocket-llc/zipline-extensions) - Zipline extensions and adapters for QuantRocket.\n- [moonshot](https://github.com/quantrocket-llc/moonshot) - Vectorized backtester and trading engine for QuantRocket based on Pandas.\n- [PyPortfolioOpt](https://github.com/robertmartin8/PyPortfolioOpt) - Financial portfolio optimization in python, including classical efficient frontier and advanced methods.\n- [Eiten](https://github.com/tradytics/eiten) - Eiten is an open source toolkit by Tradytics that implements various statistical and algorithmic investing strategies such as Eigen Portfolios, Minimum Variance Portfolios, Maximum Sharpe Ratio Portfolios, and Genetic Algorithms based Portfolios.\n- [riskparity.py](https://github.com/dppalomar/riskparity.py) - fast and scalable design of risk parity portfolios with TensorFlow 2.0\n- [mlfinlab](https://github.com/hudson-and-thames/mlfinlab) - Implementations regarding \"Advances in Financial Machine Learning\" by Marcos Lopez de Prado. (Feature Engineering, Financial Data Structures, Meta-Labeling)\n- [pyqstrat](https://github.com/abbass2/pyqstrat) - A fast, extensible, transparent python library for backtesting quantitative strategies.\n- [NowTrade](https://github.com/edouardpoitras/NowTrade) - Python library for backtesting technical/mechanical strategies in the stock and currency markets.", "tags": []}
{"fragment_id": "F_R1_117_128", "source_id": "R1", "locator": "tradingresource.md:L117-L128", "text": "- [pinkfish](https://github.com/fja05680/pinkfish) - A backtester and spreadsheet library for security analysis.\n- [aat](https://github.com/timkpaine/aat) - Async Algorithmic Trading Engine\n- [Backtesting.py](https://kernc.github.io/backtesting.py/) - Backtest trading strategies in Python\n- [catalyst](https://github.com/enigmampc/catalyst) - An Algorithmic Trading Library for Crypto-Assets in Python\n- [quantstats](https://github.com/ranaroussi/quantstats) - Portfolio analytics for quants, written in Python\n- [qtpylib](https://github.com/ranaroussi/qtpylib) - QTPyLib, Pythonic Algorithmic Trading <http://qtpylib.io>\n- [Quantdom](https://github.com/constverum/Quantdom) - Python-based framework for backtesting trading strategies & analyzing financial markets [GUI :neckbeard:]\n- [freqtrade](https://github.com/freqtrade/freqtrade) - Free, open source crypto trading bot\n- [algorithmic-trading-with-python](https://github.com/chrisconlan/algorithmic-trading-with-python) - Free `pandas` and `scikit-learn` resources for trading simulation, backtesting, and machine learning on financial data.\n- [DeepDow](https://github.com/jankrepl/deepdow) - Portfolio optimization with deep learning\n- [Qlib](https://github.com/microsoft/qlib) - An AI-oriented Quantitative Investment Platform by Microsoft. Full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution.\n- [machine-learning-for-trading](https://github.com/stefan-jansen/machine-learning-for-trading) - Code and resources for Machine Learning for Algorithmic Trading", "tags": []}
{"fragment_id": "F_R1_129_139", "source_id": "R1", "locator": "tradingresource.md:L129-L139", "text": "- [AlphaPy](https://github.com/ScottfreeLLC/AlphaPy) - Automated Machine Learning [AutoML] with Python, scikit-learn, Keras, XGBoost, LightGBM, and CatBoost\n- [jesse](https://github.com/jesse-ai/jesse) - An advanced crypto trading bot written in Python\n- [rqalpha](https://github.com/ricequant/rqalpha) - A extendable, replaceable Python algorithmic backtest && trading framework supporting multiple securities.\n- [FinRL-Library](https://github.com/AI4Finance-LLC/FinRL-Library) - A Deep Reinforcement Learning Library for Automated Trading in Quantitative Finance. NeurIPS 2020.\n- [bulbea](https://github.com/achillesrasquinha/bulbea) - Deep Learning based Python Library for Stock Market Prediction and Modelling.\n- [ib_nope](https://github.com/ajhpark/ib_nope) - Automated trading system for NOPE strategy over IBKR TWS.\n- [OctoBot](https://github.com/Drakkar-Software/OctoBot) - Open source cryptocurrency trading bot for high frequency, arbitrage, TA and social trading with an advanced web interface.\n- [bta-lib](https://github.com/mementum/bta-lib) - Technical Analysis library in pandas for backtesting algotrading and quantitative analysis.\n- [Stock-Prediction-Models](https://github.com/huseinzol05/Stock-Prediction-Models) - Gathers machine learning and deep learning models for Stock forecasting including trading bots and simulations.\n- [TuneTA](https://github.com/jmrichardson/tuneta) - TuneTA optimizes technical indicators using a distance correlation measure to a user defined target feature such as next day return.\n- [AutoTrader](https://github.com/kieran-mackle/AutoTrader) - A Python-based development platform for automated trading systems - from backtesting to optimization to livetrading.", "tags": []}
{"fragment_id": "F_R1_140_150", "source_id": "R1", "locator": "tradingresource.md:L140-L150", "text": "- [fast-trade](https://github.com/jrmeier/fast-trade) - A library built with backtest portability and performance in mind for backtest trading strategies.\n- [qf-lib](https://github.com/quarkfin/qf-lib) - QF-Lib is a Python library that provides high quality tools for quantitative finance.\n- [tda-api](https://github.com/alexgolec/tda-api) - Gather data and trade equities, options, and ETFs via TDAmeritrade.\n- [vectorbt](https://github.com/polakowo/vectorbt) - Find your trading edge, using a powerful toolkit for backtesting, algorithmic trading, and research.\n- [Lean](https://github.com/QuantConnect/Lean) - Lean Algorithmic Trading Engine by QuantConnect (Python, C#).\n- [fast-trade](https://github.com/jrmeier/fast-trade) - Low code backtesting library utilizing pandas and technical analysis indicators.\n- [pysystemtrade](https://github.com/robcarver17/pysystemtrade) - pysystemtrade is the open source version of Robert Carver's backtesting and trading engine that implements systems according to the framework outlined in his book \"Systematic Trading\", which is further developed on his [blog](https://qoppac.blogspot.com/).\n- [pytrendseries](https://github.com/rafa-rod/pytrendseries) - Detect trend in time series, drawdown, drawdown within a constant look-back window , maximum drawdown, time underwater.\n- [PyLOB](https://github.com/DrAshBooth/PyLOB) - Fully functioning fast Limit Order Book written in Python.\n- [PyBroker](https://github.com/edtechre/pybroker) - Algorithmic Trading with Machine Learning.\n- [OctoBot Script](https://github.com/Drakkar-Software/OctoBot-Script) - A quant framework to create cryptocurrencies strategies - from backtesting to optimization to livetrading.", "tags": []}
{"fragment_id": "F_R1_151_160", "source_id": "R1", "locator": "tradingresource.md:L151-L160", "text": "- [hftbacktest](https://github.com/nkaz001/hftbacktest) - A high-frequency trading and market-making backtesting tool accounts for limit orders, queue positions, and latencies, utilizing full tick data for trades and order books.\n- [vnpy](https://github.com/vnpy/vnpy) - VeighNa is a Python-based open source quantitative trading system development framework.\n- [Intelligent Trading Bot](https://github.com/asavinov/intelligent-trading-bot) - Automatically generating signals and trading based on machine learning and feature engineering\n- [fastquant](https://github.com/enzoampil/fastquant) - fastquant allows you to easily backtest investment strategies with as few as 3 lines of python code.\n- [nautilus_trader](https://github.com/nautechsystems/nautilus_trader) - A high-performance algorithmic trading platform and event-driven backtester.\n- [YABTE](https://github.com/bsdz/yabte) - Yet Another (Python) BackTesting Engine.\n- [Trading Strategy](https://github.com/tradingstrategy-ai/getting-started) - TradingStrategy.ai is a market data, backtesting, live trading and investor management framework for decentralised finance\n- [Hikyuu](https://github.com/fasiondog/hikyuu) - A base on Python/C++ open source high-performance quant framework for faster analysis and backtesting, contains the complete trading system components for reuse and combination.\n- [rust_bt](https://github.com/jensnesten/rust_bt) - A high performance, low-latency backtesting engine for testing quantitative trading strategies on historical and live data in Rust.\n- [Gunbot Quant](https://github.com/GuntharDeNiro/gunbot-quant) - Toolkit for quantitative trading analysis. It integrates an advanced market screener, a multi-strategy, multi-asset backtesting engine. Use with built-in GUI or through CLI.", "tags": []}
{"fragment_id": "F_R1_161_162", "source_id": "R1", "locator": "tradingresource.md:L161-L162", "text": "- [StrateQueue](https://github.com/StrateQueue/StrateQueue) - An open‑source, broker‑agnostic Python library that lets you seamlessly deploy strategies from any major backtesting engine to live (or paper) trading with zero code changes and built‑in safety controls.", "tags": []}
{"fragment_id": "F_R1_163_177", "source_id": "R1", "locator": "tradingresource.md:L163-L177", "text": "### Risk Analysis\n\n- [QuantLibRisks](https://github.com/auto-differentiation/QuantLib-Risks-Py) - Fast risks with QuantLib\n- [XAD](https://github.com/auto-differentiation/xad-py) - Automatic Differentation (AAD) Library\n- [pyfolio](https://github.com/quantopian/pyfolio) - Portfolio and risk analytics in Python.\n- [empyrical](https://github.com/quantopian/empyrical) - Common financial risk and performance metrics.\n- [fecon235](https://github.com/rsvp/fecon235) - Computational tools for financial economics include: Gaussian Mixture model of leptokurtotic risk, adaptive Boltzmann portfolios.\n- [finance](https://pypi.org/project/finance/) - Financial Risk Calculations. Optimized for ease of use through class construction and operator overload.\n- [qfrm](https://pypi.org/project/qfrm/) - Quantitative Financial Risk Management: awesome OOP tools for measuring, managing and visualizing risk of financial instruments and portfolios.\n- [visualize-wealth](https://github.com/benjaminmgross/visualize-wealth) - Portfolio construction and quantitative analysis.\n- [VisualPortfolio](https://github.com/wegamekinglc/VisualPortfolio) - This tool is used to visualize the performance of a portfolio.\n- [universal-portfolios](https://github.com/Marigold/universal-portfolios) - Collection of algorithms for online portfolio selection.\n- [FinQuant](https://github.com/fmilthaler/FinQuant) - A program for financial portfolio management, analysis and optimization.\n- [Empyrial](https://github.com/ssantoshp/Empyrial) - Portfolio's risk and performance analytics and returns predictions.\n- [risktools](https://github.com/bbcho/risktools-dev) - Risk tools for use within the crude and crude products trading space with partial implementation of R's PerformanceAnalytics.", "tags": []}
{"fragment_id": "F_R1_178_184", "source_id": "R1", "locator": "tradingresource.md:L178-L184", "text": "- [Riskfolio-Lib](https://github.com/dcajasn/Riskfolio-Lib) - Portfolio Optimization and Quantitative Strategic Asset Allocation in Python.\n- [empyrical-reloaded](https://github.com/stefan-jansen/empyrical-reloaded) - Common financial risk and performance metrics. [empyrical](https://github.com/quantopian/empyrical) fork.\n- [pyfolio-reloaded](https://github.com/stefan-jansen/pyfolio-reloaded) - Portfolio and risk analytics in Python. [pyfolio](https://github.com/quantopian/pyfolio) fork.\n- [fortitudo.tech](https://github.com/fortitudo-tech/fortitudo.tech) - Conditional Value-at-Risk (CVaR) portfolio optimization and Entropy Pooling views / stress-testing in Python.\n- [Quant Lab Alpha](https://github.com/husainm97/quant-lab-alpha) — Portfolio risk decomposition and Monte Carlo simulation toolkit with factor-based modeling.\n- [quantitative-finance-tools](https://github.com/omichauhan-lgtm/quantitative-finance-tools) - Library for portfolio optimization (MVO) and rigorous risk metrics (VaR/CVaR).", "tags": []}
{"fragment_id": "F_R1_185_190", "source_id": "R1", "locator": "tradingresource.md:L185-L190", "text": "### Factor Analysis\n\n- [alphalens](https://github.com/quantopian/alphalens) - Performance analysis of predictive alpha factors.\n- [alphalens-reloaded](https://github.com/stefan-jansen/alphalens-reloaded) - Performance analysis of predictive (alpha) stock factors.\n- [Spectre](https://github.com/Heerozh/spectre) - GPU-accelerated Factors analysis library and Backtester", "tags": []}
{"fragment_id": "F_R1_191_193", "source_id": "R1", "locator": "tradingresource.md:L191-L193", "text": "### Sentiment Analysis\n- [Asset News Sentiment Analyzer](https://github.com/KVignesh122/AssetNewsSentimentAnalyzer) - Sentiment analysis and report generation package for financial assets and securities utilizing GPT models.", "tags": []}
{"fragment_id": "F_R1_194_197", "source_id": "R1", "locator": "tradingresource.md:L194-L197", "text": "### Quant Research Environment\n\n- [Jupyter Quant](https://github.com/gnzsnz/jupyter-quant) - A dockerized Jupyter quant research environment with preloaded tools for quant analysis, statsmodels, pymc, arch, py_vollib, zipline-reloaded, PyPortfolioOpt, etc.", "tags": []}
{"fragment_id": "F_R1_198_210", "source_id": "R1", "locator": "tradingresource.md:L198-L210", "text": "### Time Series\n\n- [ARCH](https://github.com/bashtage/arch) - ARCH models in Python.\n- [statsmodels](http://statsmodels.sourceforge.net) - Python module that allows users to explore data, estimate statistical models, and perform statistical tests.\n- [dynts](https://github.com/quantmind/dynts) - Python package for timeseries analysis and manipulation.\n- [PyFlux](https://github.com/RJT1990/pyflux) - Python library for timeseries modelling and inference (frequentist and Bayesian) on models.\n- [tsfresh](https://github.com/blue-yonder/tsfresh) - Automatic extraction of relevant features from time series.\n- [hasura/quandl-metabase](https://platform.hasura.io/hub/projects/anirudhm/quandl-metabase-time-series) - Hasura quickstart to visualize Quandl's timeseries datasets with Metabase.\n- [Facebook Prophet](https://github.com/facebook/prophet) - Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non-linear growth.\n- [tsmoothie](https://github.com/cerlymarco/tsmoothie) - A python library for time-series smoothing and outlier detection in a vectorized way.\n- [pmdarima](https://github.com/alkaline-ml/pmdarima) - A statistical library designed to fill the void in Python's time series analysis capabilities, including the equivalent of R's auto.arima function.\n- [gluon-ts](https://github.com/awslabs/gluon-ts) - vProbabilistic time series modeling in Python.\n- [functime](https://github.com/functime-org/functime) - Time-series machine learning at scale. Built with Polars for embarrassingly parallel feature extraction and forecasts on panel data.", "tags": []}
{"fragment_id": "F_R1_211_211", "source_id": "R1", "locator": "tradingresource.md:L211-L211", "text": "", "tags": []}
{"fragment_id": "F_R1_212_217", "source_id": "R1", "locator": "tradingresource.md:L212-L217", "text": "### Calendars\n\n- [exchange_calendars](https://github.com/gerrymanoim/exchange_calendars) - Stock Exchange Trading Calendars.\n- [bizdays](https://github.com/wilsonfreitas/python-bizdays) - Business days calculations and utilities.\n- [pandas_market_calendars](https://github.com/rsheftel/pandas_market_calendars) - Exchange calendars to use with pandas for trading applications.", "tags": []}
{"fragment_id": "F_R1_218_229", "source_id": "R1", "locator": "tradingresource.md:L218-L229", "text": "### Data Sources\n- [StockAPI](https://stockapi.com.cn) – Free real-time Chinese stock data (REST & WebSocket).\n- [yfinance](https://github.com/ranaroussi/yfinance) - Yahoo! Finance market data downloader (+faster Pandas Datareader)\n- [defeatbeta-api](https://github.com/defeat-beta/defeatbeta-api) - An open-source alternative to Yahoo Finance's market data APIs with higher reliability.\n- [findatapy](https://github.com/cuemacro/findatapy) - Python library to download market data via Bloomberg, Quandl, Yahoo etc.\n- [googlefinance](https://github.com/hongtaocai/googlefinance) - Python module to get real-time stock data from Google Finance API.\n- [yahoo-finance](https://github.com/lukaszbanasiak/yahoo-finance) - Python module to get stock data from Yahoo! Finance.\n- [pandas-datareader](https://github.com/pydata/pandas-datareader) - Python module to get data from various sources (Google Finance, Yahoo Finance, FRED, OECD, Fama/French, World Bank, Eurostat...) into Pandas datastructures such as DataFrame, Panel with a caching mechanism.\n- [pandas-finance](https://github.com/davidastephens/pandas-finance) - High level API for access to and analysis of financial data.\n- [pyhoofinance](https://github.com/innes213/pyhoofinance) - Rapidly queries Yahoo Finance for multiple tickers and returns typed data for analysis.\n- [yfinanceapi](https://github.com/Karthik005/yfinanceapi) - Finance API for Python.\n- [yql-finance](https://github.com/slawek87/yql-finance) - yql-finance is simple and fast. API returns stock closing prices for current period of time and current stock ticker (i.e. APPL, GOOGL).", "tags": []}
{"fragment_id": "F_R1_230_246", "source_id": "R1", "locator": "tradingresource.md:L230-L246", "text": "- [ystockquote](https://github.com/cgoldberg/ystockquote) - Retrieve stock quote data from Yahoo Finance.\n- [wallstreet](https://github.com/mcdallas/wallstreet) - Real time stock and option data.\n- [stock_extractor](https://github.com/ZachLiuGIS/stock_extractor) - General Purpose Stock Extractors from Online Resources.\n- [Stockex](https://github.com/cttn/Stockex) - Python wrapper for Yahoo! Finance API.\n- [finsymbols](https://github.com/skillachie/finsymbols) - Obtains stock symbols and relating information for SP500, AMEX, NYSE, and NASDAQ.\n- [FRB](https://github.com/avelkoski/FRB) - Python Client for FRED® API.\n- [inquisitor](https://github.com/econdb/inquisitor) - Python Interface to Econdb.com API.\n- [yfi](https://github.com/nickelkr/yfi) - Yahoo! YQL library.\n- [chinesestockapi](https://pypi.org/project/chinesestockapi/) - Python API to get Chinese stock price.\n- [exchange](https://github.com/akarat/exchange) - Get current exchange rate.\n- [ticks](https://github.com/jamescnowell/ticks) - Simple command line tool to get stock ticker data.\n- [pybbg](https://github.com/bpsmith/pybbg) - Python interface to Bloomberg COM APIs.\n- [ccy](https://github.com/lsbardel/ccy) - Python module for currencies.\n- [tushare](https://pypi.org/project/tushare/) - A utility for crawling historical and Real-time Quotes data of China stocks.\n- [jsm](https://pypi.org/project/jsm/) - Get the japanese stock market data.\n- [cn_stock_src](https://github.com/jealous/cn_stock_src) - Utility for retrieving basic China stock data from different sources.\n- [coinmarketcap](https://github.com/barnumbirr/coinmarketcap) - Python API for coinmarketcap.", "tags": []}
{"fragment_id": "F_R1_247_257", "source_id": "R1", "locator": "tradingresource.md:L247-L257", "text": "- [after-hours](https://github.com/datawrestler/after-hours) - Obtain pre market and after hours stock prices for a given symbol.\n- [bronto-python](https://pypi.org/project/bronto-python/) - Bronto API Integration for Python.\n- [pytdx](https://github.com/rainx/pytdx) - Python Interface for retrieving chinese stock realtime quote data from TongDaXin Nodes.\n- [pdblp](https://github.com/matthewgilbert/pdblp) - A simple interface to integrate pandas and the Bloomberg Open API.\n- [tiingo](https://github.com/hydrosquall/tiingo-python) - Python interface for daily composite prices/OHLC/Volume + Real-time News Feeds, powered by the Tiingo Data Platform.\n- [iexfinance](https://github.com/addisonlynch/iexfinance) - Python Interface for retrieving real-time and historical prices and equities data from The Investor's Exchange.\n- [pyEX](https://github.com/timkpaine/pyEX) - Python interface to IEX with emphasis on pandas, support for streaming data, premium data, points data (economic, rates, commodities), and technical indicators.\n- [alpaca-trade-api](https://github.com/alpacahq/alpaca-trade-api-python) - Python interface for retrieving real-time and historical prices from Alpaca API as well as trade execution.\n- [metatrader5](https://pypi.org/project/MetaTrader5/) - API Connector to MetaTrader 5 Terminal\n- [akshare](https://github.com/jindaxiang/akshare) - AkShare is an elegant and simple financial data interface library for Python, built for human beings! <https://akshare.readthedocs.io>\n- [yahooquery](https://github.com/dpguthrie/yahooquery) - Python interface for retrieving data through unofficial Yahoo Finance API.", "tags": []}
{"fragment_id": "F_R1_258_269", "source_id": "R1", "locator": "tradingresource.md:L258-L269", "text": "- [investpy](https://github.com/alvarobartt/investpy) - Financial Data Extraction from Investing.com with Python! <https://investpy.readthedocs.io/>\n- [yliveticker](https://github.com/yahoofinancelive/yliveticker) - Live stream of market data from Yahoo Finance websocket.\n- [bbgbridge](https://github.com/ran404/bbgbridge) - Easy to use Bloomberg Desktop API wrapper for Python.\n- [polygon.io](https://github.com/polygon-io/client-python) - A python library for Polygon.io financial data APIs.\n- [alpha_vantage](https://github.com/RomelTorres/alpha_vantage) - A python wrapper for Alpha Vantage API for financial data.\n- [oilpriceapi](https://github.com/OilpriceAPI/python-sdk) - Python SDK for real-time oil and commodity prices (WTI, Brent, Urals, natural gas, coal) with OpenBB integration.\n- [FinanceDataReader](https://github.com/FinanceData/FinanceDataReader) - Open Source Financial data reader for U.S, Korean, Japanese, Chinese, Vietnamese Stocks\n- [pystlouisfed](https://github.com/TomasKoutek/pystlouisfed) - Python client for Federal Reserve Bank of St. Louis API - FRED, ALFRED, GeoFRED and FRASER.\n- [python-bcb](https://github.com/wilsonfreitas/python-bcb) - Python interface to Brazilian Central Bank web services.\n- [market-prices](https://github.com/maread99/market_prices) - Create meaningful OHLCV datasets from knowledge of [exchange-calendars](https://github.com/gerrymanoim/exchange_calendars) (works out-the-box with data from Yahoo Finance).\n- [tardis-python](https://github.com/tardis-dev/tardis-python) - Python interface for Tardis.dev high frequency crypto market data\n- [lake-api](https://github.com/crypto-lake/lake-api) - Python interface for Crypto Lake high frequency crypto market data", "tags": []}
{"fragment_id": "F_R1_270_278", "source_id": "R1", "locator": "tradingresource.md:L270-L278", "text": "- [tessa](https://github.com/ymyke/tessa) - simple, hassle-free access to price information of financial assets (currently based on yfinance and pycoingecko), including search and a symbol class.\n- [pandaSDMX](https://github.com/dr-leo/pandaSDMX) - Python package that implements SDMX 2.1 (ISO 17369:2013), a format for exchange of statistical data and metadata used by national statistical agencies, central banks, and international organisations.\n- [cif](https://github.com/LenkaV/CIF) - Python package that include few composite indicators, which summarize multidimensional relationships between individual economic indicators.\n- [finagg](https://github.com/theOGognf/finagg) - finagg is a Python package that provides implementations of popular and free financial APIs, tools for aggregating historical data from those APIs into SQL databases, and tools for transforming aggregated data into features useful for analysis and AI/ML.\n- [FinanceDatabase](https://github.com/JerBouma/FinanceDatabase) - This is a database of 300.000+ symbols containing Equities, ETFs, Funds, Indices, Currencies, Cryptocurrencies and Money Markets.\n- [Trading Strategy](https://github.com/tradingstrategy-ai/trading-strategy/) - download price data for decentralised exchanges and lending protocols (DeFi)\n- [datamule-python](https://github.com/john-friedman/datamule-python) - A package to work with SEC data. Incorporates datamule endpoints.\n- [Earnings Feed](https://earningsfeed.com/api) - Real-time SEC filings, insider trades, and institutional holdings API.\n- [Financial Data](https://financialdata.net/) - Stock Market and Financial Data API.", "tags": []}
{"fragment_id": "F_R1_279_283", "source_id": "R1", "locator": "tradingresource.md:L279-L283", "text": "- [SaxoOpenAPI](https://www.developer.saxo/) - Saxo Bank financial data API.\n- [fsynth](https://github.com/welcra/fsynth) - Python library for high-fidelity unlimited synthetic financial data generation using Heston Stochastic Volatility and Merton Jump Diffusion.\n- [fedfred](https://nikhilxsunder.github.io/fedfred/) - FRED & GeoFRED Economic data API with preprocessed dataframe output in pandas/geopandas, polars/polars_st, and dask dataframes/geodataframes.\n- [edgar-sec](https://nikhilxsunder.github.io/edgar-sec/) - EDGAR Financial data API with preprocessed dataclass outputs.", "tags": []}
{"fragment_id": "F_R1_284_295", "source_id": "R1", "locator": "tradingresource.md:L284-L295", "text": "### Excel Integration\n\n- [xlwings](https://www.xlwings.org/) - Make Excel fly with Python.\n- [openpyxl](https://openpyxl.readthedocs.io/en/latest/) - Read/Write Excel 2007 xlsx/xlsm files.\n- [xlrd](https://github.com/python-excel/xlrd) - Library for developers to extract data from Microsoft Excel spreadsheet files.\n- [xlsxwriter](https://xlsxwriter.readthedocs.io/) - Write files in the Excel 2007+ XLSX file format.\n- [xlwt](https://github.com/python-excel/xlwt) - Library to create spreadsheet files compatible with MS Excel 97/2000/XP/2003 XLS files, on any platform.\n- [DataNitro](https://datanitro.com/) - DataNitro also offers full-featured Python-Excel integration, including UDFs. Trial downloads are available, but users must purchase a license.\n- [xlloop](http://xlloop.sourceforge.net) - XLLoop is an open source framework for implementing Excel user-defined functions (UDFs) on a centralised server (a function server).\n- [expy](http://www.bnikolic.co.uk/expy/expy.html) - The ExPy add-in allows easy use of Python directly from within an Microsoft Excel spreadsheet, both to execute arbitrary code and to define new Excel functions.\n- [pyxll](https://www.pyxll.com) - PyXLL is an Excel add-in that enables you to extend Excel using nothing but Python code.", "tags": []}
{"fragment_id": "F_R1_296_304", "source_id": "R1", "locator": "tradingresource.md:L296-L304", "text": "### Visualization\n\n- [D-Tale](https://github.com/man-group/dtale) - Visualizer for pandas dataframes and xarray datasets.\n- [mplfinance](https://github.com/matplotlib/mplfinance) - matplotlib utilities for the visualization, and visual analysis, of financial data.\n- [finplot](https://github.com/highfestiva/finplot) - Performant and effortless finance plotting for Python.\n- [finvizfinance](https://github.com/lit26/finvizfinance) - Finviz analysis python library.\n- [market-analy](https://github.com/maread99/market_analy) - Analysis and interactive charting using [market-prices](https://github.com/maread99/market_prices) and bqplot.\n- [QuantInvestStrats](https://github.com/ArturSepp/QuantInvestStrats) - Quantitative Investment Strategies (QIS) package implements Python analytics for visualisation of financial data, performance reporting, analysis of quantitative strategies.", "tags": []}
{"fragment_id": "F_R1_305_306", "source_id": "R1", "locator": "tradingresource.md:L305-L306", "text": "## R", "tags": []}
{"fragment_id": "F_R1_307_317", "source_id": "R1", "locator": "tradingresource.md:L307-L317", "text": "### Numerical Libraries & Data Structures\n\n- [xts](https://github.com/joshuaulrich/xts) - eXtensible Time Series: Provide for uniform handling of R's different time-based data classes by extending zoo, maximizing native format information preservation and allowing for user level customization and extension, while simplifying cross-class interoperability.\n- [data.table](https://github.com/Rdatatable/data.table) - Extension of data.frame: Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns and a fast file reader (fread). Offers a natural and flexible syntax, for faster development.\n- [sparseEigen](https://github.com/dppalomar/sparseEigen) - Sparse principal component analysis.\n- [TSdbi](http://tsdbi.r-forge.r-project.org/) - Provides a common interface to time series databases.\n- [tseries](https://cran.r-project.org/web/packages/tseries/index.html) - Time Series Analysis and Computational Finance.\n- [zoo](https://cran.r-project.org/web/packages/zoo/index.html) - S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations).\n- [tis](https://cran.r-project.org/web/packages/tis/index.html) - Functions and S3 classes for time indexes and time indexed series, which are compatible with FAME frequencies.\n- [tfplot](https://cran.r-project.org/web/packages/tfplot/index.html) - Utilities for simple manipulation and quick plotting of time series data.\n- [tframe](https://cran.r-project.org/web/packages/tframe/index.html) - A kernel of functions for programming time series methods in a way that is relatively independently of the representation of time.", "tags": []}
{"fragment_id": "F_R1_318_318", "source_id": "R1", "locator": "tradingresource.md:L318-L318", "text": "", "tags": []}
{"fragment_id": "F_R1_319_332", "source_id": "R1", "locator": "tradingresource.md:L319-L332", "text": "### Data Sources\n\n- [IBrokers](https://cran.r-project.org/web/packages/IBrokers/index.html) - Provides native R access to Interactive Brokers Trader Workstation API.\n- [Rblpapi](https://github.com/Rblp/Rblpapi) - An R Interface to 'Bloomberg' is provided via the 'Blp API'.\n- [Quandl](https://www.quandl.com/tools/r) - Get Financial Data Directly Into R.\n- [Rbitcoin](https://github.com/jangorecki/Rbitcoin) - Unified markets API interface (bitstamp, kraken, btce, bitmarket).\n- [GetTDData](https://github.com/msperlin/GetTDData) - Downloads and aggregates data for Brazilian government issued bonds directly from the website of Tesouro Direto.\n- [GetHFData](https://github.com/msperlin/GetHFData) - Downloads and aggregates high frequency trading data for Brazilian instruments directly from Bovespa ftp site.\n- [Reddit WallstreetBets API](https://dashboard.nbshare.io/apps/reddit/api/) - Provides daily top 50 stocks from reddit (subreddit) Wallstreetbets and their sentiments via the API.\n- [td](https://github.com/eddelbuettel/td) - Interfaces the 'twelvedata' API for stocks and (digital and standard) currencies.\n- [rbcb](https://github.com/wilsonfreitas/rbcb) - R interface to Brazilian Central Bank web services.\n- [rb3](https://github.com/ropensci/rb3) - A bunch of downloaders and parsers for data delivered from B3.\n- [simfinapi](https://github.com/matthiasgomolka/simfinapi) - Makes 'SimFin' data (<https://simfin.com/>) easily accessible in R.\n- [tidyfinance](https://github.com/tidy-finance/r-tidyfinance) - Tidy Finance helper functions to download financial data and process the raw data into a structured Format (tidy data), including", "tags": []}
{"fragment_id": "F_R1_333_334", "source_id": "R1", "locator": "tradingresource.md:L333-L334", "text": "date conversion, scaling factor values, and filtering by the specified date.", "tags": []}
{"fragment_id": "F_R1_335_350", "source_id": "R1", "locator": "tradingresource.md:L335-L350", "text": "### Financial Instruments and Pricing\n\n- [RQuantLib](https://github.com/eddelbuettel/rquantlib) - RQuantLib connects GNU R with QuantLib.\n- [quantmod](https://cran.r-project.org/web/packages/quantmod/index.html) - Quantitative Financial Modelling Framework.\n- [Rmetrics](https://www.rmetrics.org) - The premier open source software solution for teaching and training quantitative finance.\n  - [fAsianOptions](https://cran.r-project.org/web/packages/fAsianOptions/index.html) - EBM and Asian Option Valuation.\n  - [fAssets](https://cran.r-project.org/web/packages/fAssets/index.html) - Analysing and Modelling Financial Assets.\n  - [fBasics](https://cran.r-project.org/web/packages/fBasics/index.html) - Markets and Basic Statistics.\n  - [fBonds](https://cran.r-project.org/web/packages/fBonds/index.html) - Bonds and Interest Rate Models.\n  - [fExoticOptions](https://cran.r-project.org/web/packages/fExoticOptions/index.html) - Exotic Option Valuation.\n  - [fOptions](https://cran.r-project.org/web/packages/fOptions/index.html) - Pricing and Evaluating Basic Options.\n  - [fPortfolio](https://cran.r-project.org/web/packages/fPortfolio/index.html) - Portfolio Selection and Optimization.\n- [portfolio](https://github.com/dgerlanc/portfolio) - Analysing equity portfolios.\n- [sparseIndexTracking](https://github.com/dppalomar/sparseIndexTracking) - Portfolio design to track an index.\n- [covFactorModel](https://github.com/dppalomar/covFactorModel) - Covariance matrix estimation via factor models.\n- [riskParityPortfolio](https://github.com/dppalomar/riskParityPortfolio) - Blazingly fast design of risk parity portfolios.", "tags": []}
{"fragment_id": "F_R1_351_362", "source_id": "R1", "locator": "tradingresource.md:L351-L362", "text": "- [sde](https://cran.r-project.org/web/packages/sde/index.html) - Simulation and Inference for Stochastic Differential Equations.\n- [YieldCurve](https://cran.r-project.org/web/packages/YieldCurve/index.html) - Modelling and estimation of the yield curve.\n- [SmithWilsonYieldCurve](https://cran.r-project.org/web/packages/SmithWilsonYieldCurve/index.html) - Constructs a yield curve by the Smith-Wilson method from a table of LIBOR and SWAP rates.\n- [ycinterextra](https://cran.r-project.org/web/packages/ycinterextra/index.html) - Yield curve or zero-coupon prices interpolation and extrapolation.\n- [AmericanCallOpt](https://cran.r-project.org/web/packages/AmericanCallOpt/index.html) - This package includes pricing function for selected American call options with underlying assets that generate payouts.\n- [VarSwapPrice](https://cran.r-project.org/web/packages/VarSwapPrice/index.html) - Pricing a variance swap on an equity index.\n- [RND](https://cran.r-project.org/web/packages/RND/index.html) - Risk Neutral Density Extraction Package.\n- [LSMonteCarlo](https://cran.r-project.org/web/packages/LSMonteCarlo/index.html) - American options pricing with Least Squares Monte Carlo method.\n- [OptHedging](https://cran.r-project.org/web/packages/OptHedging/index.html) - Estimation of value and hedging strategy of call and put options.\n- [tvm](https://cran.r-project.org/web/packages/tvm/index.html) - Time Value of Money Functions.\n- [OptionPricing](https://cran.r-project.org/web/packages/OptionPricing/index.html) - Option Pricing with Efficient Simulation Algorithms.\n- [credule](https://github.com/blenezet/credule) - Credit Default Swap Functions.", "tags": []}
{"fragment_id": "F_R1_363_370", "source_id": "R1", "locator": "tradingresource.md:L363-L370", "text": "- [derivmkts](https://cran.r-project.org/web/packages/derivmkts/index.html) - Functions and R Code to Accompany Derivatives Markets.\n- [FinCal](https://github.com/felixfan/FinCal) - Package for time value of money calculation, time series analysis and computational finance.\n- [r-quant](https://github.com/artyyouth/r-quant) - R code for quantitative analysis in finance.\n- [options.studies](https://github.com/taylorizing/options.studies) - options trading studies functions for use with options.data package and shiny.\n- [PortfolioAnalytics](https://github.com/braverock/PortfolioAnalytics) - Portfolio Analysis, Including Numerical Methods for Optimizationof Portfolios.\n- [fmbasics](https://github.com/imanuelcostigan/fmbasics) - Financial Market Building Blocks.\n- [R-fixedincome](https://github.com/wilsonfreitas/R-fixedincome) - Fixed income tools for R.", "tags": []}
{"fragment_id": "F_R1_371_378", "source_id": "R1", "locator": "tradingresource.md:L371-L378", "text": "### Trading\n\n- [backtest](https://cran.r-project.org/web/packages/backtest/index.html) - Exploring Portfolio-Based Conjectures About Financial Instruments.\n- [pa](https://cran.r-project.org/web/packages/pa/index.html) - Performance Attribution for Equity Portfolios.\n- [TTR](https://github.com/joshuaulrich/TTR) - Technical Trading Rules.\n- [QuantTools](https://quanttools.bitbucket.io/_site/index.html) - Enhanced Quantitative Trading Modelling.\n- [blotter](https://github.com/braverock/blotter) - Transaction infrastructure for defining instruments, transactions, portfolios and accounts for trading systems and simulation. Provides portfolio support for multi-asset class and multi-currency portfolios. Actively maintained and developed.", "tags": []}
{"fragment_id": "F_R1_379_382", "source_id": "R1", "locator": "tradingresource.md:L379-L382", "text": "### Backtesting\n\n- [quantstrat](https://github.com/braverock/quantstrat) - Transaction-oriented infrastructure for constructing trading systems and simulation. Provides support for multi-asset class and multi-currency portfolios for backtesting and other financial research.", "tags": []}
{"fragment_id": "F_R1_383_386", "source_id": "R1", "locator": "tradingresource.md:L383-L386", "text": "### Risk Analysis\n\n- [PerformanceAnalytics](https://github.com/braverock/PerformanceAnalytics) - Econometric tools for performance and risk analysis.", "tags": []}
{"fragment_id": "F_R1_387_391", "source_id": "R1", "locator": "tradingresource.md:L387-L391", "text": "### Factor Analysis\n\n- [FactorAnalytics](https://github.com/braverock/FactorAnalytics) - The FactorAnalytics package contains fitting and analysis methods for the three main types of factor models used in conjunction with portfolio construction, optimization and risk management, namely fundamental factor models, time series factor models and statistical factor models.\n- [Expected Returns](https://github.com/JustinMShea/ExpectedReturns) - Solutions for enhancing portfolio diversification and replications of seminal papers with R, most of which are discussed in one of the best investment references of the recent decade, Expected Returns: An Investors Guide to Harvesting Market Rewards by Antti Ilmanen.", "tags": []}
{"fragment_id": "F_R1_392_405", "source_id": "R1", "locator": "tradingresource.md:L392-L405", "text": "### Time Series\n\n- [tseries](https://cran.r-project.org/web/packages/tseries/index.html) - Time Series Analysis and Computational Finance.\n- [fGarch](https://cran.r-project.org/web/packages/fGarch/index.html) - Rmetrics - Autoregressive Conditional Heteroskedastic Modelling.\n- [timeSeries](https://cran.r-project.org/web/packages/timeSeries/index.html) - Rmetrics - Financial Time Series Objects.\n- [rugarch](https://github.com/alexiosg/rugarch) - Univariate GARCH Models.\n- [rmgarch](https://github.com/alexiosg/rmgarch) - Multivariate GARCH Models.\n- [tidypredict](https://github.com/edgararuiz/tidypredict) - Run predictions inside the database <https://tidypredict.netlify.com/>.\n- [tidyquant](https://github.com/business-science/tidyquant) - Bringing financial analysis to the tidyverse.\n- [timetk](https://github.com/business-science/timetk) - A toolkit for working with time series in R.\n- [tibbletime](https://github.com/business-science/tibbletime) - Built on top of the tidyverse, tibbletime is an extension that allows for the creation of time aware tibbles through the setting of a time index.\n- [matrixprofile](https://github.com/matrix-profile-foundation/matrixprofile) - Time series data mining library built on top of the novel Matrix Profile data structure and algorithms.\n- [garchmodels](https://github.com/AlbertoAlmuinha/garchmodels) - A parsnip backend for GARCH models.", "tags": []}
{"fragment_id": "F_R1_406_410", "source_id": "R1", "locator": "tradingresource.md:L406-L410", "text": "### Calendars\n\n- [timeDate](https://cran.r-project.org/web/packages/timeDate/index.html) - Chronological and Calendar Objects\n- [bizdays](https://github.com/wilsonfreitas/R-bizdays) - Business days calculations and utilities", "tags": []}
{"fragment_id": "F_R1_411_412", "source_id": "R1", "locator": "tradingresource.md:L411-L412", "text": "## Matlab", "tags": []}
{"fragment_id": "F_R1_413_416", "source_id": "R1", "locator": "tradingresource.md:L413-L416", "text": "### Alternatives\n\n- [RunMat](https://runmat.org) - High performance, Open Source, MATLAB syntax runtime.", "tags": []}
{"fragment_id": "F_R1_417_421", "source_id": "R1", "locator": "tradingresource.md:L417-L421", "text": "### FrameWorks\n\n- [QUANTAXIS](https://github.com/yutiansut/quantaxis) - Integrated Quantitative Toolbox with Matlab.\n- [PROJ_Option_Pricing_Matlab](https://github.com/jkirkby3/PROJ_Option_Pricing_Matlab) - Quant Option Pricing - Exotic/Vanilla: Barrier, Asian, European, American, Parisian, Lookback, Cliquet, Variance Swap, Swing, Forward Starting, Step, Fader", "tags": []}
{"fragment_id": "F_R1_422_437", "source_id": "R1", "locator": "tradingresource.md:L422-L437", "text": "## Julia\n\n- [CcyConv.jl](https://github.com/bhftbootcamp/CcyConv.jl) - Currency conversion library for Julia\n- [CryptoExchangeAPIs.jl](https://github.com/bhftbootcamp/CryptoExchangeAPIs.jl) - A Julia library for cryptocurrency exchange APIs\n- [Fastback.jl](https://github.com/rbeeli/Fastback.jl) - Blazing fast Julia backtester.\n- [Lucky.jl](https://github.com/oliviermilla/Lucky.jl) - Modular, asynchronous trading engine in pure Julia.\n- [QuantLib.jl](https://github.com/pazzo83/QuantLib.jl) - Quantlib implementation in pure Julia.\n- [Ito.jl](https://github.com/aviks/Ito.jl) - A Julia package for quantitative finance.\n- [LightweightCharts.jl](https://github.com/bhftbootcamp/LightweightCharts.jl) - Julia wrapper for Lightweight Charts™ by TradingView.\n- [TALib.jl](https://github.com/femtotrader/TALib.jl) - A Julia wrapper for TA-Lib.\n- [Miletus.jl](https://github.com/JuliaComputing/Miletus.jl) - A financial contract definition, modeling language, and valuation framework.\n- [Temporal.jl](https://github.com/dysonance/Temporal.jl) - Flexible and efficient time series class & methods.\n- [Indicators.jl](https://github.com/dysonance/Indicators.jl) - Financial market technical analysis & indicators on top of Temporal.\n- [Strategems.jl](https://github.com/dysonance/Strategems.jl) - Quantitative systematic trading strategy development and backtesting.\n- [TimeSeries.jl](https://github.com/JuliaStats/TimeSeries.jl) - Time series toolkit for Julia.\n- [TechnicalIndicatorCharts.jl](https://github.com/g-gundam/TechnicalIndicatorCharts.jl) - Visualize OnlineTechnicalIndicators.jl using LightweightCharts.jl.", "tags": []}
{"fragment_id": "F_R1_438_448", "source_id": "R1", "locator": "tradingresource.md:L438-L448", "text": "- [MarketTechnicals.jl](https://github.com/JuliaQuant/MarketTechnicals.jl) - Technical analysis of financial time series on top of TimeSeries.\n- [MarketData.jl](https://github.com/JuliaQuant/MarketData.jl) - Time series market data.\n- [OnlineTechnicalIndicators.jl](https://github.com/femtotrader/OnlineTechnicalIndicators.jl) - Julia Technical Analysis Indicators via online algorithms.\n- [OnlinePortfolioAnalytics.jl](https://github.com/femtotrader/OnlinePortfolioAnalytics.jl) - A Julia quantitative portfolio analytics (risk / performance) via online algorithms.\n- [OnlineResamplers.jl](https://github.com/femtotrader/OnlineResamplers.jl) - High-performance Julia package for real-time resampling of financial market data.\n- [RiskPerf.jl](https://github.com/rbeeli/RiskPerf.jl) - Quantitative risk and performance analysis package for financial time series powered by the Julia language.\n- [TimeFrames.jl](https://github.com/femtotrader/TimeFrames.jl) - A Julia library that defines TimeFrame (essentially for resampling TimeSeries).\n- [DataFrames.jl](https://github.com/JuliaData/DataFrames.jl) - In-memory tabular data in Julia\n- [TSFrames.jl](https://github.com/xKDR/TSFrames.jl) - Handle timeseries data on top of the powerful and mature DataFrames.jl\n- [TimeArrays.jl](https://github.com/bhftbootcamp/TimeArrays.jl) - Time series handling for Julia", "tags": []}
{"fragment_id": "F_R1_449_457", "source_id": "R1", "locator": "tradingresource.md:L449-L457", "text": "## Java\n\n- [Strata](http://strata.opengamma.io/) - Modern open-source analytics and market risk library designed and written in Java.\n- [JQuantLib](https://github.com/frgomes/jquantlib) - JQuantLib is a free, open-source, comprehensive framework for quantitative finance, written in 100% Java.\n- [finmath.net](http://finmath.net) - Java library with algorithms and methodologies related to mathematical finance.\n- [quantcomponents](https://github.com/lsgro/quantcomponents) - Free Java components for Quantitative Finance and Algorithmic Trading.\n- [DRIP](https://lakshmidrip.github.io/DRIP) - Fixed Income, Asset Allocation, Transaction Cost Analysis, XVA Metrics Libraries.\n- [ta4j](https://github.com/ta4j/ta4j) - A Java library for technical analysis.", "tags": []}
{"fragment_id": "F_R1_458_468", "source_id": "R1", "locator": "tradingresource.md:L458-L468", "text": "## JavaScript\n\n- [finance.js](https://github.com/ebradyjobory/finance.js) - A JavaScript library for common financial calculations.\n- [portfolio-allocation](https://github.com/lequant40/portfolio_allocation_js) - PortfolioAllocation is a JavaScript library designed to help constructing financial portfolios made of several assets: bonds, commodities, cryptocurrencies, currencies, exchange traded funds (ETFs), mutual funds, stocks...\n- [Ghostfolio](https://github.com/ghostfolio/ghostfolio) - Wealth management software to keep track of financial assets like stocks, ETFs or cryptocurrencies and make solid, data-driven investment decisions.\n- [IndicatorTS](https://github.com/cinar/indicatorts) - Indicator is a TypeScript module providing various stock technical analysis indicators, strategies, and a backtest framework for trading.\n- [chart-patterns](https://github.com/focus1691/chart-patterns) - Technical analysis library for Market Profile, Volume Profile, Stacked Imbalances and High Volume Node indicators.\n- [orderflow](https://github.com/focus1691/orderflow) - Orderflow trade aggregator for building Footprint Candles from exchange websocket data.\n- [ccxt](https://github.com/ccxt/ccxt) - A JavaScript / Python / PHP cryptocurrency trading API with support for more than 100 bitcoin/altcoin exchanges.\n- [PENDAX](https://github.com/CompendiumFi/PENDAX-SDK) - Javascript SDK for Trading/Data API and Websockets for FTX, FTXUS, OKX, Bybit, & More.", "tags": []}
{"fragment_id": "F_R1_469_472", "source_id": "R1", "locator": "tradingresource.md:L469-L472", "text": "### Data Visualization\n\n- [QUANTAXIS_Webkit](https://github.com/yutiansut/QUANTAXIS_Webkit) - An awesome visualization center based on quantaxis.", "tags": []}
{"fragment_id": "F_R1_473_478", "source_id": "R1", "locator": "tradingresource.md:L473-L478", "text": "## Haskell\n\n- [quantfin](https://github.com/boundedvariation/quantfin) - quant finance in pure haskell.\n- [Haxcel](https://github.com/MarcusRainbow/Haxcel) - Excel Addin for Haskell.\n- [Ffinar](https://github.com/MarcusRainbow/Ffinar) - A financial maths library in Haskell.", "tags": []}
{"fragment_id": "F_R1_479_483", "source_id": "R1", "locator": "tradingresource.md:L479-L483", "text": "## Scala\n\n- [QuantScale](https://github.com/choucrifahed/quantscale) - Scala Quantitative Finance Library.\n- [Scala Quant](https://github.com/frankcash/Scala-Quant) - Scala library for working with stock data from IFTTT recipes or Google Finance.", "tags": []}
{"fragment_id": "F_R1_484_487", "source_id": "R1", "locator": "tradingresource.md:L484-L487", "text": "## Ruby\n\n- [Jiji](https://github.com/unageanu/jiji2) - Open Source Forex algorithmic trading framework using OANDA REST API.", "tags": []}
{"fragment_id": "F_R1_488_493", "source_id": "R1", "locator": "tradingresource.md:L488-L493", "text": "## Elixir/Erlang\n\n- [Tai](https://github.com/fremantle-capital/tai) - Open Source composable, real time, market data and trade execution toolkit.\n- [Workbench](https://github.com/fremantle-industries/workbench) - From Idea to Execution - Manage your trading operation across a globally distributed cluster\n- [Prop](https://github.com/fremantle-industries/prop) - An open and opinionated trading platform using productive & familiar open source libraries and tools for strategy research, execution and operation.", "tags": []}
{"fragment_id": "F_R1_494_499", "source_id": "R1", "locator": "tradingresource.md:L494-L499", "text": "## Golang\n\n- [Kelp](https://github.com/stellar/kelp) - Kelp is an open-source Golang algorithmic cryptocurrency trading bot that runs on centralized exchanges and Stellar DEX (command-line usage and desktop GUI).\n- [marketstore](https://github.com/alpacahq/marketstore) - DataFrame Server for Financial Timeseries Data.\n- [IndicatorGo](https://github.com/cinar/indicator) - IndicatorGo is a Golang module providing various stock technical analysis indicators, strategies, and a backtest framework for trading.", "tags": []}
{"fragment_id": "F_R1_500_508", "source_id": "R1", "locator": "tradingresource.md:L500-L508", "text": "## CPP\n\n- [QuantLib](https://github.com/lballabio/QuantLib) - The QuantLib project is aimed at providing a comprehensive software framework for quantitative finance.\n- [QuantLibRisks](https://github.com/auto-differentiation/QuantLib-Risks-Cpp) - Fast risks with QuantLib in C++\n- [XAD](https://github.com/auto-differentiation/xad) - Automatic Differentation (AAD) Library\n- [TradeFrame](https://github.com/rburkholder/trade-frame) - C++ 17 based framework/library (with sample applications) for testing options based automated trading ideas using DTN IQ real time data feed and Interactive Brokers (TWS API) for trade execution. Comes with built-in [Option Greeks/IV](https://github.com/rburkholder/trade-frame/tree/master/lib/TFOptions) calculation library.\n- [Hikyuu](https://github.com/fasiondog/hikyuu) - A base on Python/C++ open source high-performance quant framework for faster analysis and backtesting, contains the complete trading system components for reuse and combination. You can use python or c++ freely.\n- [OrderMatchingEngine](https://github.com/PIYUSH-KUMAR1809/order-matching-engine) - A production-grade, lock-free, high-frequency trading matching engine achieving 150M+ orders/sec.", "tags": []}
{"fragment_id": "F_R1_509_522", "source_id": "R1", "locator": "tradingresource.md:L509-L522", "text": "## Frameworks\n\n- [QuantLib](https://github.com/lballabio/QuantLib) - The QuantLib project is aimed at providing a comprehensive software framework for quantitative finance.\n  - QuantLibRisks - Fast risks with QuantLib in [Python](https://pypi.org/project/QuantLib-Risks/) and [C++](https://github.com/auto-differentiation/QuantLib-Risks-Cpp)\n  - XAD - Automatic Differentiation (AAD) Library in [Python](https://pypi.org/project/xad/) and [C++](https://github.com/auto-differentiation/xad/)\n  - [JQuantLib](https://github.com/frgomes/jquantlib) - Java port.\n  - [RQuantLib](https://github.com/eddelbuettel/rquantlib) - R port.\n  - [QuantLibAddin](https://www.quantlib.org/quantlibaddin/) - Excel support.\n  - [QuantLibXL](https://www.quantlib.org/quantlibxl/) - Excel support.\n  - [QLNet](https://github.com/amaggiulli/qlnet) - .Net port.\n  - [PyQL](https://github.com/enthought/pyql) - Python port.\n  - [QuantLib.jl](https://github.com/pazzo83/QuantLib.jl) - Julia port.\n  - [QuantLib-Python Documentation](https://quantlib-python-docs.readthedocs.io/) - Documentation for the Python bindings for the QuantLib library", "tags": []}
{"fragment_id": "F_R1_523_529", "source_id": "R1", "locator": "tradingresource.md:L523-L529", "text": "- [TA-Lib](https://ta-lib.org) - perform technical analysis of financial market data.\n  - [ta-lib-python](https://github.com/TA-Lib/ta-lib-python)\n  - [ta-lib](https://github.com/TA-Lib/ta-lib)\n- [Portfolio Optimizer](https://portfoliooptimizer.io/) - Portfolio Optimizer is a Web API for portfolio analysis and optimization.\n- XAD: Automatic Differentation (AAD) Library for [Python](https://pypi.org/project/xad/) and [C++](https://github.com/auto-differentiation/xad)", "tags": []}
{"fragment_id": "F_R1_530_535", "source_id": "R1", "locator": "tradingresource.md:L530-L535", "text": "## CSharp\n\n- [QuantConnect](https://github.com/QuantConnect/Lean) - Lean Engine is an open-source fully managed C# algorithmic trading engine built for desktop and cloud usage.\n- [StockSharp](https://github.com/StockSharp/StockSharp) - Algorithmic trading and quantitative trading open source platform to develop trading robots (stock markets, forex, crypto, bitcoins, and options).\n- [TDAmeritrade.DotNetCore](https://github.com/NVentimiglia/TDAmeritrade.DotNetCore) - Free, open-source .NET Client for the TD Ameritrade Trading Platform. Helps developers integrate TD Ameritrade API into custom trading solutions.", "tags": []}
{"fragment_id": "F_R1_536_546", "source_id": "R1", "locator": "tradingresource.md:L536-L546", "text": "## Rust\n\n- [QuantMath](https://github.com/MarcusRainbow/QuantMath) - Financial maths library for risk-neutral pricing and risk\n- [Barter](https://github.com/barter-rs/barter-rs) - Open-source Rust framework for building event-driven live-trading & backtesting systems\n- [LFEST](https://github.com/MathisWellmann/lfest-rs) - Simulated perpetual futures exchange to trade your strategy against.\n- [TradeAggregation](https://github.com/MathisWellmann/trade_aggregation-rs) - Aggregate trades into user-defined candles using information driven rules.\n- [SlidingFeatures](https://github.com/MathisWellmann/sliding_features-rs) - Chainable tree-like sliding windows for signal processing and technical analysis.\n- [RustQuant](https://github.com/avhz/RustQuant) - Quantitative finance library written in Rust.\n- [finalytics](https://github.com/Nnamdi-sys/finalytics) - A rust library for financial data analysis.\n- [RunMat](https://github.com/runmat-org/runmat) - Rust runtime for MATLAB-syntax array math with automatic CPU/GPU execution and fused kernels for quant simulations.", "tags": []}
{"fragment_id": "F_R1_547_548", "source_id": "R1", "locator": "tradingresource.md:L547-L548", "text": "", "tags": []}
{"fragment_id": "F_R1_549_560", "source_id": "R1", "locator": "tradingresource.md:L549-L560", "text": "## Reproducing Works, Training & Books\n\n- [Auto-Differentiation Website](https://auto-differentiation.github.io/) - Background and  resources on Automatic Differentiation (AD) / Adjoint Algorithmic Differentitation (AAD).\n- [Derman Papers](https://github.com/MarcosCarreira/DermanPapers) - Notebooks that replicate original quantitative finance papers from Emanuel Derman.\n- [ML-Quant](https://www.ml-quant.com/) - Top Quant resources like ArXiv (sanity), SSRN, RePec, Journals, Podcasts, Videos, and Blogs.\n- [volatility-trading](https://github.com/jasonstrimpel/volatility-trading) - A complete set of volatility estimators based on Euan Sinclair's Volatility Trading.\n- [quant](https://github.com/paulperry/quant) - Quantitative Finance and Algorithmic Trading exhaust; mostly ipython notebooks based on Quantopian, Zipline, or Pandas.\n- [fecon235](https://github.com/rsvp/fecon235) - Open source project for software tools in financial economics. Many jupyter notebook to verify theoretical ideas and practical methods interactively.\n- [Quantitative-Notebooks](https://github.com/LongOnly/Quantitative-Notebooks) - Educational notebooks on quantitative finance, algorithmic trading, financial modelling and investment strategy\n- [QuantEcon](https://quantecon.org/) - Lecture series on economics, finance, econometrics and data science; QuantEcon.py, QuantEcon.jl, notebooks\n- [FinanceHub](https://github.com/Finance-Hub/FinanceHub) - Resources for Quantitative Finance\n- [Python_Option_Pricing](https://github.com/dedwards25/Python_Option_Pricing) - An library to price financial options written in Python. Includes: Black Scholes, Black 76, Implied Volatility, American, European, Asian, Spread Options.", "tags": []}
{"fragment_id": "F_R1_561_570", "source_id": "R1", "locator": "tradingresource.md:L561-L570", "text": "- [python-training](https://github.com/jpmorganchase/python-training) - J.P. Morgan's Python training for business analysts and traders.\n- [Stock_Analysis_For_Quant](https://github.com/LastAncientOne/Stock_Analysis_For_Quant) - Different Types of Stock Analysis in Excel, Matlab, Power BI, Python, R, and Tableau.\n- [algorithmic-trading-with-python](https://github.com/chrisconlan/algorithmic-trading-with-python) - Source code for Algorithmic Trading with Python (2020) by Chris Conlan.\n- [MEDIUM_NoteBook](https://github.com/cerlymarco/MEDIUM_NoteBook) - Repository containing notebooks of [cerlymarco](https://github.com/cerlymarco)'s posts on Medium.\n- [QuantFinance](https://github.com/PythonCharmers/QuantFinance) - Training materials in quantitative finance.\n- [IPythonScripts](https://github.com/mgroncki/IPythonScripts) - Tutorials about Quantitative Finance in Python and QuantLib: Pricing, xVAs, Hedging, Portfolio Optimisation, Machine Learning and Deep Learning.\n- [Computational-Finance-Course](https://github.com/LechGrzelak/Computational-Finance-Course) - Materials for the course of Computational Finance.\n- [Machine-Learning-for-Asset-Managers](https://github.com/emoen/Machine-Learning-for-Asset-Managers) - Implementation of code snippets, exercises and application to live data from Machine Learning for Asset Managers (Elements in Quantitative Finance) written by Prof. Marcos López de Prado.\n- [Python-for-Finance-Cookbook](https://github.com/PacktPublishing/Python-for-Finance-Cookbook) - Python for Finance Cookbook, published by Packt.\n- [modelos_vol_derivativos](https://github.com/ysaporito/modelos_vol_derivativos) - \"Modelos de Volatilidade para Derivativos\" book's Jupyter notebooks", "tags": []}
{"fragment_id": "F_R1_571_581", "source_id": "R1", "locator": "tradingresource.md:L571-L581", "text": "- [NMOF](https://github.com/enricoschumann/NMOF) - Functions, examples and data from the first and the second edition of \"Numerical Methods and Optimization in Finance\" by M. Gilli, D. Maringer and E. Schumann (2019, ISBN:978-0128150658).\n- [py4fi2nd](https://github.com/yhilpisch/py4fi2nd) - Jupyter Notebooks and code for Python for Finance (2nd ed., O'Reilly) by Yves Hilpisch.\n- [aiif](https://github.com/yhilpisch/aiif) - Jupyter Notebooks and code for the book Artificial Intelligence in Finance (O'Reilly) by Yves Hilpisch.\n- [py4at](https://github.com/yhilpisch/py4at) - Jupyter Notebooks and code for the book Python for Algorithmic Trading (O'Reilly) by Yves Hilpisch.\n- [dawp](https://github.com/yhilpisch/dawp) - Jupyter Notebooks and code for Derivatives Analytics with Python (Wiley Finance) by Yves Hilpisch.\n- [dx](https://github.com/yhilpisch/dx) - DX Analytics | Financial and Derivatives Analytics with Python.\n- [QuantFinanceBook](https://github.com/LechGrzelak/QuantFinanceBook) - Quantitative Finance book.\n- [rough_bergomi](https://github.com/ryanmccrickerd/rough_bergomi) - A Python implementation of the rough Bergomi model.\n- [frh-fx](https://github.com/ryanmccrickerd/frh-fx) - A python implementation of the fast-reversion Heston model of Mechkov for FX purposes.\n- [Value Investing Studies](https://github.com/euclidjda/value-investing-studies) - A collection of data analysis studies that examine the performance and characteristics of value investing over long periods of time.\n- [Machine Learning Asset Management](https://github.com/firmai/machine-learning-asset-management) - Machine Learning in Asset Management (by @firmai).", "tags": []}
{"fragment_id": "F_R1_582_589", "source_id": "R1", "locator": "tradingresource.md:L582-L589", "text": "- [Deep Learning Machine Learning Stock](https://github.com/LastAncientOne/Deep-Learning-Machine-Learning-Stock) - Deep Learning and Machine Learning stocks represent a promising long-term or short-term opportunity for investors and traders.\n- [Technical Analysis and Feature Engineering](https://github.com/jo-cho/Technical_Analysis_and_Feature_Engineering) - Feature Engineering and Feature Importance of Machine Learning in Financial Market.\n- [Differential Machine Learning and Axes that matter by Brian Huge and Antoine Savine](https://github.com/differential-machine-learning/notebooks) - Implement, demonstrate, reproduce and extend the results of the Risk articles 'Differential Machine Learning' (2020) and 'PCA with a Difference' (2021) by Huge and Savine, and cover implementation details left out from the papers.\n- [systematictradingexamples](https://github.com/robcarver17/systematictradingexamples) - Examples of code related to book [Systematic Trading](www.systematictrading.org) and [blog](http://qoppac.blogspot.com)\n- [pysystemtrade_examples](https://github.com/robcarver17/pysystemtrade_examples) - Examples using pysystemtrade for Robert Carver's [blog](http://qoppac.blogspot.com).\n- [ML_Finance_Codes](https://github.com/mfrdixon/ML_Finance_Codes) - Machine Learning in Finance: From Theory to Practice Book\n- [Hands-On Machine Learning for Algorithmic Trading](https://github.com/packtpublishing/hands-on-machine-learning-for-algorithmic-trading) - Hands-On Machine Learning for Algorithmic Trading, published by Packt\n- [financialnoob-misc](https://github.com/financialnoob/misc) - Codes from @financialnoob's posts", "tags": []}
{"fragment_id": "F_R1_590_597", "source_id": "R1", "locator": "tradingresource.md:L590-L597", "text": "- [MesoSim Options Trading Strategy Library](https://github.com/deltaray-io/strategy-library) - Free and public Options Trading strategy library for MesoSim. \n- [Quant-Finance-With-Python-Code](https://github.com/lingyixu/Quant-Finance-With-Python-Code) - Repo for code examples in Quantitative Finance with Python by Chris Kelliher\n- [QuantFinanceTraining](https://github.com/JoaoJungblut/QuantFinanceTraining) - This repository contains codes that were executed during my training in the CQF (Certificate in Quantitative Finance). The codes are organized by class, facilitating navigation and reference.\n- [Statistical-Learning-based-Portfolio-Optimization](https://github.com/YannickKae/Statistical-Learning-based-Portfolio-Optimization) - This R Shiny App utilizes the Hierarchical Equal Risk Contribution (HERC) approach, a modern portfolio optimization method developed by Raffinot (2018).\n- [book_irds3](https://github.com/attack68/book_irds3) - Code repository for Pricing and Trading Interest Rate Derivatives.\n- [Autoencoder-Asset-Pricing-Models](https://github.com/RichardS0268/Autoencoder-Asset-Pricing-Models) - Reimplementation of Autoencoder Asset Pricing Models ([GKX, 2019](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3335536)).\n- [Finance](https://github.com/shashankvemuri/Finance) - 150+ quantitative finance Python programs to help you gather, manipulate, and analyze stock market data.\n- [101_formulaic_alphas](https://github.com/ram-ki/101_formulaic_alphas) - Implementation of [101 formulaic alphas](https://arxiv.org/ftp/arxiv/papers/1601/1601.00991.pdf) using qstrader.", "tags": []}
{"fragment_id": "F_R1_598_601", "source_id": "R1", "locator": "tradingresource.md:L598-L601", "text": "- [Tidy Finance](https://www.tidy-finance.org/) - An opinionated approach to empirical research in financial economics - a fully transparent, open-source code base in multiple programming languages (Python and R) to enable the reproducible implementation of financial research projects for students and practitioners.\n- [RoughVolatilityWorkshop](https://github.com/jgatheral/RoughVolatilityWorkshop) - 2024 QuantMind's Rough Volatility Workshop lectures.\n- [AFML](https://github.com/boyboi86/AFML) - All the answers for exercises from Advances in Financial Machine Learning by Dr Marco Lopez de Parodo.\n- [AlgoTradingLib](https://github.com/usdaud/algotradinglib.github.io) - A catalog of algorithmic trading libraries, frameworks, strategies, and educational materials.", "tags": []}
{"fragment_id": "F_R2_1_12", "source_id": "R2", "locator": "research_system_mechanics_full.md:L1-L12", "text": "# Research System Mechanics — Full Operational Doctrine\nGenerated: 2026-02-18 22:54 UTC\n\nThis document defines the **Research System Mechanics** underlying a systematic crypto research organization.\nIt integrates all supplied research into a unified operational layer describing **how research itself functions**,\nnot merely how strategies are built.\n\nThe goal is to specify the mechanics that transform raw hypotheses into deployable portfolio components\nthrough deterministic, auditable, and capacity-aware processes.\n\n---", "tags": []}
{"fragment_id": "F_R2_13_25", "source_id": "R2", "locator": "research_system_mechanics_full.md:L13-L25", "text": "# 0. PURPOSE OF THE RESEARCH SYSTEM\n\nThe research system exists to solve a specific problem:\n\n> Distinguish *statistical artifacts* from *deployable market edges* under realistic execution constraints.\n\nKey principle:\n\nResearch is not idea generation.\nResearch is **controlled falsification under market constraints**.\n\n---", "tags": []}
{"fragment_id": "F_R2_26_29", "source_id": "R2", "locator": "research_system_mechanics_full.md:L26-L29", "text": "# 1. RESEARCH ONTOLOGY\n\nThe system operates on six canonical object classes.", "tags": []}
{"fragment_id": "F_R2_30_43", "source_id": "R2", "locator": "research_system_mechanics_full.md:L30-L43", "text": "## 1.1 Object Hierarchy\n\nMarketMechanism\n→ Event\n→ State\n→ CandidateEdge\n→ ExecutedEdge\n→ Strategy\n→ PortfolioComponent\n\nEach stage introduces stricter constraints.\n\n---", "tags": []}
{"fragment_id": "F_R2_44_45", "source_id": "R2", "locator": "research_system_mechanics_full.md:L44-L45", "text": "## 1.2 Object Definitions", "tags": []}
{"fragment_id": "F_R2_46_56", "source_id": "R2", "locator": "research_system_mechanics_full.md:L46-L56", "text": "### MarketMechanism\nStructural exchange rules and feedback loops:\n- funding transfer rules\n- liquidation mechanics\n- mark/index pricing\n- orderbook formation\n\nMechanisms define feasible dynamics.\n\n---", "tags": []}
{"fragment_id": "F_R2_57_73", "source_id": "R2", "locator": "research_system_mechanics_full.md:L57-L73", "text": "### Event\nA measurable structural deviation.\n\nEvent :=\n{\n    event_id,\n    mechanism,\n    trigger_condition,\n    timestamp,\n    venue,\n    symbol\n}\n\nEvents are deterministic functions of data.\n\n---", "tags": []}
{"fragment_id": "F_R2_74_88", "source_id": "R2", "locator": "research_system_mechanics_full.md:L74-L88", "text": "### State\nPersistent market configuration.\n\nState := vector of regime variables:\n- volatility regime\n- liquidity depth\n- funding persistence\n- OI trend\n- spread structure\n- macro correlation\n\nEdges depend on state transitions, not prices.\n\n---", "tags": []}
{"fragment_id": "F_R2_89_101", "source_id": "R2", "locator": "research_system_mechanics_full.md:L89-L101", "text": "### CandidateEdge\nHypothesis linking event + state → action.\n\nContains:\n- conditions\n- entry logic\n- exit logic\n- expected horizon\n\nNot yet tradable.\n\n---", "tags": []}
{"fragment_id": "F_R2_102_112", "source_id": "R2", "locator": "research_system_mechanics_full.md:L102-L112", "text": "### ExecutedEdge\nCandidateEdge after execution simulation.\n\nAdds:\n- fill model\n- slippage realization\n- impact penalty\n- latency model\n\n---", "tags": []}
{"fragment_id": "F_R2_113_117", "source_id": "R2", "locator": "research_system_mechanics_full.md:L113-L117", "text": "### Strategy\nDeterministic executable program proving reproducibility.\n\n---", "tags": []}
{"fragment_id": "F_R2_118_122", "source_id": "R2", "locator": "research_system_mechanics_full.md:L118-L122", "text": "### PortfolioComponent\nStrategy validated under interaction constraints.\n\n---", "tags": []}
{"fragment_id": "F_R2_123_124", "source_id": "R2", "locator": "research_system_mechanics_full.md:L123-L124", "text": "# 2. RESEARCH PIPELINE MECHANICS", "tags": []}
{"fragment_id": "F_R2_125_142", "source_id": "R2", "locator": "research_system_mechanics_full.md:L125-L142", "text": "## Stage 1 — Event Discovery\n\nInput:\n- raw market datasets\n\nProcess:\n1. Detect structural anomalies.\n2. Segment regime transitions.\n3. Register events.\n\nOutput:\nEvent Registry (versioned).\n\nFailure Mode:\nOverfitting event definitions to outcomes.\n\n---", "tags": []}
{"fragment_id": "F_R2_143_157", "source_id": "R2", "locator": "research_system_mechanics_full.md:L143-L157", "text": "## Stage 2 — Hypothesis Expansion\n\nGenerate conditional hypotheses:\n\nEvent × State × Action × Horizon\n\nConstraints:\n- hypothesis must be falsifiable\n- rules must be executable\n\nOutput:\nCandidateEdge set.\n\n---", "tags": []}
{"fragment_id": "F_R2_158_173", "source_id": "R2", "locator": "research_system_mechanics_full.md:L158-L173", "text": "## Stage 3 — Statistical Validation\n\nCompute:\n- forward expectancy\n- regime robustness\n- turnover statistics\n\nMandatory tests:\n- walkforward\n- regime partitioning\n- bootstrap stability\n\nReject purely statistical edges.\n\n---", "tags": []}
{"fragment_id": "F_R2_174_187", "source_id": "R2", "locator": "research_system_mechanics_full.md:L174-L187", "text": "## Stage 4 — Multiplicity Control\n\nProblem:\nLarge hypothesis spaces create false discoveries.\n\nControls:\n- FDR correction\n- reality-check bootstrap\n- minimum sample thresholds\n\nResearch throughput tracked explicitly.\n\n---", "tags": []}
{"fragment_id": "F_R2_188_204", "source_id": "R2", "locator": "research_system_mechanics_full.md:L188-L204", "text": "## Stage 5 — Bridge (Execution Translation)\n\nTransforms theoretical signals into executable trades.\n\nSimulations include:\n- orderbook interaction\n- fill probability\n- queue dynamics\n- funding timing\n\nBridge Score =\nexecuted_PnL / theoretical_PnL\n\nEdges failing threshold are discarded.\n\n---", "tags": []}
{"fragment_id": "F_R2_205_218", "source_id": "R2", "locator": "research_system_mechanics_full.md:L205-L218", "text": "## Stage 6 — Strategy Compilation\n\nCompile edge into deterministic artifact.\n\nRequirements:\n- dataset hash\n- config hash\n- code commit\n- container digest\n\nReproducibility is mandatory.\n\n---", "tags": []}
{"fragment_id": "F_R2_219_227", "source_id": "R2", "locator": "research_system_mechanics_full.md:L219-L227", "text": "## Stage 7 — Walkforward Validation\n\nRepeated out-of-sample testing across rolling windows.\n\nGoal:\nDetect regime dependence.\n\n---", "tags": []}
{"fragment_id": "F_R2_228_241", "source_id": "R2", "locator": "research_system_mechanics_full.md:L228-L241", "text": "## Stage 8 — Portfolio Integration\n\nSimulate multi-strategy system:\n\nTests:\n- correlation clustering\n- liquidity contention\n- execution overlap\n- synchronized drawdown risk\n\nPortfolio acts as final gate.\n\n---", "tags": []}
{"fragment_id": "F_R2_242_260", "source_id": "R2", "locator": "research_system_mechanics_full.md:L242-L260", "text": "# 3. RESEARCH FUNNEL MECHANICS\n\nResearch behaves as a funnel.\n\nExample survival ratios:\n\nEvents detected:        10,000\nCandidate edges:        1,000\nStatistically viable:   100\nBridge viable:          20\nStrategy stable:        5\nPortfolio deployable:   1–2\n\nAttrition is expected behavior.\n\nHigh rejection rate indicates healthy research.\n\n---", "tags": []}
{"fragment_id": "F_R2_261_280", "source_id": "R2", "locator": "research_system_mechanics_full.md:L261-L280", "text": "# 4. RESEARCH DIAGNOSTICS\n\nSystem continuously measures itself.\n\nMetrics:\n\nDiscovery Density =\nevents_detected / time\n\nEdge Survival Rate =\nexecuted_edges / candidates\n\nExecution Degradation =\ntheoretical − executed returns\n\nCapacity Sensitivity =\nPnL change vs participation\n\n---", "tags": []}
{"fragment_id": "F_R2_281_282", "source_id": "R2", "locator": "research_system_mechanics_full.md:L281-L282", "text": "# 5. FAILURE MODE TAXONOMY", "tags": []}
{"fragment_id": "F_R2_283_285", "source_id": "R2", "locator": "research_system_mechanics_full.md:L283-L285", "text": "## F1 — Lookahead Leakage\nFeature contamination by future information.", "tags": []}
{"fragment_id": "F_R2_286_288", "source_id": "R2", "locator": "research_system_mechanics_full.md:L286-L288", "text": "## F2 — Cost Illusion\nGross alpha erased after execution.", "tags": []}
{"fragment_id": "F_R2_289_291", "source_id": "R2", "locator": "research_system_mechanics_full.md:L289-L291", "text": "## F3 — Capacity Saturation\nEdge disappears when scaled.", "tags": []}
{"fragment_id": "F_R2_292_294", "source_id": "R2", "locator": "research_system_mechanics_full.md:L292-L294", "text": "## F4 — Regime Dependency\nEdge limited to narrow market state.", "tags": []}
{"fragment_id": "F_R2_295_297", "source_id": "R2", "locator": "research_system_mechanics_full.md:L295-L297", "text": "## F5 — Parameter Fragility\nPerformance collapses outside optimum.", "tags": []}
{"fragment_id": "F_R2_298_302", "source_id": "R2", "locator": "research_system_mechanics_full.md:L298-L302", "text": "## F6 — Portfolio Interaction Failure\nStrategies interfere destructively.\n\n---", "tags": []}
{"fragment_id": "F_R2_303_320", "source_id": "R2", "locator": "research_system_mechanics_full.md:L303-L320", "text": "# 6. GOVERNANCE & REPRODUCIBILITY\n\nEvery experiment logged:\n\nExperiment :=\n{\n    dataset_version,\n    code_version,\n    parameters,\n    random_seed,\n    timestamp,\n    results\n}\n\nResearch becomes auditable history.\n\n---", "tags": []}
{"fragment_id": "F_R2_321_336", "source_id": "R2", "locator": "research_system_mechanics_full.md:L321-L336", "text": "# 7. DATA LINEAGE\n\nAll datasets immutable snapshots.\n\nDataset :=\n{\n    source,\n    ingestion_time,\n    transformation_chain,\n    hash\n}\n\nEnsures deterministic replay.\n\n---", "tags": []}
{"fragment_id": "F_R2_337_354", "source_id": "R2", "locator": "research_system_mechanics_full.md:L337-L354", "text": "# 8. EDGE ECOLOGY (SYSTEM DYNAMICS)\n\nEdges evolve:\n\n1. Discovery\n2. Exploitation\n3. Crowding\n4. Decay\n\nResearch must monitor lifecycle stage.\n\nCrowding indicators:\n- declining slippage-adjusted returns\n- increased correlation across strategies\n- rising impact costs\n\n---", "tags": []}
{"fragment_id": "F_R2_355_370", "source_id": "R2", "locator": "research_system_mechanics_full.md:L355-L370", "text": "# 9. RESEARCH CONTROL LOOP\n\nContinuous cycle:\n\nDetect → Test → Execute → Measure → Refine\n\nFeedback updates:\n- event definitions\n- state variables\n- cost models\n- capacity assumptions\n\nResearch system is adaptive.\n\n---", "tags": []}
{"fragment_id": "F_R2_371_378", "source_id": "R2", "locator": "research_system_mechanics_full.md:L371-L378", "text": "# 10. CORE MECHANICAL PRINCIPLE\n\nThe research system converts uncertainty into constraint-tested knowledge.\n\nAlpha is not predicted.\nAlpha is **filtered through progressively stricter mechanical tests** until only\nexecution-survivable edges remain.", "tags": []}
{"fragment_id": "F_R3_1_15", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L1-L15", "text": "# Master Unified Crypto Research Framework — Technical Operational Synthesis\nGenerated: 2026-02-18 22:50 UTC\n\nThis document replaces generic synthesis with an **operational, research-grade specification**\nderived from all provided materials.\n\nObjective:\nTransform the combined research corpus into a **precise, implementable systematic crypto\nresearch doctrine** aligned with real backtesting and execution pipelines.\n\nNo summaries. No abstraction without operational meaning.\nAll concepts defined as objects, processes, constraints, or measurable tests.\n\n---", "tags": []}
{"fragment_id": "F_R3_16_25", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L16-L25", "text": "# 0. RESEARCH AXIOMS (Derived Constraints)\n\nA1. Markets are constrained systems, not prediction problems.\nA2. Alpha must survive execution simulation before statistical validation is meaningful.\nA3. Every research artifact must be replayable deterministically.\nA4. Capacity is a property of the edge itself.\nA5. Discovery and deployment are distinct optimization problems.\n\n---", "tags": []}
{"fragment_id": "F_R3_26_27", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L26-L27", "text": "# 1. DISCOVERY — EVENT ENGINEERING", "tags": []}
{"fragment_id": "F_R3_28_41", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L28-L41", "text": "## 1.1 Event Object Specification\n\nEvent :=\n{\n    event_id,\n    venue,\n    symbol,\n    timestamp_start,\n    timestamp_end,\n    trigger_condition,\n    state_vector_before,\n    state_vector_after\n}", "tags": []}
{"fragment_id": "F_R3_42_67", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L42-L67", "text": "### Allowed Event Classes (merged corpus)\n\nFunding Events\n- funding_extreme\n- funding_persistence\n- funding_flip\n\nBasis Events\n- basis_expansion\n- basis_compression\n\nLiquidity Events\n- depth_collapse\n- spread_widening\n- orderbook_imbalance\n\nLeverage Events\n- liquidation_cluster\n- OI_acceleration\n\nVolatility Events\n- realized_vol_break\n- volatility_compression_release\n\n---", "tags": []}
{"fragment_id": "F_R3_68_86", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L68-L86", "text": "## 1.2 State Vector (Canonical Form)\n\nS(t) =\n{\n    realized_volatility,\n    implied_volatility_proxy,\n    orderbook_depth,\n    spread,\n    funding_rate,\n    open_interest,\n    trade_imbalance,\n    return_autocorrelation,\n    market_beta\n}\n\nEdges operate on ΔS(t).\n\n---", "tags": []}
{"fragment_id": "F_R3_87_101", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L87-L101", "text": "## 1.3 Labeling Logic (Point‑in‑Time)\n\nFor event at t₀:\n\nFeatures:\nX ← data(ts ≤ t₀)\n\nLabels:\nY ← forward returns window [t₀+Δ₁, t₀+Δ₂]\n\nConstraint:\nNo derived feature may depend on Y horizon.\n\n---", "tags": []}
{"fragment_id": "F_R3_102_103", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L102-L103", "text": "# 2. VALIDATION — EDGE CONTRACT", "tags": []}
{"fragment_id": "F_R3_104_122", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L104-L122", "text": "## 2.1 Candidate Edge Object\n\nEdge :=\n{\n    edge_id,\n    event_type,\n    condition_set,\n    action_rule,\n    holding_rule,\n    exit_rule\n}\n\nExample:\n\nIF funding_persistence AND volatility_low\nTHEN long_spot_short_perp\n\n---", "tags": []}
{"fragment_id": "F_R3_123_137", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L123-L137", "text": "## 2.2 After‑Cost Expectancy Model\n\nPnL_net =\nPnL_gross\n− fees\n− spread_cost\n− slippage_model(q)\n− impact_model(q)\n− funding_cost\n− latency_penalty\n\nImpact_model(q) ≈ σ × √(q / ADV)\n\n---", "tags": []}
{"fragment_id": "F_R3_138_149", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L138-L149", "text": "## 2.3 Validation Tests (Mandatory)\n\nEdge passes only if:\n\n1. Expectancy > 0 across ≥ 3 regime partitions\n2. Sharpe_adj survives deflated Sharpe test\n3. Turnover-adjusted return positive\n4. Capacity stress ≤ predefined degradation threshold\n5. Parameter neighborhood stability satisfied\n\n---", "tags": []}
{"fragment_id": "F_R3_150_162", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L150-L162", "text": "## 2.4 Multiplicity Control Pipeline\n\nDiscovery → multiple hypotheses → correction layer:\n\nApply:\n- FDR (Benjamini–Hochberg)\n- Reality Check bootstrap\n- White’s SPA (optional)\n\nReject edges failing adjusted significance.\n\n---", "tags": []}
{"fragment_id": "F_R3_163_180", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L163-L180", "text": "# 3. BRIDGE — EXECUTION FEASIBILITY (Critical Layer)\n\nBridge transforms statistical edge → executable edge.\n\nBridge Tests:\n\nB1. Fill probability simulation\nB2. Queue position modeling\nB3. Partial fill persistence\nB4. Latency sensitivity\nB5. Funding payment timing alignment\n\nBridge Score := executed_PnL / theoretical_PnL\n\nMinimum threshold required.\n\n---", "tags": []}
{"fragment_id": "F_R3_181_182", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L181-L182", "text": "# 4. STRATEGY — DETERMINISTIC PROGRAM", "tags": []}
{"fragment_id": "F_R3_183_199", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L183-L199", "text": "## 4.1 Strategy Specification\n\nStrategy :=\n{\n    strategy_id,\n    edge_reference,\n    dataset_hash,\n    code_commit,\n    config_hash,\n    container_digest,\n    random_seed\n}\n\nRe-running must produce identical trades.\n\n---", "tags": []}
{"fragment_id": "F_R3_200_211", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L200-L211", "text": "## 4.2 Parameter Stability Test\n\nFor parameter θ:\n\nEvaluate neighborhood N(θ):\n\nPerformance variance must remain bounded.\n\nReject sharp optima.\n\n---", "tags": []}
{"fragment_id": "F_R3_212_224", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L212-L224", "text": "## 4.3 Stress Matrix\n\nRun simulations under:\n\n- fees ×2\n- liquidity −50%\n- volatility +100%\n- execution delay +1 bar\n\nEdge must remain profitable.\n\n---", "tags": []}
{"fragment_id": "F_R3_225_226", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L225-L226", "text": "# 5. PORTFOLIO — SYSTEM OPTIMIZATION", "tags": []}
{"fragment_id": "F_R3_227_238", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L227-L238", "text": "## 5.1 Portfolio Object\n\nPortfolio :=\n{\n    strategies[],\n    covariance_matrix,\n    capacity_constraints,\n    execution_model\n}\n\n---", "tags": []}
{"fragment_id": "F_R3_239_245", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L239-L245", "text": "## 5.2 Allocation Rule\n\nw_i ∝ (Edge_i_expectancy)\n      / (Risk_i × CapacityPenalty_i × CorrelationCluster_i)\n\n---", "tags": []}
{"fragment_id": "F_R3_246_258", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L246-L258", "text": "## 5.3 Interaction Risks\n\nSimulate:\n\n- simultaneous execution overlap\n- liquidity contention\n- correlation spikes\n- regime collapse\n\nPortfolio failure overrides strategy success.\n\n---", "tags": []}
{"fragment_id": "F_R3_259_280", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L259-L280", "text": "# 6. DATA REQUIREMENTS (Merged Corpus)\n\nRequired datasets:\n\nMarket Data\n- spot OHLCV\n- perp OHLCV\n- trades\n- orderbook snapshots\n\nDerivatives Data\n- funding\n- open interest\n- liquidation feeds\n\nExecution Data\n- fees schedules (point‑in‑time)\n- tick size\n- lot size\n\n---", "tags": []}
{"fragment_id": "F_R3_281_293", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L281-L293", "text": "# 7. FAILURE MODES (Cross‑Research Diagnosis)\n\nObserved structural failure classes:\n\nF1. Lookahead leakage via normalization.\nF2. Edge disappears after costs.\nF3. Capacity saturation.\nF4. Regime dependence hidden by aggregation.\nF5. Parameter instability.\nF6. Portfolio correlation explosion.\n\n---", "tags": []}
{"fragment_id": "F_R3_294_306", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L294-L306", "text": "# 8. RESEARCH WORKFLOW (Operational)\n\n1. Build Event Registry\n2. Generate candidate edges\n3. Run validation tests\n4. Bridge execution feasibility\n5. Compile deterministic strategy\n6. Walkforward validation\n7. Portfolio simulation\n8. Deployment eligibility\n\n---", "tags": []}
{"fragment_id": "F_R3_307_318", "source_id": "R3", "locator": "master_unified_crypto_research_OPERATIONAL.md:L307-L318", "text": "# 9. CORE SYNTHESIS RESULT\n\nSystematic crypto research is an **engineering pipeline**:\n\nEvent Detection\n→ State Conditioning\n→ Edge Contract Testing\n→ Execution Validation\n→ Portfolio System Integration\n\nPrediction plays a secondary role to constraint exploitation.", "tags": []}
{"fragment_id": "F_R4_1_16", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1-L16", "text": "# Master Unified Crypto Research Framework (Synthesized)\nGenerated: 2026-02-18 22:42 UTC\n\nThis document integrates all provided research into a single master synthesis.\n\nRules:\n- Duplicate concepts merged\n- No information removed\n- All statements traceable [R1–R5]\n- Full originals preserved in appendix\n\nFramework Backbone:\nDiscovery → Validation → Strategy → Portfolio\n\n---", "tags": []}
{"fragment_id": "F_R4_17_30", "source_id": "R4", "locator": "unified_crypto_research_master.md:L17-L30", "text": "## Executive Synthesis\n\nEmergent unified conclusions:\n\n1. Alpha arises from mechanism dislocations rather than indicators.\n2. Execution realism determines survivability of research.\n3. Deterministic replayability is mandatory infrastructure.\n4. Capacity constraints act as final reality filter.\n\nNew emergent structure:\nStructural Layer → Statistical Layer → Execution Layer.\n\n---", "tags": []}
{"fragment_id": "F_R4_31_40", "source_id": "R4", "locator": "unified_crypto_research_master.md:L31-L40", "text": "## Source Map\n- [R1] Full-Stack Research Framework for Systematic Crypto Portfolios\n- [R2] Inputs: event time t0, lookback window L, raw time-sorted data D\n- [R3] Institutional Crypto Research Framework Audit and Expansion\n- [R4] 1. Core Idea (What This Framework Actually Is)\n- [R5] 1. What You Are Actually Researching\n\n\n---", "tags": []}
{"fragment_id": "F_R4_41_61", "source_id": "R4", "locator": "unified_crypto_research_master.md:L41-L61", "text": "# DISCOVERY (Merged)\n\nUnified Event Taxonomy:\n- Funding regime shifts [R1][R2][R4]\n- Basis convergence/divergence [R2][R3]\n- Liquidation cascades [R1][R5]\n- Liquidity regime breaks [R3]\n\nContext Deltas:\n- Volatility regimes\n- Liquidity states\n- OI acceleration\n- Funding persistence\n\nInvariant Constraints:\n- Point-in-time construction\n- No lookahead\n- Venue-specific mechanics\n\n---", "tags": []}
{"fragment_id": "F_R4_62_75", "source_id": "R4", "locator": "unified_crypto_research_master.md:L62-L75", "text": "# VALIDATION (Merged)\n\nEdge Definition:\nExpectancy AFTER costs AND capacity penalties.\n\nMerged validation stack:\n- Explicit fees\n- Slippage/impact\n- Participation limits\n- Liquidity degradation stress\n- Multiplicity controls (FDR, reality check)\n\n---", "tags": []}
{"fragment_id": "F_R4_76_93", "source_id": "R4", "locator": "unified_crypto_research_master.md:L76-L93", "text": "# STRATEGY (Merged)\n\nStrategy = Deterministic executable research artifact.\n\nRequired provenance:\n- Dataset hash\n- Code commit\n- Container digest\n- RNG seed\n- Event registry version\n\nStability:\n- Parameter neighborhoods\n- Regime robustness\n- Stress testing\n\n---", "tags": []}
{"fragment_id": "F_R4_94_104", "source_id": "R4", "locator": "unified_crypto_research_master.md:L94-L104", "text": "# PORTFOLIO (Merged)\n\nUnified allocation logic:\n- Correlation-aware sizing\n- Capacity constraints\n- Execution simulation embedded\n\nPortfolio acts as final falsification layer.\n\n---", "tags": []}
{"fragment_id": "F_R4_105_116", "source_id": "R4", "locator": "unified_crypto_research_master.md:L105-L116", "text": "# CONTRADICTIONS & TENSIONS\n\n| Topic | View A | View B | Diagnosis |\n|------|--------|--------|-----------|\n| Funding role | Mechanism primitive [R2] | Signal proxy [R1] | Layer confusion |\n| Capacity | Conservative [R2] | Exploratory scaling [R4] | Research vs deployment |\n| Validation strictness | Hard gating [R3] | Discovery-first [R5] | Pipeline stage mismatch |\n\nUnresolved with provided information.\n\n---", "tags": []}
{"fragment_id": "F_R4_117_122", "source_id": "R4", "locator": "unified_crypto_research_master.md:L117-L122", "text": "# DEFINITIONS & NORMALIZATION\n\nDefinitions preserved per source; none collapsed.\n\n---", "tags": []}
{"fragment_id": "F_R4_123_128", "source_id": "R4", "locator": "unified_crypto_research_master.md:L123-L128", "text": "# LIMITATIONS\n\nMissing metadata and referenced graphics in some sources.\n\n---", "tags": []}
{"fragment_id": "F_R4_129_133", "source_id": "R4", "locator": "unified_crypto_research_master.md:L129-L133", "text": "# APPENDIX — FULL SOURCE PRESERVATION\n\n\n---", "tags": []}
{"fragment_id": "F_R4_134_135", "source_id": "R4", "locator": "unified_crypto_research_master.md:L134-L135", "text": "## FULL SOURCE [R1]", "tags": []}
{"fragment_id": "F_R4_136_137", "source_id": "R4", "locator": "unified_crypto_research_master.md:L136-L137", "text": "# Full-Stack Research Framework for Systematic Crypto Portfolios", "tags": []}
{"fragment_id": "F_R4_138_147", "source_id": "R4", "locator": "unified_crypto_research_master.md:L138-L147", "text": "## Scope, objects, and hard constraints\n\nThis framework assumes trading decisions are generated from **point-in-time, venue-specific state** and executed under an explicit transaction cost + capacity model, with every research artifact versioned for deterministic replay. Funding-driven convergence between perpetual futures and spot, and mark/index constructs provided by venues, are treated as *mechanism primitives* rather than “signals.” citeturn6view0turn6view2turn6view3\n\n**Design knobs (if unspecified, defaults are shown):**\n- **Universe**: spot + perpetual futures (default: both; perps are required to study funding/basis regimes). citeturn6view0turn2search28turn6view3  \n- **Horizon**: intraday to multi-day (default: event-horizon dependent, e.g., minutes–hours for liquidation/vol shocks; hours–days for funding/basis). citeturn6view0turn6view4  \n- **Venue set**: centralized venues first (default), with DEX optional as an additional “routing + latency + gas” regime. Mark/index definitions differ by venue; treat as venue-specific data contracts. citeturn6view2turn5search1turn5search9  \n- **Stack**: Python research + replayable containers; event registry + datasets are immutable snapshots with hashes.", "tags": []}
{"fragment_id": "F_R4_148_153", "source_id": "R4", "locator": "unified_crypto_research_master.md:L148-L153", "text": "**Hard constraints enforced everywhere:**\n- **No lookahead bias**: features at decision time \\(t_0\\) use only information available up to \\(t_0\\); labels can use \\(t>t_0\\) but must never leak into features/splits. Look-ahead bias is explicitly defined as using future-unavailable information in a simulation. citeturn8search1turn0search19  \n- **In-sample vs out-of-sample separation**: all edge selection, parameter choosing, and multiplicity correction happen in-sample; only locked specs are evaluated out-of-sample.  \n- **Data-snooping control**: repeated reuse of the same data for model selection inflates false discoveries; therefore apply multiplicity controls and, when appropriate, backtest selection-bias diagnostics and/or “reality check” style procedures. citeturn8search2turn3search3turn0search13  \n- **Execution realism**: PnL is always computed after explicit + implicit costs, using implementation shortfall-style accounting against a decision/arrival benchmark. citeturn7search6turn2search15turn0search20", "tags": []}
{"fragment_id": "F_R4_154_159", "source_id": "R4", "locator": "unified_crypto_research_master.md:L154-L159", "text": "Key research objects used across the pipeline:\n- **Event instance** \\(e\\): \\((\\text{event\\_type}, i, v, t_0, \\text{attrs}, \\text{context})\\)\n- **Candidate edge** \\(c\\): \\((e \\rightarrow \\text{action rule})\\) + parameterization + cost/capacity constraints\n- **Strategy spec** \\(s\\): fully executable rules + parameter bounds + data snapshot hash\n- **Portfolio spec** \\(p\\): allocation + correlation control + capacity + execution simulator assumptions", "tags": []}
{"fragment_id": "F_R4_160_163", "source_id": "R4", "locator": "unified_crypto_research_master.md:L160-L163", "text": "## Phase 1 — Discovery\n\nDiscovery produces (i) a formal event registry, (ii) context-delta definitions (state transitions), and (iii) invariants that act as cross-asset/venue constraints and data-quality gates.", "tags": []}
{"fragment_id": "F_R4_164_175", "source_id": "R4", "locator": "unified_crypto_research_master.md:L164-L175", "text": "### Event registry\n\nAn **event registry** is a deterministic taxonomy of *tradable, labelable* market events with standardized fields and labeling logic. The registry must be versioned because any taxonomy change changes the candidate universe and invalidates prior multiplicity accounting. citeturn8search2turn0search13\n\nBelow is a minimal-but-complete registry suitable for systematic crypto research, with event types chosen to map to well-defined venue mechanics (funding, liquidations, mark/index) and general market microstructure (volatility shocks, liquidity regime breaks, structural breaks). citeturn6view0turn6view2turn0search6turn3search2\n\n**Event registry table (formal definitions + required data fields):**\n\n| Event type | Formal trigger (decision-time predicate) | Minimal required fields (point-in-time) | Notes on mechanism / why it’s definable |\n|---|---|---|---|\n| Funding dislocation | \\(|z(\\text{funding\\_rate}_{i,v}(t_0))| \\ge z_\\*\\) and \\(|\\Delta \\text{premium\\_index}|\\ge p_\\*\\) | perp funding rate, funding interval schedule, mark price, index/spot reference, premium index (or proxy), top-of-book, timestamp | Funding transfers are peer-to-peer and computed from notional × funding rate (venue-defined); designed to anchor perp prices to the underlying index. citeturn6view0turn2search28turn6view3 |\n| Basis / forward dislocation | \\(|\\ln(F/S)|\\ge b_\\*\\) relative to carry bounds and costs | spot mid, futures/perp mid, funding (if perp), borrow/lend proxy (if available), time-to-expiry (if dated), fees/spreads | No-arbitrage forward/spot parity uses cost-of-carry relationships; deviations only tradable if they exceed transaction + financing frictions. citeturn3search12turn3search28turn2search14turn6view4 |", "tags": []}
{"fragment_id": "F_R4_176_181", "source_id": "R4", "locator": "unified_crypto_research_master.md:L176-L181", "text": "| Liquidation cascade | liquidation prints intensity above threshold and concurrent OI drop + large return | liquidation volume (if available), open interest, mark price, returns, spread/depth | Liquidations and exchange risk backstops (insurance funds, ADL) create regime-like bursts in forced flow. citeturn4search10turn4search6turn4search22 |\n| Volatility shock | realized vol jump: \\(\\sigma_{\\text{rv}}(t_0)/\\text{MA}(\\sigma_{\\text{rv}})\\ge v_\\*\\) or implied–realized gap spike | OHLCV or trades, realized vol estimator, (optional) implied vol surface | Volatility regimes are persistent and can be modeled with regime-switching / change-point logic. citeturn3search2turn0search6turn6view4 |\n| Liquidity shock | spread widens + depth collapses: \\(s(t_0)\\uparrow\\), \\(D_{\\text{top}}(t_0)\\downarrow\\) | bid/ask, spread, depth at levels, trade/quote volume | Liquidity metrics (spread/depth/impact) are core microstructure state variables; deterioration drives slippage/impact. citeturn4search11turn2search15turn7search5 |\n| Structural break | breakpoint detected in returns/vol/liquidity process | returns series, vol series, liquidity series, breakpoint test outputs | Multiple structural change procedures formalize regime boundary detection in a statistically testable way. citeturn0search2turn0search6 |\n| Regime shift (latent) | posterior \\(P(r_t\\neq r_{t-1})\\ge \\pi_\\*\\) for an HMM/MSM | regime model state, filtered probs, observed features | Markov regime-switching models provide a tractable regime formalism for time series. citeturn3search2turn3search34 |\n| Microstructure imbalance shock | order-book imbalance beyond threshold | L2/L3 order book snapshots, imbalance metric | Order-book imbalance is a definable microstructure metric based on bid/ask queued quantities. citeturn4search11turn4search23 |", "tags": []}
{"fragment_id": "F_R4_182_191", "source_id": "R4", "locator": "unified_crypto_research_master.md:L182-L191", "text": "image_group{\"layout\":\"carousel\",\"aspect_ratio\":\"16:9\",\"query\":[\"perpetual swap funding rate diagram mark price index price\",\"crypto liquidation cascade chart open interest drop\",\"order book heatmap liquidity heatmap bid ask depth\"],\"num_per_query\":1}\n\n**Core data fields (normalized schema) required by the registry (minimum viable):**\n- **Identifiers**: `ts_event` (UTC), `instrument_id`, `venue_id`, `contract_type` (spot/perp/future), `quote_ccy`, `base_ccy`\n- **Prices**: `bid1`, `ask1`, `mid`, `last`, `mark_price`, `index_price` (if venue provides), `open/high/low/close` for bar resolutions\n- **Microstructure**: `spread = ask1-bid1`, `depth_L1..Lk` (bid/ask), `trades_count`, `trade_volume`, `vwap`\n- **Derivatives mechanics**: `funding_rate`, `funding_interval`, `premium_index` (or reconstructable proxy), `open_interest`, `liquidation_volume` (if available)\n- **Costs**: `maker_fee_bps`, `taker_fee_bps`, rebates, fee tier info (point-in-time); maker/taker definitions are venue-specific. citeturn5search12turn5search10turn5search14", "tags": []}
{"fragment_id": "F_R4_192_196", "source_id": "R4", "locator": "unified_crypto_research_master.md:L192-L196", "text": "Venue data-contract examples (to motivate the need for explicit fields):\n- On entity[\"company\",\"Binance\",\"crypto exchange\"] Futures, funding amount is computed as notional (mark × position size) × funding rate, with default 8-hour intervals; mark price is computed from index/last/funding/order-book inputs using a published formula. citeturn6view0turn6view2  \n- entity[\"company\",\"BitMEX\",\"crypto derivatives exchange\"] documents a funding mechanism composed of interest + premium/discount intended to keep the perpetual price near spot/index. citeturn2search4turn6view3  \n- entity[\"company\",\"Kraken\",\"crypto exchange\"] describes funding periodicity/payments as the mechanism that anchors perpetual prices to spot. citeturn2search28turn5search2", "tags": []}
{"fragment_id": "F_R4_197_223", "source_id": "R4", "locator": "unified_crypto_research_master.md:L197-L223", "text": "### Context deltas\n\nA **context delta** is a state transition \\(\\Delta x(t_0)\\) over a fixed “decision horizon” (e.g., last 5 minutes) that modifies the conditional distribution of outcomes given an event. The explicit goal is to avoid unconditional averaging and instead estimate \\( \\mathbb{E}[\\text{PnL} \\mid e, \\Delta x] \\). Markov switching and structural break literature motivates explicit regime/state representations. citeturn3search2turn0search6turn0search2\n\nDefine the context state vector at venue \\(v\\), instrument \\(i\\):\n\\[\nx_{i,v}(t) =\n\\Big[\n\\sigma_{\\text{rv}}(t),\\ \ns(t),\\ \nD_{\\text{top}}(t),\\ \n\\text{imbalance}(t),\\ \n\\text{volume}(t),\\ \n\\text{OI}(t),\\ \n\\text{funding}(t),\\ \n\\text{basis}(t)\n\\Big]\n\\]\nand context delta:\n\\[\n\\Delta x_{i,v}(t_0;\\tau)=x_{i,v}(t_0)-x_{i,v}(t_0-\\tau)\n\\]\n\nRegime labeling options (choose one, but make it deterministic and versioned):\n- **Breakpoint-based regimes**: detect structural changes in returns/vol/liquidity using multiple structural change methods; define regimes as segments between breakpoints. citeturn0search2turn0search6  \n- **Markov regime-switching**: fit a Markov switching model to returns/vol and use filtered probabilities to label regimes (e.g., low-vol vs high-vol). citeturn3search2turn3search34", "tags": []}
{"fragment_id": "F_R4_224_233", "source_id": "R4", "locator": "unified_crypto_research_master.md:L224-L233", "text": "### Invariants\n\nInvariants are constraints that should *approximately* hold absent frictions; they are used for (1) sanity checks, (2) generating candidate “mispricing” events, and (3) bounding expected profits by execution/financing frictions.\n\nKey invariants relevant to crypto market structure:\n- **Perp anchoring invariant (mechanism-level)**: perpetual funding is designed to anchor perpetual prices to an underlying spot/index, with funding transfers between longs/shorts computed from notional × funding rate (venue-defined). citeturn6view0turn6view3  \n- **Forward/spot parity (cost-of-carry)**: for an investment asset with no income and no storage costs, no-arbitrage forward pricing relates forward/futures price to spot via \\(F_0 = S_0 e^{rT}\\) (continuous compounding); income/yield modifies the carry term. citeturn3search12turn3search28  \n- **Triangular consistency (cross-rate)**: in currency-like markets, cross rates must align to prevent triangular arbitrage; implement as a product/ratio constraint with bounds widened by spreads/fees. citeturn4search8turn4search0  \n- **Put–call parity (options, if included)**: European put–call parity provides a replicating relationship; deviations are bounded by financing + transaction costs. citeturn4search13turn4search1", "tags": []}
{"fragment_id": "F_R4_234_235", "source_id": "R4", "locator": "unified_crypto_research_master.md:L234-L235", "text": "**Invariant enforcement rule:** treat violations as either (a) **data errors** (drop/repair) or (b) **candidate events** only if the violation magnitude exceeds conservative friction bounds (fees + spread + transfer/latency assumptions). The “bounds-first” principle prevents turning micro noise into spurious edges. citeturn8search2turn7search6turn2search15", "tags": []}
{"fragment_id": "F_R4_236_244", "source_id": "R4", "locator": "unified_crypto_research_master.md:L236-L244", "text": "### Labeling logic and pseudocode\n\nLabeling must be **event-driven** and **horizon-explicit**. For each event instance at \\(t_0\\), define a fixed label horizon \\(H\\) (in seconds/bars) and compute:\n\\[\ny(e;H) = \\ln\\frac{P(t_0+H)}{P(t_0)}\n\\]\nOptionally define **path-dependent labels** (e.g., max adverse excursion) for risk diagnostics; keep them out of the discovery-stage “edge” ranking unless you already enforce purging/embargo later. citeturn0search19turn8search1\n\n```python", "tags": []}
{"fragment_id": "F_R4_245_245", "source_id": "R4", "locator": "unified_crypto_research_master.md:L245-L245", "text": "# PSEUDO-CODE (discovery): registry-driven event labeling", "tags": []}
{"fragment_id": "F_R4_246_248", "source_id": "R4", "locator": "unified_crypto_research_master.md:L246-L248", "text": "# Assumes point-in-time feature computation, no future reads for features.\n\ndef compute_features(pt_data, t0, lookbacks):", "tags": []}
{"fragment_id": "F_R4_249_264", "source_id": "R4", "locator": "unified_crypto_research_master.md:L249-L264", "text": "# only use data with ts <= t0\n    feats = {}\n    feats[\"mid\"] = mid(pt_data.book[t0])\n    feats[\"spread\"] = pt_data.book[t0].ask1 - pt_data.book[t0].bid1\n    feats[\"depth_top\"] = pt_data.book[t0].bid_qty1 + pt_data.book[t0].ask_qty1\n    feats[\"rv\"] = realized_vol(pt_data.returns.window(end=t0, len=lookbacks[\"rv\"]))\n    feats[\"obi\"] = order_book_imbalance(pt_data.book[t0], levels=lookbacks[\"obi_levels\"])\n    feats[\"funding\"] = pt_data.funding.get_last(t0)          # perp only\n    feats[\"mark\"] = pt_data.mark.get_last(t0)                # if provided\n    feats[\"index\"] = pt_data.index.get_last(t0)              # if provided\n    feats[\"oi\"] = pt_data.open_interest.get_last(t0)         # if provided\n    feats[\"liq_vol\"] = pt_data.liquidations.sum(t0 - 300, t0) # last 5 min\n    return feats\n\ndef detect_events(feats, params):\n    events = []", "tags": []}
{"fragment_id": "F_R4_265_269", "source_id": "R4", "locator": "unified_crypto_research_master.md:L265-L269", "text": "# funding dislocation\n    if \"funding\" in feats:\n        z = zscore(feats[\"funding\"], params[\"funding_window\"])\n        if abs(z) >= params[\"funding_z_th\"]:\n            events.append({\"event_type\": \"FUNDING_DISLOCATION\", \"z\": z})", "tags": []}
{"fragment_id": "F_R4_270_272", "source_id": "R4", "locator": "unified_crypto_research_master.md:L270-L272", "text": "# liquidation cascade\n    if feats.get(\"liq_vol\", 0) >= params[\"liq_vol_th\"] and feats.get(\"oi\", 0) <= params[\"oi_drop_th\"]:\n        events.append({\"event_type\": \"LIQUIDATION_CASCADE\"})", "tags": []}
{"fragment_id": "F_R4_273_278", "source_id": "R4", "locator": "unified_crypto_research_master.md:L273-L278", "text": "# liquidity shock\n    if feats[\"spread\"] >= params[\"spread_th\"] and feats[\"depth_top\"] <= params[\"depth_th\"]:\n        events.append({\"event_type\": \"LIQUIDITY_SHOCK\"})\n    return events\n\ndef label_event(price_series, t0, H):", "tags": []}
{"fragment_id": "F_R4_279_281", "source_id": "R4", "locator": "unified_crypto_research_master.md:L279-L281", "text": "# labels are allowed to look forward, but must never feed into features\n    return log(price_series[t0 + H] / price_series[t0])", "tags": []}
{"fragment_id": "F_R4_282_290", "source_id": "R4", "locator": "unified_crypto_research_master.md:L282-L290", "text": "# main loop: generate event_instances dataset\nfor (instrument, venue) in universe:\n    for t0 in decision_times:\n        feats = compute_features(pt_data[(instrument, venue)], t0, lookbacks)\n        for ev in detect_events(feats, params):\n            y = label_event(pt_data[(instrument, venue)].mid_price, t0, H=params[\"label_horizon\"])\n            emit_event_instance(instrument, venue, t0, feats, ev, y)\n```", "tags": []}
{"fragment_id": "F_R4_291_294", "source_id": "R4", "locator": "unified_crypto_research_master.md:L291-L294", "text": "## Phase 2 — Validation\n\nValidation converts “interesting conditional returns” into **candidate edges** with explicit action rules, after-cost expectancy, statistical controls, and capacity limits.", "tags": []}
{"fragment_id": "F_R4_295_308", "source_id": "R4", "locator": "unified_crypto_research_master.md:L295-L308", "text": "### Candidate edge definition and conditional action rules\n\nA candidate edge is a tuple:\n\\[\nc = (\\text{event predicate }E,\\ \\text{context predicate }C,\\ \\text{action }A,\\ \\text{exit }X,\\ \\text{risk }R,\\ \\theta)\n\\]\nwhere \\(\\theta\\) are parameters bounded by pre-registered ranges.\n\nAction rules must be explicit about:\n- **Entry time**: \\(t_{\\text{enter}} = t_0 + \\delta\\) (to model detection + order placement latency)\n- **Entry mechanism**: market vs limit, single venue vs router, target participation rate\n- **Exit rule**: time-based \\(H\\), signal-based reversal, or risk-based stop\n- **Position sizing**: function of risk budget and capacity, not of ex-post performance", "tags": []}
{"fragment_id": "F_R4_309_323", "source_id": "R4", "locator": "unified_crypto_research_master.md:L309-L323", "text": "### After-cost expectancy with an explicit execution cost model\n\nUse an **implementation shortfall** style decomposition: compare “paper” decision price to realized execution, capturing explicit fees and implicit costs (spread, impact, delay, opportunity). This is standard transaction-cost accounting and is directly applicable to electronic markets. citeturn7search6turn2search15turn7search30\n\nDefine for each trade \\(j\\) (signed quantity \\(q_j\\), positive for buy):\n- Decision/arrival benchmark \\(p^{\\text{arr}}_j\\) (e.g., mid at signal time \\(t_0\\))\n- Execution price \\(p^{\\text{exec}}_j\\)\n- Fee rate \\(f_j\\) (maker/taker, notional-based; venue schedule is part of point-in-time data) citeturn5search12turn5search10turn5search14  \n\nImplementation shortfall (IS) in currency units:\n\\[\n\\text{IS} = \\sum_j q_j (p^{\\text{exec}}_j - p^{\\text{arr}}_j) + \\sum_j \\text{fees}_j\n\\]\nThis matches the “arrival price” framing used in futures TCA materials and broader execution literature. citeturn2search15turn7search6turn0search20", "tags": []}
{"fragment_id": "F_R4_324_337", "source_id": "R4", "locator": "unified_crypto_research_master.md:L324-L337", "text": "**Execution price model (deterministic, calibration-ready):**\n\\[\np^{\\text{exec}}(q,t)=m(t) + \\operatorname{sign}(q)\\left(\\frac{s(t)}{2} + \\text{slip}(q,t) + \\text{impact}(q,t)\\right)\n\\]\nwith:\n- \\(m(t)\\): mid price\n- \\(s(t)\\): spread\n- \\(\\text{slip}\\): short-horizon adverse selection + queue effects (can be modeled empirically per venue/order type)\n- \\(\\text{impact}\\): market impact, calibrated from historical executions or proxy models\n\nImpact modeling options (choose one, then validate it):\n- **Temporary/permanent impact optimal execution family**: foundational models separate temporary vs permanent impact and motivate cost terms used in execution simulators. citeturn0search20turn0search8turn0search0  \n- **Square-root impact scaling**: empirically, impact of large “metaorders” often scales approximately with \\(\\sqrt{Q/ADV}\\) (with volatility scaling), providing a capacity-aware penalty. citeturn7search20turn7search5turn7search0", "tags": []}
{"fragment_id": "F_R4_338_343", "source_id": "R4", "locator": "unified_crypto_research_master.md:L338-L343", "text": "A capacity-friendly parametric impact term:\n\\[\n\\text{impact}(Q) = \\eta\\ \\sigma_d\\ \\sqrt{\\frac{Q}{ADV_d}}\n\\]\nwhere \\(\\sigma_d\\) and \\(ADV_d\\) are daily volatility and daily traded value (or volume) proxies computed point-in-time (use rolling estimates). citeturn7search20turn7search5turn6view4", "tags": []}
{"fragment_id": "F_R4_344_351", "source_id": "R4", "locator": "unified_crypto_research_master.md:L344-L351", "text": "### Stability across time slices and regimes\n\nValidation must show that expectancy remains positive (after costs) across:\n- **Time slices** (e.g., yearly/quarterly)\n- **Regimes** (low/high vol, liquidity-stressed, funding-stressed), defined by the regime model specified in Discovery. citeturn3search2turn0search6  \n\nBecause labels in event-driven trading often overlap in time (multi-bar horizons), naive k-fold splits leak information. Use **purging and embargoing**: remove training samples whose label horizons overlap the test window, with an added buffer (embargo). citeturn0search19turn0search3turn0search7", "tags": []}
{"fragment_id": "F_R4_352_359", "source_id": "R4", "locator": "unified_crypto_research_master.md:L352-L359", "text": "### Multiplicity-adjusted significance\n\nDiscovery + tuning implicitly creates many hypotheses. Correct for multiple testing using at least one of:\n- **False discovery rate (FDR) control** via the step-up procedure (sort p-values \\(p_{(k)}\\), find largest \\(k\\) with \\(p_{(k)} \\le \\frac{k}{m}\\alpha\\)). citeturn0search13turn0search1  \n- **Reality-check style bootstrap for data snooping** when comparing many candidate rules to a benchmark on the same sample (addresses “best-of-many” selection). citeturn8search2turn8search25  \n\nSelection-bias in optimized backtests is also addressed by adjusted performance statistics such as the **Deflated Sharpe Ratio** when many trials/parameter sets are tested. citeturn3search3turn3search7", "tags": []}
{"fragment_id": "F_R4_360_373", "source_id": "R4", "locator": "unified_crypto_research_master.md:L360-L373", "text": "### Density and capacity constraints\n\nA candidate that “works” at tiny size but fails at realistic size is not a tradable edge. Enforce:\n- **Participation constraint**: for execution horizon \\(T_{\\text{exec}}\\), require\n\\[\n\\rho = \\frac{|Q|}{V(t_0, t_0+T_{\\text{exec}})} \\le \\rho_{\\max}\n\\]\nwith \\(V\\) measured volume (or dollar volume) in the execution window.\n- **Impact scaling constraint**: projected impact must not consume expectancy:\n\\[\n\\mathbb{E}[\\text{edge}] - \\mathbb{E}[\\text{costs}] \\ge \\epsilon_{\\min} \\quad \\text{where costs include } \\text{impact}(Q)\n\\]\nSquare-root impact scaling provides a direct way to quantify how costs increase with size. citeturn7search20turn7search0turn7search5", "tags": []}
{"fragment_id": "F_R4_374_389", "source_id": "R4", "locator": "unified_crypto_research_master.md:L374-L389", "text": "### Validation workflow and rejection criteria\n\n**Workflow (deterministic):**\n1. Build event instances from the registry (fixed version + fixed dataset hash).\n2. For each candidate rule family, pre-register parameter bounds \\(\\theta \\in [\\theta_{\\min},\\theta_{\\max}]\\).\n3. Estimate after-cost returns using the explicit execution model; compute expectancy and distributional stats.\n4. Evaluate across slices and regimes; compute multiplicity-adjusted significance.\n5. Apply capacity filter and stress cost assumptions (spread widening, reduced depth).\n6. Freeze passing candidates into the Blueprint YAML.\n\n**Minimum rejection criteria (implementation-ready):**\n- After-cost mean return \\(\\le 0\\) in aggregate **or** in any “core regime” bucket (e.g., top-2 most frequent regimes).\n- Fails FDR/reality-check threshold at target \\(\\alpha\\) after accounting for tested hypotheses. citeturn0search13turn8search2  \n- Capacity at target capital implies participation/impact costs that erase ≥X% of expectancy.\n- Performance collapses under modest execution perturbations consistent with liquidity shocks (spread/depth deterioration). citeturn2search15turn4search11turn7search5", "tags": []}
{"fragment_id": "F_R4_390_425", "source_id": "R4", "locator": "unified_crypto_research_master.md:L390-L425", "text": "## Blueprint\n\nBlueprint is the “contract” that makes research deterministic, reproducible, and auditable. It encodes all identifiers, datasets, bounds, and run controls as executable specs (not prose). Data endpoints and mechanics (e.g., funding schedules, candlestick identity) are treated as part of the dataset contract. citeturn5search1turn6view0turn5search9\n\n```yaml\nblueprint_version: \"1.0.0\"\n\nevent_registry:\n  event_registry_version: \"0.3.0\"\n  registry_hash_sha256: \"<sha256_of_registry_yaml>\"\n  definitions_source_notes:\n    - \"Perp funding/mark/index definitions are venue-specific; store raw fields + normalization.\"\n    - \"Funding payment = notional * funding_rate (venue-defined).\"\n\ndataset:\n  dataset_id: \"crypto_research_snapshot_2026-02-18\"\n  dataset_hash_sha256: \"<sha256_of_canonical_export>\"\n  canonicalization:\n    format: \"parquet\"\n    sort_keys: [\"venue_id\", \"instrument_id\", \"ts\"]\n    tz: \"UTC\"\n    null_policy: \"explicit_nulls_preserved\"\n  sources:\n    market_data:\n      - type: \"trades\"\n        fields: [\"ts\", \"price\", \"qty\", \"side\"]\n      - type: \"l2_book\"\n        fields: [\"ts\", \"bid_px_1\", \"bid_qty_1\", \"ask_px_1\", \"ask_qty_1\", \"depth_levels_k\"]\n      - type: \"mark_index\"\n        fields: [\"ts\", \"mark_price\", \"index_price\"]\n      - type: \"funding_open_interest\"\n        fields: [\"ts\", \"funding_rate\", \"funding_interval\", \"open_interest\"]\n    cost_data:\n      - type: \"fee_schedule_point_in_time\"\n        fields: [\"ts_effective\", \"maker_fee_bps\", \"taker_fee_bps\", \"rebates\", \"tier_rule_id\"]", "tags": []}
{"fragment_id": "F_R4_426_465", "source_id": "R4", "locator": "unified_crypto_research_master.md:L426-L465", "text": "candidate_id:\n  schema: \"C-{event_type}-{instrument_id}-{venue_id}-{horizon}-{direction}-{param_hash}-{spec_version}\"\n  example: \"C-FUNDING_DISLOCATION-BTCUSDT-PERP-VENUEA-H8H-LONG-<phash>-v1\"\n  determinism:\n    param_hash: \"sha256(json_canonical(params))\"\n    code_commit: \"<git_commit>\"\n    rng_seed: 0\n\nparameter_bounds:\n  funding_dislocation:\n    funding_window_hours: [24, 720]\n    funding_z_th: [1.5, 5.0]\n    premium_proxy_th: [0.0001, 0.01]\n    entry_delay_seconds: [0, 10]\n  liquidation_cascade:\n    liq_vol_th_percentile: [0.90, 0.999]\n    oi_drop_th_percentile: [0.80, 0.99]\n    entry_delay_seconds: [0, 10]\n  liquidity_shock:\n    spread_th_bps: [2, 200]\n    depth_drop_percentile: [0.01, 0.20]\n\nvalidation_spec:\n  in_sample:\n    split_method: \"purged_walk_forward\"\n    embargo_fraction: 0.05\n    min_train_days: 365\n    test_block_days: 30\n  out_of_sample:\n    locked_params: true\n    no_refitting: true\n  multiplicity_control:\n    method: \"FDR_step_up\"\n    alpha: 0.05\n  execution_cost_model:\n    fees: \"maker_taker_point_in_time\"\n    spread: \"half_spread_crossing\"\n    impact: \"sqrt(Q/ADV)_vol_scaled\"\n    slippage: \"empirical_bucket_model\"", "tags": []}
{"fragment_id": "F_R4_466_473", "source_id": "R4", "locator": "unified_crypto_research_master.md:L466-L473", "text": "reproducibility_checklist:\n  - \"All features computed with ts <= decision_ts.\"\n  - \"All fee schedules are point-in-time (ts_effective <= decision_ts).\"\n  - \"All splits are time-ordered; purging/embargo applied for overlapping horizons.\"\n  - \"Store: dataset_hash, registry_hash, code_commit, container_digest, run_config_hash.\"\n  - \"Store full random seeds and any sampling bootstrap seeds.\"\n```", "tags": []}
{"fragment_id": "F_R4_474_477", "source_id": "R4", "locator": "unified_crypto_research_master.md:L474-L477", "text": "## Strategy\n\nStrategy turns each validated candidate into a single-strategy research artifact with a clean train/test split, sensitivity surfaces, and stress tests. Overfitting risk rises with parameter search; therefore the strategy workflow must report robustness, not just point estimates. citeturn3search3turn8search2turn0search13", "tags": []}
{"fragment_id": "F_R4_478_485", "source_id": "R4", "locator": "unified_crypto_research_master.md:L478-L485", "text": "### Single-strategy backtest specs\n\n**Backtest unit of simulation:** event-triggered orders with explicit entry latency, order type, and execution simulator. Execution quality is measured using implementation shortfall framing against an arrival benchmark. citeturn2search15turn7search6turn0search20\n\n**Train/test split (deterministic):**\n- Use **time-ordered** splits; never random shuffle. citeturn8search1turn0search19  \n- Default: **purged walk-forward** with embargo (defined in Blueprint) to avoid label overlap leakage. citeturn0search19turn0search3", "tags": []}
{"fragment_id": "F_R4_486_493", "source_id": "R4", "locator": "unified_crypto_research_master.md:L486-L493", "text": "### Parameter sweeps and sensitivity maps\n\nFor each parameter in \\(\\theta\\), compute a sensitivity grid and report:\n- Heatmap data: \\(\\text{Sharpe}_{\\text{after-cost}}(\\theta)\\), hit-rate, turnover, max drawdown, tail loss\n- *Stability score*: fraction of parameter neighborhood with positive after-cost expectancy and significant under multiplicity control\n\nWhen many parameter sets are tried, report a selection-bias-aware statistic (e.g., deflated Sharpe) or a data-snooping-aware procedure alongside conventional metrics. citeturn3search3turn8search2", "tags": []}
{"fragment_id": "F_R4_494_501", "source_id": "R4", "locator": "unified_crypto_research_master.md:L494-L501", "text": "### Stress tests\n\nStress tests must map directly to known crypto venue failure modes and microstructure deterioration:\n\n- **Liquidity shock**: multiply spread by \\(k_s\\), reduce top-of-book depth by \\(k_d\\), and re-simulate fills (captures deterioration in post-trade costs). citeturn4search11turn7search5turn2search15  \n- **Volatility expansion**: scale realized volatility used in impact model; square-root impact law implies higher volatility increases impact cost for a given \\(Q/ADV\\). citeturn7search20turn7search0  \n- **Exchange outage / forced deleveraging regime**: simulate a no-fill window for a venue and/or forced position reduction; exchanges may use insurance funds and auto-deleveraging mechanisms in extreme conditions. citeturn4search10turn4search6turn4search22", "tags": []}
{"fragment_id": "F_R4_502_523", "source_id": "R4", "locator": "unified_crypto_research_master.md:L502-L523", "text": "### Output performance table and robustness diagnostics (template)\n\n**Performance table schema (populate with your computed values):**\n\n| Metric | In-sample | Out-of-sample | Notes |\n|---|---:|---:|---|\n| Mean after-cost return per trade |  |  | After all modeled costs |\n| Sharpe (after-cost) |  |  | Also report deflated Sharpe if many trials |\n| Hit rate |  |  | Conditional on event triggers |\n| Avg / p95 implementation shortfall |  |  | Arrival-price benchmark |\n| Turnover (notional/day) |  |  | Drives cost + capacity |\n| Max drawdown |  |  | Use equity curve from simulated fills |\n| Capacity at \\(\\rho_{\\max}\\) |  |  | Participation/impact constrained |\n| Regime stability score |  |  | Fraction of regimes with positive expectancy |\n| Stress delta (liq shock) |  |  | Degradation under spread/depth shock |\n| Stress delta (venue outage) |  |  | Exposure to venue availability |\n\nRobustness diagnostics to store per strategy:\n- Multiplicity-adjusted significance result (FDR / reality-check outcome). citeturn0search13turn8search2  \n- Parameter neighborhood stability and “edge half-life” across rolling windows.  \n- Execution sensitivity: outcomes under fee tier changes, maker vs taker mix (maker/taker schedules are venue-defined). citeturn5search12turn5search14", "tags": []}
{"fragment_id": "F_R4_524_527", "source_id": "R4", "locator": "unified_crypto_research_master.md:L524-L527", "text": "## Portfolio\n\nPortfolio combines validated strategies into a single allocation + execution system with correlation control, capacity modeling, and cross-venue execution simulation. Portfolio optimization with transaction costs and constraints is naturally expressed in convex optimization form when costs/constraints are convex (or approximated as such). citeturn7search3turn1search3turn7search11", "tags": []}
{"fragment_id": "F_R4_528_544", "source_id": "R4", "locator": "unified_crypto_research_master.md:L528-L544", "text": "### Allocation method\n\nRepresent each strategy \\(s\\) by an after-cost return series \\(r_s(t)\\) and (optional) a forecast \\(\\hat{\\mu}_s(t)\\). Portfolio weights \\(w(t)\\) can be set by one of these deterministic methods (choose one as the “primary,” keep others as ablations):\n\n**Convex optimization (cost-aware, constraint-first):**\n\\[\n\\max_{w}\\ \\hat{\\mu}^\\top w - \\lambda w^\\top \\Sigma w - \\gamma \\, \\text{TC}(w, w_{-1})\n\\]\nsubject to bounds:\n\\[\nw_{\\min}\\le w \\le w_{\\max},\\quad \\|w\\|_1 \\le L_{\\max},\\quad \\text{exposure/venue caps}\n\\]\nConvex optimization is the standard framework for efficiently solving such constrained problems, and portfolio optimization with linear/fixed transaction costs has established convex formulations/relaxations. citeturn1search3turn7search3turn7search11  \n\n**Risk parity / equal risk contribution (forecast-light):**\nSolve for weights such that each component contributes equally to total portfolio risk (ex-ante), reducing reliance on fragile mean estimates. citeturn1search0turn1search34", "tags": []}
{"fragment_id": "F_R4_545_547", "source_id": "R4", "locator": "unified_crypto_research_master.md:L545-L547", "text": "**Growth-optimal (log-utility) sizing, capped:**\nUse growth-optimal sizing logic as an input, then cap per-strategy leverage/exposure to control estimation error; the original growth-optimal principle maximizes expected log wealth under repeated betting assumptions. citeturn1search1turn1search21", "tags": []}
{"fragment_id": "F_R4_548_560", "source_id": "R4", "locator": "unified_crypto_research_master.md:L548-L560", "text": "### Correlation control and covariance estimation\n\n**Rolling covariance** is necessary but noisy; improve stability with **shrinkage** toward a structured target (e.g., identity) to obtain a better-conditioned estimator in higher dimensions. citeturn1search12turn1search4turn1search24  \n\nRegime-conditioned correlation:\n- Compute regimes \\(r(t)\\) from the Discovery regime model.\n- Estimate \\(\\Sigma^{(k)}\\) within each regime \\(k\\).\n- Allocate under a “worst-regime” or weighted-regime risk objective:\n\\[\n\\min_{w} \\ \\max_k \\ w^\\top \\Sigma^{(k)} w\n\\]\nThis explicitly controls correlation spikes in stress regimes instead of assuming stationarity. citeturn3search2turn0search6turn1search3", "tags": []}
{"fragment_id": "F_R4_561_574", "source_id": "R4", "locator": "unified_crypto_research_master.md:L561-L574", "text": "### Capacity modeling\n\nCapacity is computed per strategy and then aggregated with portfolio-level constraints.\n\nPer strategy \\(s\\):\n1. Determine feasible participation \\(\\rho_{\\max}\\) and execution window \\(T_{\\text{exec}}\\).\n2. Estimate \\(ADV\\) and \\(\\sigma\\) in the traded venue/instrument.\n3. Use an impact model (e.g., square-root) to map capital \\(K\\) to expected impact cost.\n4. Define capacity \\(K^\\*\\) as largest \\(K\\) satisfying:\n\\[\n\\mathbb{E}[\\text{edge}(K)] - \\mathbb{E}[\\text{cost}(K)] \\ge \\epsilon_{\\min}\n\\]\nSquare-root impact scaling provides a concrete way to do step (3). citeturn7search20turn7search0turn7search5", "tags": []}
{"fragment_id": "F_R4_575_586", "source_id": "R4", "locator": "unified_crypto_research_master.md:L575-L586", "text": "### Execution simulation across venues\n\nA minimal cross-venue simulator must model:\n- **Order type**: market vs limit (with queue uncertainty)\n- **Fee schedule**: maker/taker notional fees (point-in-time, tier-aware) citeturn5search12turn5search10  \n- **Benchmark**: arrival price implementation shortfall accounting citeturn2search15turn7search6  \n- **Market impact**: size-dependent cost term citeturn0search20turn7search20  \n- **Venue risk events**: liquidation/ADL/outage conditions as scenario toggles citeturn4search10turn4search22  \n\nExecution inputs derived from venue documentation and market data contracts:\n- Candlestick and index-price kline identity rules (key for deterministic bar building) are defined in venue APIs. citeturn5search1turn5search9", "tags": []}
{"fragment_id": "F_R4_587_596", "source_id": "R4", "locator": "unified_crypto_research_master.md:L587-L596", "text": "### Portfolio risk controls\n\nRisk controls are deterministic gates that operate on point-in-time observables:\n- **Exposure caps**: per instrument, per venue, per strategy, and gross/net leverage.\n- **Liquidity gates**: if spread/depth exceed thresholds (liquidity shock context), reduce or halt new risk. citeturn4search11turn7search5  \n- **Mechanism-aware venue risk gates**: monitor liquidation/ADL risk periods; exchanges describe ADL as an emergency mechanism in extreme cases. citeturn4search10turn4search22  \n- **Rebalance friction control**: incorporate transaction costs directly into allocation (convex cost-aware optimization / risk parity with turnover penalty). citeturn7search3turn7search11turn1search0  \n\n**Next deeper angle of analysis:** once registry + validation are wired, quantify how *taxonomy breadth* (number of event types and parameter sweeps) changes false discovery rates and capacity-adjusted opportunity set size—then use that to set an explicit “research budget” (maximum hypotheses per dataset snapshot) under your chosen multiplicity and data-snooping controls. citeturn8search2turn0search13turn3search3", "tags": []}
{"fragment_id": "F_R4_597_598", "source_id": "R4", "locator": "unified_crypto_research_master.md:L597-L598", "text": "---", "tags": []}
{"fragment_id": "F_R4_599_608", "source_id": "R4", "locator": "unified_crypto_research_master.md:L599-L608", "text": "## FULL SOURCE [R2]\n\nFull‑Stack Research Framework for Systematic Crypto Portfolios\n1 System Architecture\n1.1 End‑to‑end components\n\nA small, systematic crypto operator requires a pipeline that transforms raw market events into validated, capacity‑aware strategies and finally allocates them to a portfolio. The architecture must be point‑in‑time (PIT) with no look‑ahead; all features at decision time must use only data available at time $t_0$; labels use data after $t_0$ but are never leaked into features. Components include:\n\nIngestion/Canonicalization – Collect raw trade, order book and funding data from multiple venues (CEX mandatory; DEX optional). Canonicalize into a time‑sorted event stream with uniform identifiers and time stamps (e.g., UTC timestamps truncated/rounded to the exchange’s minimum latency). Store raw fields for spot and perpetual contracts (see §2). Include venue‑specific fee schedules and tier changes as time‑stamped records.", "tags": []}
{"fragment_id": "F_R4_609_612", "source_id": "R4", "locator": "unified_crypto_research_master.md:L609-L612", "text": "PIT feature engine – Compute features only from events with timestamps ≤ $t_0$. For example, order‑book imbalance $\\text{Imb}_t=(V^b_t - V^a_t)/(V^b_t + V^a_t)$ as the difference between best‑bid and best‑ask volumes divided by their sum. Funding‑related features (premium index, mark‑index deviation, etc.) rely on canonical definitions (§2). Derive microstructure features such as spreads, depth, volume buckets, realized volatility, liquidation count, OI changes, etc.\n\nEvent registry – A versioned database that defines event types (funding dislocation, basis dislocation, liquidation cascade, volatility shock, liquidity shock, structural break, regime shift, microstructure imbalance shock). Each event type has deterministic triggers expressed as functions of PIT features (e.g., funding dislocation when premium index + interest exceeds a threshold; order‑book imbalance shock when $|\\text{Imb}_t|>0.6$). Each registry entry contains the required PIT fields and triggers, the look‑back horizon for context, and a unique ID.", "tags": []}
{"fragment_id": "F_R4_613_618", "source_id": "R4", "locator": "unified_crypto_research_master.md:L613-L618", "text": "Labeling module – For each event $e$ at time $t_0$, compute horizon‑explicit labels $y(e,H)=\\log(P_{t_0+H}/P_{t_0})$ with $P$ using spot or mark price; optionally compute path diagnostics (max drawdown, realized volatility). The labeling uses data strictly after $t_0$.\n\nValidation/backtesting engine – Evaluate candidate strategies under realistic execution and capacity constraints (§4). Use purged and embargoed cross‑validation to avoid look‑ahead and overlapping horizons. Implement deterministic simulation of market/IOC orders with explicit and implicit costs (half‑spread crossing, slippage buckets, square‑root impact cost). Version run parameters (code commit hash, container digest, dataset snapshot hash, RNG seeds) to ensure deterministic replay.\n\nPortfolio allocator – Combine validated strategies into a portfolio subject to capacity, liquidity and risk gates (§6). Use a cost‑aware optimisation (e.g., convex risk parity or turnover‑penalised mean‑variance) that incorporates expected after‑cost returns and covariance. Enforce deterministic risk gates (exposure caps, venue‑mechanism limits, liquidity gates).", "tags": []}
{"fragment_id": "F_R4_619_624", "source_id": "R4", "locator": "unified_crypto_research_master.md:L619-L624", "text": "Execution simulator – Simulate order execution at various venues, modelling maker/taker fees, latency, partial fills and slippage. Use point‑in‑time fee schedules and square‑root impact models: the market impact of a meta‑order of size $n$ shares with volatility $\\sigma$ and daily turnover $\\nu$ is $\\Delta P = c,\\sigma,\\sqrt{n/\\nu}$. Include participation constraints: participation rate $\\rho=|Q|/V(t_0, t_0+T_{\\mathrm{exec}})$ must be ≤ $\\rho_{\\max}$.\n\nReporting/audit – Generate run manifests with dataset hashes, registry versions, strategy parameters, and results. Store result tables with after‑cost returns, risk, capacity, sensitivity maps and stress‑test diagnostics. Provide acceptance tests for “no future reads” (fail if a feature references $t>t_0$), and deterministic replay tests (re‑run pipeline with identical seeds yields identical output).\n\n1.2 Deterministic replay and versioning", "tags": []}
{"fragment_id": "F_R4_625_636", "source_id": "R4", "locator": "unified_crypto_research_master.md:L625-L636", "text": "To enforce reproducibility and auditability:\n\nDataset snapshot – Each dataset (spot, perp, funding, order book) is versioned by a snapshot ID and hash. Snapshots are immutable; new data are appended as new versions.\n\nEvent registry version – Each registry has a version number and a hash of event definitions. Changing triggers increments the version.\n\nCode and environment – Record the git commit ID, container image digest, and library versions. Freeze external dependencies.\n\nRun manifest – Create a YAML/JSON manifest capturing dataset versions, registry version, parameter grid, cost model configuration, capacity constraints, RNG seeds and run timestamp. Compute a run hash from these fields.\n\nAcceptance tests – Unit tests assert that (a) no feature uses data after the decision timestamp; (b) repeated runs with the same manifest produce identical outputs; (c) event triggers produce the same set of events across runs.", "tags": []}
{"fragment_id": "F_R4_637_685", "source_id": "R4", "locator": "unified_crypto_research_master.md:L637-L685", "text": "2 Data Layer & Venue Contracts\n2.1 Normalized schema\n\nA unified schema should support spot instruments (BTC, ETH, stablecoins) and perpetual contracts. Each record contains:\n\nField\tType\tDescription\nts\tdatetime (UTC)\tevent timestamp (microsecond resolution)\nvenue_id\tstring\texchange identifier (e.g., Binance, Coinbase, Bybit)\ninstrument_id\tstring\tunique symbol (e.g., BTC‑USDT spot or BTC‑PERP)\ntype\tenum {trade, book, funding, index, mark, oi, liquidation, fee_change}\tcategory of event\nprice_bid, price_ask\tfloat\tbest bid/ask price (spot or perp)\nprice_last\tfloat\tlast traded price\nprice_mid\tfloat\tmid‑price (mean of best bid and ask)\nprice_mark\tfloat (perps)\tmark price used for PnL and liquidation; computed from index price + funding basis\nprice_index\tfloat (perps)\tindex price: weighted average of spot prices across venues\nspread\tfloat\tbid–ask spread (ask – bid)\ndepth_bid, depth_ask\tfloat\taggregated volume at best bid/ask\nobi (order‑book imbalance)\tfloat\t$(V^b_t - V^a_t)/(V^b_t + V^a_t)$\nvolume_traded\tfloat\ttrade volume in base currency\nopen_interest\tfloat (perps)\ttotal open interest\nfunding_rate\tfloat (perps)\tperiodic funding rate; positive when longs pay shorts\npremium_index\tfloat (perps)\tdifference between mark and index price; formula: \n𝑃\n=\nmax\n⁡\n(\n0\n,\nImpactBid\n−\nIndex\n)\n−\nmax\n⁡\n(\n0\n,\nIndex\n−\nImpactAsk\n)\nIndex\nP=\nIndex\nmax(0,ImpactBid−Index)−max(0,Index−ImpactAsk)\n\t​", "tags": []}
{"fragment_id": "F_R4_686_698", "source_id": "R4", "locator": "unified_crypto_research_master.md:L686-L698", "text": "funding_interval\tinteger (minutes)\tinterval at which funding is exchanged (e.g., 8 h)\nfee_tiers\tobject\tmaker/taker fee schedule applicable at time ts\nliquidations\tinteger\tnumber of liquidation events in interval\noi_change\tfloat\tchange in open interest\nevent_metadata\tJSON\traw fields not covered above (e.g., impact bid/ask price, index constituents, instrument‑specific parameters)\n2.2 Venue contract handling\n\nRaw fields + normalization – Store raw venue messages (trade, order book changes, funding updates). Apply venue‑specific normalization to unify naming conventions and decimals; do not derive derived values (e.g., funding rate or mark price) from formulas; instead, store the values published by the venue along with reference formulas/metadata for reproducibility.\n\nMaker/taker fees – Capture the tiered fee schedule as a time‑series table with fields: ts_start, ts_end, tier, maker_fee, taker_fee. Link to accounts via volume tiers.", "tags": []}
{"fragment_id": "F_R4_699_709", "source_id": "R4", "locator": "unified_crypto_research_master.md:L699-L709", "text": "Perp anchoring & carry parity bounds – Check that the mark price is close to the index price adjusted for funding basis. A large deviation indicates a candidate event (funding dislocation). For a fair perpetual, $\\text{mark price} \\approx \\text{index price}+\\text{funding basis}$.\n\nTriangular consistency – For multi‑asset pairs, ensure that price relationships (e.g., BTC/USDT × ETH/BTC = ETH/USDT) hold within tolerance; flag arbitrage events when broken.\n\nBounds‑first gating – Each data feed enters through a bounds check: fields must lie within plausible ranges (e.g., spreads ≥ 0, funding rates within ±5 bp per interval). Violations may indicate data errors (to be cleaned) or extreme market events. Decision rules should classify observations failing bounds but meeting event triggers as candidate events requiring manual review.\n\n3 Phase 1 — Discovery\n3.1 Versioned event registry\n\nDefine a schema for the event registry. Each entry has:", "tags": []}
{"fragment_id": "F_R4_710_731", "source_id": "R4", "locator": "unified_crypto_research_master.md:L710-L731", "text": "event_id: unique identifier\nname: string\ntrigger_logic: expression using PIT features\ncontext_fields: list of PIT fields needed\nlookback: duration to compute context state vector x_{i,v}(t)\nlabel_horizons: list of forecast horizons H (e.g., [1h, 4h, 1d])\nnotes: description and references\n\n\nEvent types (examples):\n\nID\tEvent type\tTrigger example\nFND_DISLOC\tFunding dislocation\tPremium index + interest rate > threshold or < –threshold\nBASIS_DISLOC\tSpot–perp basis shock\t(Perp mark – spot index)/spot index beyond ±k * roll yield\nLIQ_CASCADE\tLiquidation cascade\tNumber of liquidations in a short window > quantile threshold; OI drops sharply\nVOL_SHOCK\tVolatility shock\tRealized volatility > percentile; implied vol skew jumps\nLIQ_SHOCK\tLiquidity shock\tSpread × depth ratio increases; order‑book imbalance absolute value > 0.6\nSTRUCT_BREAK\tStructural break\tStatistical test (e.g., Chow test) signals break point in price/funding/regime\nREGIME_SHIFT\tLatent regime shift\tHidden Markov model or Markov‑switching detection of new regime\nMICRO_IMBAL\tMicrostructure imbalance\tOrder‑book imbalance crosses regimes; queue imbalance persists beyond look‑back period\n3.2 Context state vector and labeling", "tags": []}
{"fragment_id": "F_R4_732_741", "source_id": "R4", "locator": "unified_crypto_research_master.md:L732-L741", "text": "For each instrument $i$ and venue $v$ at time $t$, define a context state vector $x_{i,v}(t)$ containing features such as spreads, depth, order‑book imbalance, realized volatility, funding rates, premium index, open interest, and cross‑asset signals. Also compute the delta $\\Delta x(t_0;\\tau) = x(t_0) - x(t_0 - \\tau)$ for multiple look‑back windows ($\\tau$ may vary from minutes to days). The event registry specifies which features to compute.\n\nLabel each event $e$ with horizon‑explicit labels:\n\nPrice label: $y(e;H) = \\log(P_{t_0+H}/P_{t_0})$ using the mark price for perps or mid price for spot.\n\nPath diagnostics: maximum adverse excursion, maximum favourable excursion, realized volatility and volume during $[t_0, t_0+H]$.\n\nRegime label: assign deterministic regimes by break point detection or Markov‑switching (e.g., low‑vol, high‑vol, trending). The context features feed into this classification.", "tags": []}
{"fragment_id": "F_R4_742_742", "source_id": "R4", "locator": "unified_crypto_research_master.md:L742-L742", "text": "3.3 Pseudocode for PIT feature computation", "tags": []}
{"fragment_id": "F_R4_743_743", "source_id": "R4", "locator": "unified_crypto_research_master.md:L743-L743", "text": "# Inputs: event time t0, lookback window L, raw time-sorted data D", "tags": []}
{"fragment_id": "F_R4_744_746", "source_id": "R4", "locator": "unified_crypto_research_master.md:L744-L746", "text": "# Output: feature vector features[t0] without lookahead\n\ndef compute_features(t0, L, D):", "tags": []}
{"fragment_id": "F_R4_747_748", "source_id": "R4", "locator": "unified_crypto_research_master.md:L747-L748", "text": "# slice data up to decision time\n    data_past = D[D.ts <= t0]", "tags": []}
{"fragment_id": "F_R4_749_751", "source_id": "R4", "locator": "unified_crypto_research_master.md:L749-L751", "text": "# compute aggregated microstructure features\n    past_window = data_past[data_past.ts >= t0 - L]\n    features = {}", "tags": []}
{"fragment_id": "F_R4_752_755", "source_id": "R4", "locator": "unified_crypto_research_master.md:L752-L755", "text": "# order book imbalance at t0\n    Vb = past_window.last().depth_bid\n    Va = past_window.last().depth_ask\n    features['obi'] = (Vb - Va) / (Vb + Va)", "tags": []}
{"fragment_id": "F_R4_756_758", "source_id": "R4", "locator": "unified_crypto_research_master.md:L756-L758", "text": "# spread, depth, realized vol\n    features['spread'] = past_window.last().price_ask - past_window.last().price_bid\n    features['depth_ratio'] = (Vb + Va) / max(1e-9, past_window['volume_traded'].rolling(L).sum())", "tags": []}
{"fragment_id": "F_R4_759_761", "source_id": "R4", "locator": "unified_crypto_research_master.md:L759-L761", "text": "# funding and premium\n    features['funding_rate'] = past_window.last().funding_rate\n    features['premium_index'] = past_window.last().premium_index", "tags": []}
{"fragment_id": "F_R4_762_784", "source_id": "R4", "locator": "unified_crypto_research_master.md:L762-L784", "text": "# realized volatility over window\n    returns = np.log(past_window.price_mark).diff().dropna()\n    features['realized_vol'] = returns.std() * sqrt(len(returns))\n    return features\n\n\nThis pattern ensures that only data with ts ≤ t0 are used. Labels are computed in a separate pass using future data.\n\n4 Phase 2 — Validation\n4.1 Candidate edge tuple\n\nEach candidate strategy is defined by a tuple $c=(E,C,A,X,R,\\theta)$ where:\n\nE – event type from registry.\n\nC – context conditions (e.g., region of order‑book imbalance, funding extremes, regime assignment).\n\nA – action (long, short, hedge, do nothing) with entry delay and order type (market/IOC; maker orders optional due to small size). Order quantity may be fixed or proportional to historical volatility/volume.\n\nX – instrument(s) and venue(s) to trade.\n\nR – risk management rules (stop loss, take profit, time stop, position limit, notional cap).", "tags": []}
{"fragment_id": "F_R4_785_798", "source_id": "R4", "locator": "unified_crypto_research_master.md:L785-L798", "text": "θ – parameter set (thresholds, delays, scaling factors) bounded by pre‑registered ranges.\n\nAll parameters and event types must be pre‑registered before research to avoid p‑hacking. Parameter ranges are specified in the run manifest.\n\n4.2 Splitting and cross‑validation\n\nUse purged and embargoed walk‑forward splitting to avoid overlap and look‑ahead. Purging removes from training any observation whose time interval overlaps the label formation window of test observations. Embargoing applies a temporal buffer after each test fold to prevent spill‑over effects. For each horizon $H$, split the event timeline into $k$ sequential folds. Training uses earlier folds excluding purged intervals; test uses the current fold.\n\n4.3 Execution realism and cost modelling\n\nImplementation shortfall is measured relative to the mid price at decision time. The execution simulator applies:\n\nExplicit fees – Maker/taker fees at the chosen venue (point‑in‑time schedule). If market orders are used, apply taker fee; if maker orders are used, apply maker fee but account for fill probability and opportunity cost.", "tags": []}
{"fragment_id": "F_R4_799_810", "source_id": "R4", "locator": "unified_crypto_research_master.md:L799-L810", "text": "Spread crossing – Market orders cross half the bid–ask spread. Maker orders attempt to earn the spread but may suffer partial fills.\n\nSlippage buckets – Model slippage as a function of order size relative to recent traded volume; calibrate from historical market impact data.\n\nSquare‑root impact – For larger orders, apply the square‑root law: $\\Delta P = c \\sigma \\sqrt{n/\\nu}$. Parameter $c$ is estimated from historical impact curves; $\\sigma$ is recent volatility; $\\nu$ is average daily turnover. The law is concave; small trades have disproportionate impact.\n\nParticipation constraint – Constrain the participation rate $\\rho = |Q| / V(t_0, t_0+T_{\\mathrm{exec}})$ ≤ $\\rho_{\\max}$ (e.g., 5–10%). If capacity is insufficient, scale down or skip trades.\n\n4.4 Multiplicity and data‑snooping controls\n\nFalse discovery rate (FDR) – Use Benjamini–Hochberg procedure to control the expected proportion of false positives when testing many strategies. FDR allows more power than family‑wise error control; the threshold depends on both the number of tests and the acceptable false‑discovery proportion. For example, testing 1000 strategies at 5% FWER would require t‑statistics > 4, which is overly conservative; FDR allows a relaxed threshold while controlling the proportion of false discoveries.", "tags": []}
{"fragment_id": "F_R4_811_816", "source_id": "R4", "locator": "unified_crypto_research_master.md:L811-L816", "text": "Reality‑check bootstrap – Apply the White (2000) reality‑check or Hansen (2005) step‑wise bootstrap: simulate the null distribution of performance differences under no edge. Select the best strategy only if its performance exceeds the maximum of bootstrap draws at the desired confidence level.\n\nDeflated Sharpe ratio (DSR) – Adjust Sharpe ratios for multiple testing and non‑normal returns. The DSR corrects for selection bias by estimating the expected maximum Sharpe ratio under $N$ independent trials; it adjusts the observed Sharpe ratio downward. The deflated Sharpe ratio is computed using the probabilistic Sharpe ratio and the variance of Sharpe estimates across trials. Only strategies with DSR above a threshold are retained.\n\nPre‑registration and FDR step‑up – Pre‑register all event types and parameter grids in the manifest; specify the number of hypotheses $N$. Apply FDR step‑up on p‑values across candidates. When performing parameter sweeps for a single event, treat each parameter combination as a separate hypothesis; compute FDR accordingly.", "tags": []}
{"fragment_id": "F_R4_817_836", "source_id": "R4", "locator": "unified_crypto_research_master.md:L817-L836", "text": "4.5 Rejection criteria\n\nReject a candidate strategy if any of the following holds:\n\nAfter‑cost expected return ≤ 0 in key regimes (bull, bear, high‑vol) using training data.\n\nFails multiplicity control: p‑value adjusted by FDR > α (e.g., 0.05) or DSR below threshold.\n\nCapacity erases more than X% (e.g., 50%) of expectancy when scaled to intended capital. Evaluate capacity by simulating increasing participation rates and measuring impact costs.\n\nFragile under moderate liquidity shocks: stress tests (spread × $k_s$, depth × $k_d$, volatility scaling) show negative expectancy.\n\n5 Strategy Artifact\n\nFor each accepted strategy, produce a formal specification:\n\nEvent‑triggered orders – On event $e$ at time $t_0$, schedule an entry after a deterministic delay (e.g., 1 minute). Choose order type (market/IOC or simple maker). For exit, use time‑based exit (horizon $H$) or stop conditions.\n\nParameter sweeps – Evaluate the strategy across a grid of parameters (thresholds, delays, position size scalars). Record performance metrics and produce stability heatmaps: matrix of parameters vs after‑cost returns and Sharpe ratios. Compute a neighbourhood robustness score: fraction of parameter combinations within the neighbourhood of the optimum that meet selection criteria. Prefer strategies with broad robustness rather than point estimates.", "tags": []}
{"fragment_id": "F_R4_837_851", "source_id": "R4", "locator": "unified_crypto_research_master.md:L837-L851", "text": "Stress tests – Simulate extreme conditions relevant to crypto:\n\nMultiply spreads by $k_s$ (e.g., 2×, 3×) and depth by $k_d$ (e.g., 0.5×) to mimic liquidity droughts.\n\nIncrease volatility in the impact model (raise $\\sigma$) to test slippage sensitivity.\n\nRandomly drop fills (venue outage) and simulate partial execution.\n\nApply forced deleveraging/ADL scenarios where positions are reduced at adverse prices.\n\nResult table schema – Store results with fields: strategy_id, run_id, parameter_set, horizon, mean_return_after_cost, volatility, max_drawdown, Sharpe, deflated_sharpe, p_value, capacity_adjusted_return, participation_rate, num_trades, hit_ratio, slippage_cost, impact_cost, test_fold_id, regime. The diagnostics checklist includes tests for stationarity, autocorrelation of returns, stability across regimes and time periods, and normality of residuals.\n\n6 Portfolio Layer (Small Systematic Emphasis)\n6.1 Allocation method", "tags": []}
{"fragment_id": "F_R4_852_908", "source_id": "R4", "locator": "unified_crypto_research_master.md:L852-L908", "text": "Use a cost‑aware convex optimisation or risk‑parity with turnover penalty to allocate capital across strategies. Inputs are expected after‑cost returns $\\mu_i$, covariance matrix $\\Sigma$ and cost coefficients $\\kappa_i$. Solve:\n\nmin\n⁡\n𝑤\n  \n𝑤\n𝑇\nΣ\n𝑤\n−\n𝜆\n \n𝑤\n𝑇\n𝜇\n+\n𝛾\n∑\n𝑖\n𝜅\n𝑖\n∣\nΔ\n𝑤\n𝑖\n∣\nmin\nw\n\t​\n\nw\nT\nΣw−λw\nT\nμ+γ∑\ni\n\t​\n\nκ\ni\n\t​\n\n∣Δw\ni\n\t​\n\n∣\n\nsubject to $\\sum_i w_i = 1$, $0 ≤ w_i ≤ w^{\\max}_i$. Here $\\lambda$ trades off return vs risk; $\\gamma$ penalises turnover; $w^{\\max}_i$ enforces capacity and exposure caps.\n\nAlternatively, implement regime‑conditioned risk parity: estimate separate covariance matrices for each regime (low‑vol, high‑vol) and allocate using the worst‑regime covariance to ensure resilience. Shrink covariance estimates using Ledoit–Wolf or similar shrinkage; incorporate open‑interest‑weighted scaling.\n\n6.2 Portfolio capacity and aggregation\n\nAggregate per‑strategy capacity constraints considering shared instruments and venues. Compute the sum of expected participation across strategies for each instrument and ensure it remains below the venue’s participation cap. When multiple strategies trigger on the same event, schedule them sequentially or allocate a meta‑order across venues to reduce impact. Use dynamic scaling: if aggregated participation exceeds $\\rho_{\\max}$, down‑scale all strategies proportionally.", "tags": []}
{"fragment_id": "F_R4_909_945", "source_id": "R4", "locator": "unified_crypto_research_master.md:L909-L945", "text": "6.3 Deterministic risk gates\n\nExposure caps – Limit notional exposure per instrument and per side (long/short). For instance, no more than 25% of portfolio NAV in BTC, 25% in ETH; 50% total long or short.\n\nLiquidity gates – Skip trades if instantaneous spread or order‑book depth falls below thresholds. Use signals like spread/depth ratio and obi to determine if the market is sufficiently liquid.\n\nVenue‑mechanism risk gates – Restrict exposure to venues with high funding volatility or poor execution reliability. Implement circuit breakers for events like liquidation cascades.\n\nRebalance friction control – Constrain portfolio turnover by incorporating transaction costs directly into the optimiser and imposing minimum holding periods.\n\n7 Executable Blueprint (YAML Template)\n\nA template for research runs can be encoded in YAML as follows:\n\nblueprint_version: \"1.0\"\nregistry_version: \"2026-02-18\"  # version/date of event definitions\nregistry_hash: \"<sha256-of-registry>\"\ndataset_id: \"crypto_data_snapshot_2026-02-17\"\ndataset_hash: \"<sha256-of-snapshot>\"\nparameters:\n  event_types: [FND_DISLOC, BASIS_DISLOC, LIQ_CASCADE, VOL_SHOCK, LIQ_SHOCK, STRUCT_BREAK, REGIME_SHIFT, MICRO_IMBAL]\n  horizons: [1h, 4h, 1d]\n  theta_bounds:\n    funding_threshold: [-50bp, 50bp]\n    obi_threshold: [0.2, 0.8]\n    delay: [0m, 5m]\n    position_scaler: [0.1, 1.0]\n  split_specs:\n    k_folds: 5\n    purging_window: \"H\"\n    embargo_fraction: 0.05\n  multiplicity_method: FDR  # options: FDR, reality_check, DSR\n  alpha: 0.05\nexecution_cost_model:\n  spread_cross: true\n  slippage_buckets: true\n  square_root_impact:", "tags": []}
{"fragment_id": "F_R4_946_968", "source_id": "R4", "locator": "unified_crypto_research_master.md:L946-L968", "text": "enabled: true\n    impact_coefficient: 0.5  # calibrated constant c\ncapacity_constraints:\n  participation_max: 0.05\n  max_notional_per_trade: 0.02  # as fraction of daily volume\n  max_aggregate_position: 0.25\nstress_tests:\n  spread_multipliers: [1, 2, 3]\n  depth_multipliers: [1, 0.5, 0.25]\n  vol_multipliers: [1, 1.5, 2]\n  venue_outage_probability: 0.05\nreproducibility:\n  code_commit: \"<git sha>\"\n  container_digest: \"<docker digest>\"\n  run_config_hash: \"<sha256-of-this-file>\"\n  rng_seeds: 123456\n\n\nThe run script reads this YAML, loads the corresponding dataset snapshot and event registry, performs PIT feature computation, executes the validation pipeline with specified splits and cost model, applies multiplicity corrections, and produces a report with strategy artefacts and portfolio allocation.\n\n8 Research Budget & Taxonomy Breadth Analysis\n8.1 Quantifying hypothesis space expansion", "tags": []}
{"fragment_id": "F_R4_969_976", "source_id": "R4", "locator": "unified_crypto_research_master.md:L969-L976", "text": "Expanding the number of event types and parameter sweeps increases the number of hypotheses $N$. Under the Benjamini–Hochberg FDR procedure, the expected number of false discoveries is $\\alpha \\cdot N / m$ where $m$ is the number of true positives. The deflated Sharpe ratio paper shows that the expected maximum Sharpe ratio increases logarithmically with the number of independent trials; thus the threshold for significance must grow with $\\sqrt{\\ln N}$. When more event types or parameters are tested, selection bias inflates observed performance; DSR and reality‑check corrections reduce this inflation.\n\nTo evaluate the effect of taxonomy breadth:\n\nEstimate independent trials $N$ – Multiply the number of event types by the number of parameter combinations per event (grid size). Dependencies (e.g., overlapping triggers) reduce effective $N$; adjust using correlation estimates.\n\nCompute expected false discoveries – Under FDR with level α, expected false positives ≈ α·$N$. Use this to determine how many candidate strategies can be investigated before the risk of false discovery becomes unacceptable.", "tags": []}
{"fragment_id": "F_R4_977_986", "source_id": "R4", "locator": "unified_crypto_research_master.md:L977-L986", "text": "Capacity‑adjusted opportunity set – For each candidate, compute capacity‑adjusted expected return. The number of feasible strategies is limited by capacity and correlation; adding more events may not increase opportunity if they share underlying liquidity.\n\n8.2 Max hypotheses per dataset and governance\n\nDefine a research budget: a maximum number of hypotheses $N_{\\max}$ per dataset snapshot. For example, limit $N_{\\max}$ to 200 trials per monthly snapshot. Pre‑registration requires researchers to list event types, parameter grids and evaluation criteria in a registry (manifest) before accessing labels. Each hypothesis consumes budget credits; once the budget is exhausted, new hypotheses must wait for the next dataset snapshot or require justification (e.g., new event type from market evolution). Track usage and remaining budget in a governance log.\n\nGovernance process:\n\nPre‑registration – Before running experiments, submit a manifest with event types and parameter ranges. The manifest is hashed and time‑stamped.", "tags": []}
{"fragment_id": "F_R4_987_1002", "source_id": "R4", "locator": "unified_crypto_research_master.md:L987-L1002", "text": "Approval – An internal committee or automated check verifies that proposed experiments fit within the budget and follow pre‑defined bounds.\n\nExecution – Run the research pipeline using the manifest. Record results and update budget usage.\n\nReview – At periodic intervals, review performance and budget consumption. Adjust budgets based on capacity and business priorities.\n\nThis process disciplines research, reduces data‑snooping risk, and ensures that the limited capacity of a small systematic operator is directed toward promising hypothesis spaces.\n\nFurther directions\n\nThe proposed framework is intentionally modular. Future enhancements may include:\n\nIntegrating DEX order book data to handle on‑chain liquidity and gas costs; this requires separate latency modelling and cost schedules.\n\nEmploying reinforcement learning for event‑triggered policies while respecting PIT constraints and multiplicity controls.", "tags": []}
{"fragment_id": "F_R4_1003_1010", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1003-L1010", "text": "Using Bayesian hierarchical models to share information across similar events (e.g., funding dislocation in BTC and ETH) while controlling false discoveries.\n\nExtending the event registry with macro‑economic or on‑chain analytics (e.g., whale movements, stablecoin flows) as additional context features.\n\nThis framework provides a rigorous, deterministic, capacity‑aware research process designed for a small systematic crypto operation. It emphasises reproducibility, microstructure realism, multiple‑testing control and robust portfolio construction.\n\n---", "tags": []}
{"fragment_id": "F_R4_1011_1012", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1011-L1012", "text": "## FULL SOURCE [R3]", "tags": []}
{"fragment_id": "F_R4_1013_1014", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1013-L1014", "text": "# Institutional Crypto Research Framework Audit and Expansion", "tags": []}
{"fragment_id": "F_R4_1015_1018", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1015-L1018", "text": "## Framework Audit\n\n**Baseline artifact being audited:** the provided “Full-Stack Research Framework for Systematic Crypto Portfolios” focuses on **systematic trading research** driven by point‑in‑time market state, explicit transaction-cost/capacity modeling, and deterministic replay (dataset/spec hashes, blueprinting). fileciteturn0file0L5-L17 fileciteturn0file0L255-L337", "tags": []}
{"fragment_id": "F_R4_1019_1029", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1019-L1029", "text": "### Coverage\n\n**What it covers well (strengths to preserve and reuse in the institutional version)**\n\n- **Point-in-time rigor + reproducibility as first-class constraints.** The framework explicitly enforces no lookahead, in-sample/out-of-sample separation, and deterministic artifact versioning (dataset hashes, blueprint specs). fileciteturn0file0L13-L17 fileciteturn0file0L255-L337  \n  This directly addresses well-known failure modes in quantitative research where reuse of the same dataset across many trials inflates false discoveries and overfitting risk. citeturn6search1turn6search8turn6search3\n\n- **Clear “unit of research” definitions.** The event instance → candidate edge → strategy spec → portfolio spec progression is explicit and operational, enabling auditability of what was tested and what was deployed. fileciteturn0file0L19-L23\n\n- **Event-driven discovery with formal triggers.** A versioned event registry (funding dislocations, liquidation cascades, liquidity shocks, structural breaks, regime shifts) is a strong mechanism to avoid ad-hoc “signal soup.” fileciteturn0file0L25-L47", "tags": []}
{"fragment_id": "F_R4_1030_1034", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1030-L1034", "text": "- **Execution realism via implementation shortfall and capacity modeling.** The explicit “arrival price / implementation shortfall” framing and a parameterized impact model make PnL claims falsifiable under explicit trading frictions. fileciteturn0file0L174-L207  \n  Implementation shortfall is a standard TCA objective and helps prevent paper alpha that disappears under realistic execution. citeturn10search8turn10search0\n\n- **Multiplicity controls are explicitly acknowledged.** The framework requires multiple-testing adjustments (FDR/reality check) and selection-bias-aware statistics (deflated Sharpe) when many variants are tried. fileciteturn0file0L217-L223 citeturn6search3turn6search8turn6search1", "tags": []}
{"fragment_id": "F_R4_1035_1048", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1035-L1048", "text": "### Gaps\n\n**What’s missing relative to an institutional “fundamentals + risk” crypto research framework**\n\nThe baseline is optimized for **tradable edges and portfolio construction**. It largely omits the “why will this asset be valuable and survivable?” layers:\n\n- **Thesis/macro/cycle positioning** is mostly absent (beyond market regime handling for returns), so it can’t answer: *what macro regime is this asset structurally long/short?*\n\n- **Problem/PMF** is not evaluated (who is the user, why now, what switching costs). This is central for long-only or venture-style decisions.\n\n- **Value accrual** is not formalized: where does durable value concentrate (token, equity, sequencer fees, MEV, off-chain capture).\n\n- **Tokenomics, incentives, governance, security, decentralization** are not treated as scored diligence categories. These are particularly important because DeFi systems have both **technical security** and **economic security** (incentive/game design risk). citeturn0search0turn0search4", "tags": []}
{"fragment_id": "F_R4_1049_1052", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1049-L1052", "text": "- **DEX-specific adverse selection / MEV** is only implicitly present (execution realism), but not elevated to a first-order threat model. Transaction reordering/frontrunning and MEV are empirically documented as core structural risks in on-chain markets (and can propagate to consensus-layer incentives). citeturn0search1turn8search3\n\n- **Regulatory/compliance risk** is absent. For institutions, eligibility and distribution are constrained by AML/sanctions, securities/commodities classification, and jurisdictional regimes (e.g., EU MiCA, FATF Travel Rule expectations). citeturn0search3turn0search7turn1search3turn1search11", "tags": []}
{"fragment_id": "F_R4_1053_1060", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1053-L1060", "text": "### Vague or underspecified terms\n\nWhere the baseline is operational but still ambiguous for institutional use:\n\n- **“Event thresholds” and “regime definitions”** (z-scores, percentiles, breakpoint tests) are parameterized but not anchored to an explicit *research budget* (max hypotheses per snapshot) and not tied to decision objectives (long-horizon vs tactical). fileciteturn0file0L219-L223 fileciteturn0file0L460-L460\n\n- **“DEX optional”** understates that DEX execution is not a minor routing variant: it introduces MEV, gas auctions, private orderflow, sandwich risk, and on-chain liquidity fragmentation. citeturn0search1turn8search3", "tags": []}
{"fragment_id": "F_R4_1061_1070", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1061-L1070", "text": "### Hidden assumptions\n\n**Assumptions embedded in the baseline (explicit + implicit)**\n\nTrading/system assumptions (baseline-specific):\n- **Liquid, continuous markets exist** for the universe (spot/perps) on venues you can access, with stable APIs and sufficient depth. fileciteturn0file0L7-L12\n- **Point-in-time data is obtainable** (fees, OI, funding, liquidation prints, order books) and can be normalized across venues without silent schema drift. fileciteturn0file0L50-L56 fileciteturn0file0L255-L337\n- **Execution model fidelity is adequate**: impact model form (often √Q) and slippage buckets approximate real fills at the intended scale. fileciteturn0file0L189-L207 citeturn10search17turn10search1\n- **Research process controls are sufficient** to manage multiple testing and overlapping labels (purging/embargo). fileciteturn0file0L215-L223 citeturn6search1turn6search8", "tags": []}
{"fragment_id": "F_R4_1071_1076", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1071-L1076", "text": "Institutional/fundamentals assumptions (commonly made unless explicitly denied; must be forced into the open):\n- “**Token price tracks usage**” (or some monotone relation exists), despite possible value leakage to off-chain actors, MEV, centralized sequencers, or equity.  \n- “**Team can ship**” (delivery risk is ignored), including ability to handle incidents and governance complexity.  \n- “**Regulatory neutrality**” (asset remains tradeable, listable, and custodiable across target jurisdictions), despite evolving frameworks and AML/sanctions requirements. citeturn0search3turn1search3turn7search10\n- “**On-chain activity is real demand**,” not subsidized or Sybil-driven—an assumption frequently violated in practice. citeturn9search1turn9search5", "tags": []}
{"fragment_id": "F_R4_1077_1087", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1077-L1087", "text": "### Failure modes\n\n**Where the baseline would greenlight bad projects**\n- **“Tradability masking insolvency.”** The framework can approve a tradeable asset/venue regime with statistically robust microstructure edges while ignoring existential protocol risks (admin-key upgradeability, governance capture, audit history, legal exposure).\n- **“Volume/liq mirage.”** If venue-reported volume is inflated (wash trading), capacity and cost assumptions can be catastrophically wrong. citeturn9search0turn9search4\n- **“DEX execution blind spot.”** Treating DEX as “optional routing” can greenlight strategies that are structurally MEV-dominated (sandwichable flow, toxic orderflow). citeturn0search1turn8search3\n\n**Where it would reject good projects**\n- **Early-stage/wedge-phase networks** with weak current liquidity but strong PMF/architecture/security could be rejected because they are not yet “tradable” or lack perps/OI/funding regimes.\n- **Non-financial utility networks** (where value accrues via adoption or off-chain services) may be rejected because microstructure edges are not the right objective function.", "tags": []}
{"fragment_id": "F_R4_1088_1089", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1088-L1089", "text": "## Measurable Criteria, Thresholds, and Falsifiable Hypotheses", "tags": []}
{"fragment_id": "F_R4_1090_1113", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1090-L1113", "text": "### How to read this section\n\n- **Criteria:** 3–8 measurable items per section (quantitative when possible; otherwise operational proxies).  \n- **Thresholds:** “Good / Neutral / Bad” are **default institutional heuristics**; they must be calibrated by sector (L1/L2, DeFi, infra, privacy) and strategy type (long-only vs market-neutral).  \n- **Hypotheses:** Each includes **confirm**, **disconfirm**, and **time window**. Use placeholders where project inputs are missing.\n\n---\n\n**Thesis & macro/cycle positioning**\n\n- **Measurable criteria**\n  - Risk sensitivity: rolling β to crypto “market” proxy (e.g., BTC index), and correlation structure under stress.\n  - Liquidity regime sensitivity: bid–ask/spread and depth behavior when volatility rises (stress elasticity).\n  - Reflexivity exposure: % of demand driven by leverage (perp OI / spot volume proxies); liquidation sensitivity.\n  - Narrative cyclicality: share of attention driven by social/media vs usage (proxy: engagement-to-usage ratio).\n- **Threshold heuristics**\n  - Good: thesis specifies *when it should underperform* (explicit “anti-thesis” regime) and provides hedge plan.\n  - Neutral: thesis is regime-aware but lacks quantified exposures.\n  - Bad: thesis is always-true (no falsifiable macro claims).\n- **Falsifiable hypotheses**\n  1) *“Asset outperforms in regime R because driver D is non-cyclical.”*  \n     - Confirm: excess returns and usage/fee driver persistence during ≥2 risk-off episodes.  \n     - Disconfirm: driver collapses with market regime; returns indistinguishable from β exposure.  \n     - Time window: 6–18 months (or ≥2 regime shifts).", "tags": []}
{"fragment_id": "F_R4_1114_1133", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1114-L1133", "text": "---\n\n**Problem/PMF + user segment**\n\n- **Measurable criteria**\n  - Defined user segment and job-to-be-done; measurable pain (time/cost reduction vs alternatives).\n  - Retention proxy: cohort stickiness (repeat users / new users; repeat tx per address).\n  - Willingness-to-pay proxy: fee generation per active user; fee/tx stability after incentives removed.\n  - Switching costs: composability lock-in, integrations, developer tooling, capital inertia.\n- **Threshold heuristics**\n  - Good: PMF evidence persists **without** subsidies; user segment is narrow + provable.\n  - Neutral: usage exists but appears incentive-sensitive; unclear segmentation.\n  - Bad: “everyone” is the user; usage explained primarily by rewards.\n- **Falsifiable hypotheses**\n  1) *“Protocol has PMF in segment S.”*  \n     - Confirm: retention + fee/user stable or rising across 2–3 cohorts with reduced incentives.  \n     - Disconfirm: activity collapses when rewards end; high churn and low repeat usage.  \n     - Time window: 3–9 months of cohort tracking.", "tags": []}
{"fragment_id": "F_R4_1134_1152", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1134-L1152", "text": "---\n\n**Value accrual (where value concentrates)**\n\n- **Measurable criteria**\n  - Value capture map: fees/rents go to token holders vs LPs/users vs sequencer/validators vs treasury/equity.\n  - Capture durability: is capture enforced at protocol level (hard-coded) or governance-toggle (optional).\n  - Leakages: MEV extracted externally; off-chain intermediaries capture (front-ends/relays/validators).\n  - Unit economics: “take rate” = protocol-controlled revenue / gross economic activity.\n- **Threshold heuristics**\n  - Good: value capture is explicit, enforceable, and aligned with bearing-risk stakeholders.\n  - Neutral: capture exists but discretionary (fee switch, governance toggles).\n  - Bad: token has weak/indirect capture; economics leak mainly to external actors.\n- **Falsifiable hypotheses**\n  1) *“Token capture will increase as adoption grows.”*  \n     - Confirm: protocol-controlled revenue share rises with volume; governance execution timelines credible.  \n     - Disconfirm: competitive pressure forces fees down or shifts value to LPs/validators/MEV.  \n     - Time window: 6–18 months.", "tags": []}
{"fragment_id": "F_R4_1153_1176", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1153-L1176", "text": "(For on-chain markets, explicitly model MEV as a value leakage and security externality. citeturn0search1turn8search3)\n\n---\n\n**Tokenomics (supply, emissions, sinks, dilution, distribution)**\n\n- **Measurable criteria**\n  - Supply schedule: circulating supply now; emissions curve; unlock calendar; discretionary mint rights.\n  - Dilution risk: expected inflation rate (annualized) and variance; conditions that accelerate emissions.\n  - Sinks: burns, lockups, staking, buybacks; are they endogenous to usage or discretionary.\n  - Distribution concentration: top holders’ share; treasury control; insider/VC unlock dominance.\n  - Airdrop integrity: Sybil resistance and distribution fairness (cluster analysis, behavioral features).\n- **Threshold heuristics**\n  - Good: emissions are bounded/predictable; sinks are usage-linked; distribution is not governance-capturable by a small group.\n  - Neutral: emissions moderate but near-term unlocks create overhang; sink mechanisms uncertain.\n  - Bad: discretionary minting or opaque lock/unlock mechanics; extreme concentration; Sybil-prone airdrops.\n- **Falsifiable hypotheses**\n  1) *“Net supply growth will be ≤ X% annually under base assumptions.”*  \n     - Confirm: observed supply + emission contracts match schedule; no emergency mints.  \n     - Disconfirm: governance/ops repeatedly change emissions upward.  \n     - Time window: 6–12 months (track releases vs schedule).\n  2) *“Airdrop distribution is Sybil-resistant.”*  \n     - Confirm: low post-drop clustering similarity; limited multi-wallet patterns.  \n     - Disconfirm: large share of allocation traceable to Sybil clusters.", "tags": []}
{"fragment_id": "F_R4_1177_1199", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1177-L1199", "text": "- Time window: 1–3 months post-distribution. citeturn9search1turn9search5\n\n(Token models that link valuation to adoption dynamics are explicitly studied in academic tokenomics literature; use them to stress-test whether claimed adoption curves are consistent with sustainable token value. citeturn2search4turn5search1)\n\n---\n\n**Incentives & game theory (actors, payoff alignment, attack surfaces)**\n\n- **Measurable criteria**\n  - Actor map: users, LPs, borrowers/lenders, validators/sequencers, governance delegates, MEV actors.\n  - Payoff alignment: who profits when users lose? Identify “toxic positive externalities” (e.g., liquidation bots).\n  - Manipulation surfaces: oracle dependence, reentrancy/exploit incentives, governance bribery, MEV.\n  - Adversary profitability: expected profit of key attacks vs cost (capital, fees, bribery).\n- **Threshold heuristics**\n  - Good: key attacks are unprofitable or reliably mitigated; adversary costs scale faster than gains.\n  - Neutral: mitigations exist but rely on monitoring/ops.\n  - Bad: profitable attack classes exist (oracle manipulation, MEV sandwich, governance capture) with weak deterrence.\n- **Falsifiable hypotheses**\n  1) *“MEV/extraction is bounded and does not impair user outcomes.”*  \n     - Confirm: stable effective spreads/price impact after accounting for MEV; mitigations deployed.  \n     - Disconfirm: persistent sandwich/frontrun patterns; user execution degrades as activity rises.  \n     - Time window: 3–6 months. citeturn0search1turn8search3", "tags": []}
{"fragment_id": "F_R4_1200_1218", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1200-L1218", "text": "---\n\n**Governance & upgrade risk**\n\n- **Measurable criteria**\n  - Upgradeability model: immutable vs proxy; timelocks; emergency powers; pause/blacklist features.\n  - Governance participation: voter turnout, quorum, delegate concentration, proposal throughput.\n  - Capture risk: concentration metrics on voting power; bribery markets; voter apathy.\n  - Change surface area: how many parameters can be modified; blast radius of a single proposal.\n- **Threshold heuristics**\n  - Good: upgrades are timelocked + transparent; emergency powers are narrow, audited, and monitored.\n  - Neutral: governance works but concentration high; timelocks shorter than institutional comfort.\n  - Bad: upgrade keys can change core logic without delay; governance effectively centralized.\n- **Falsifiable hypotheses**\n  1) *“Governance cannot be captured by ≤ N entities.”*  \n     - Confirm: voting power dispersion; no consistent cartel outcomes; proposal outcomes diverse.  \n     - Disconfirm: repeated wins by a small coalition; quorum depends on insiders.  \n     - Time window: 6–12 months.", "tags": []}
{"fragment_id": "F_R4_1219_1239", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1219-L1239", "text": "(For upgradeable systems, timelocks are a standard mitigation for admin misuse risk; treat un-timelocked upgrade authority as a structural red flag. citeturn10search18)\n\n---\n\n**Technical architecture (trust assumptions, dependencies, liveness/finality)**\n\n- **Measurable criteria**\n  - Trust assumptions: honest majority? data availability? sequencer trust? multisig dependencies?\n  - Dependency map: oracles, bridges, L2/L1 settlement, off-chain relayers, RPC providers.\n  - Failure domains: what breaks if dependency fails (halt vs incorrect state transition vs fund loss).\n  - Liveness/finality: time to finality, reorg risk, withdrawal finality (for L2), downtime history.\n- **Threshold heuristics**\n  - Good: trust assumptions are explicit; dependency failures degrade gracefully; recovery plan exists.\n  - Neutral: dependencies exist but are standard; recovery is plausible.\n  - Bad: opaque dependencies; single points of failure can cause catastrophic loss or censorship.\n- **Falsifiable hypotheses**\n  1) *“System maintains liveness under stress scenario S.”*  \n     - Confirm: historical stress tests / incidents show bounded downtime; architecture supports failover.  \n     - Disconfirm: repeated halts/censorship; no credible failover.  \n     - Time window: 6–18 months (or incident-based).", "tags": []}
{"fragment_id": "F_R4_1240_1258", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1240-L1258", "text": "---\n\n**Security posture (audits, bug bounties, incident history)**\n\n- **Measurable criteria**\n  - Audit coverage: count + recency + scope; whether audits include core + periphery + dependencies.\n  - Verification/testing depth: fuzzing/invariant tests, formal methods where warranted.\n  - Bug bounty strength: program exists, payout realism, response SLAs.\n  - Incident history: past exploits, severity, time-to-patch, user restitution.\n- **Threshold heuristics**\n  - Good: multiple independent audits; serious bounty; quantified testing; strong incident response.\n  - Neutral: some audits; bounty exists but limited; partial coverage.\n  - Bad: unaudited core or repeat critical incidents; opaque postmortems.\n- **Falsifiable hypotheses**\n  1) *“No critical exploit class remains in critical path given current code.”*  \n     - Confirm: audit + fuzz + invariant test coverage; critical findings remediated and verified.  \n     - Disconfirm: new critical findings recur; core invariants fail in testing.  \n     - Time window: continuous; formal re-check at each major release.", "tags": []}
{"fragment_id": "F_R4_1259_1280", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1259-L1280", "text": "(Smart contract vulnerability taxonomies and tool landscapes are extensively surveyed; use them to ensure your audit/testing stack covers dominant bug classes. citeturn1search17turn1search1)  \n(Formal verification is feasible for high-criticality contracts and has been applied in major ecosystems. citeturn1search6)  \n(Bug bounty economics and reward rules vary; treat “bounty exists” as insufficient—evaluate realism. citeturn10search3)\n\n---\n\n**Decentralization & censorship resistance (where relevant)**\n\n- **Measurable criteria**\n  - Consensus concentration: validator/miner concentration; stake distribution; operator diversity.\n  - Client diversity: software diversity reduces correlated failure risk.\n  - Infrastructure concentration: hosting/provider concentration; geographic spread.\n  - Governance decentralization: proposer/delegate concentration; upgrade authority dispersion.\n  - Censorship indicators: transaction inclusion anomalies; OFAC-style compliance concentration signals.\n- **Threshold heuristics**\n  - Good: decentralization measured across subsystems; no single chokepoint dominates.\n  - Neutral: moderate concentration typical of PoS; mitigations exist (delegation diversity, client diversity).\n  - Bad: few entities can censor/finalize/upgrade; dependencies are centralized.\n- **Falsifiable hypotheses**\n  1) *“No small set of entities can halt/censor the system.”*  \n     - Confirm: decentralization metrics improve or remain stable as system scales.  \n     - Disconfirm: concentration increases with TVL/usage; coercion or outages show dependence on a few actors.", "tags": []}
{"fragment_id": "F_R4_1281_1303", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1281-L1303", "text": "- Time window: 6–24 months.\n\n(Decentralization measurement is an active research topic; treat it as multi-dimensional, not a single “node count.” citeturn2search2turn2search10)\n\n---\n\n**Ecosystem & distribution (partners, integrations, community health)**\n\n- **Measurable criteria**\n  - Integration breadth: number/quality of credible integrations; share of usage from top N integrators.\n  - Developer momentum: repo activity, unique contributors, issue/PR velocity (normalize for spam).\n  - Community resilience: governance participation breadth, forum activity quality, concentration of discourse.\n  - Distribution channels: wallets, exchanges, on-chain routing (aggregators), enterprise partners.\n- **Threshold heuristics**\n  - Good: diversified integrations; developer activity is sustained and non-incentivized.\n  - Neutral: a few large integrators dominate.\n  - Bad: ecosystem is fragile (single distribution partner); “community” is largely paid.\n- **Falsifiable hypotheses**\n  1) *“Ecosystem growth compounds (more integrations → more usage).”*  \n     - Confirm: integrations and non-incentivized usage grow together; churn of integrators is low.  \n     - Disconfirm: integrations are shallow/marketing-only; usage doesn’t follow.  \n     - Time window: 6–12 months.", "tags": []}
{"fragment_id": "F_R4_1304_1326", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1304-L1326", "text": "---\n\n**Competitive landscape & moats**\n\n- **Measurable criteria**\n  - Differentiation: measurable performance/security/UX advantage vs closest substitutes.\n  - Switching costs: liquidity depth, composability position, developer tooling lock-in.\n  - Sustainability: fee compression risk; multi-chain commoditization.\n  - Substitutability: can a fork + incentives replicate the product?\n- **Threshold heuristics**\n  - Good: moat is structural (network effects, deep liquidity, defensible tech).\n  - Neutral: differentiation exists but can be copied.\n  - Bad: commodity product + incentives-only moat.\n- **Falsifiable hypotheses**\n  1) *“Competitors cannot replicate advantage A without cost C.”*  \n     - Confirm: competitor attempts fail or require uneconomic subsidies.  \n     - Disconfirm: feature parity reached quickly and users migrate.  \n     - Time window: 6–18 months.\n\n---\n\n**On-chain / usage metrics (leading indicators vs lagging)**", "tags": []}
{"fragment_id": "F_R4_1327_1341", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1327-L1341", "text": "- **Measurable criteria**\n  - Leading indicators: net new quality users (Sybil-adjusted), retained cohorts, recurring fees.\n  - Lagging indicators: TVL, raw tx count, gross volume (often incentive-manipulable).\n  - Quality adjustments: filter wash/incentive activity; cluster Sybil; remove internal routing loops.\n  - Price/usage decoupling: detect whether usage metrics are merely price-driven.\n- **Threshold heuristics**\n  - Good: leading indicators improve and remain after subsidy changes; metrics are Sybil-adjusted.\n  - Neutral: mixed; leading indicators uncertain.\n  - Bad: only lagging metrics look strong; metrics collapse when incentives change.\n- **Falsifiable hypotheses**\n  1) *“Usage growth is organic, not subsidy-driven.”*  \n     - Confirm: activity persists after incentives reduce; cohort retention remains.  \n     - Disconfirm: sharp drop coinciding with incentive changes.  \n     - Time window: 3–9 months.", "tags": []}
{"fragment_id": "F_R4_1342_1362", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1342-L1362", "text": "(Sybil detection for airdrops is actively researched; use it as part of “quality user” measurement. citeturn9search5turn9search9)\n\n---\n\n**Liquidity & market structure (venues, float, unlocks, reflexivity)**\n\n- **Measurable criteria**\n  - Real liquidity: consolidated order book depth, effective spread, slippage for size Q across venues.\n  - Volume integrity: detect wash trading / fake volume; compare volume vs web traffic vs on-chain flows.\n  - Float dynamics: circulating float, borrow availability, unlock calendar, market maker concentration.\n  - Derivatives reflexivity: perp OI, funding regime volatility, liquidation clusters.\n- **Threshold heuristics**\n  - Good: liquidity supports target position size with modeled costs; volume integrity checks pass.\n  - Neutral: liquidity adequate but unlocks create near-term overhang.\n  - Bad: liquidity is thin or fake; unlocks are large relative to real liquidity; venue concentration extreme.\n- **Falsifiable hypotheses**\n  1) *“Reported volume reflects real liquidity.”*  \n     - Confirm: independent checks (traffic, flows, cross-venue consistency) align.  \n     - Disconfirm: wash-trading signatures; volume decoupled from traffic/funds held.  \n     - Time window: 1–3 months.", "tags": []}
{"fragment_id": "F_R4_1363_1383", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1363-L1383", "text": "(Wash trading is empirically documented in centralized exchange settings; treat “reported volume” as untrusted until validated. citeturn9search0turn9search4)\n\n---\n\n**Regulatory & legal risk (jurisdictional exposure, classification risks)**\n\n- **Measurable criteria**\n  - Jurisdiction map: where team, foundation, key service providers, and major users are located.\n  - Token classification exposure: plausible security/derivatives/payment interpretations by jurisdiction.\n  - AML/sanctions exposure: VASP touchpoints, mixers, privacy features; sanctions screening controls.\n  - Market access: exchange listing/custody constraints; stablecoin dependencies.\n- **Threshold heuristics**\n  - Good: clear compliance posture; limited reliance on high-risk flows; credible legal opinions *with scope*.\n  - Neutral: moderate uncertainty; manageable exposure if limited distribution.\n  - Bad: high probability of enforcement or delisting in target jurisdictions; sanctions/AML red flags.\n- **Falsifiable hypotheses**\n  1) *“Asset remains accessible to target investor base in jurisdictions J.”*  \n     - Confirm: custody/listing/reg pathway exists; no major restrictions triggered.  \n     - Disconfirm: delistings, enforcement actions, or custody exclusion.  \n     - Time window: 6–24 months (monitor continuously).", "tags": []}
{"fragment_id": "F_R4_1384_1406", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1384-L1406", "text": "(EU MiCA timelines and implementation details matter for European distribution. citeturn0search3turn0search7)  \n(FATF guidance frames global AML expectations for virtual assets and Travel Rule implementation. citeturn1search3turn1search11)  \n(US sanctions compliance expectations for virtual currency industry are explicitly published; treat as operational requirements. citeturn7search10)\n\n---\n\n**Execution risk (team, runway, ops)**\n\n- **Measurable criteria**\n  - Team capacity: release cadence vs roadmap; historical delivery variance.\n  - Runway: treasury assets, burn rate proxy, funding concentration, stablecoin exposure.\n  - Operational maturity: incident response playbooks, key management, access controls, monitoring.\n  - Vendor risk: reliance on a single infra provider (RPC, sequencer, custody).\n- **Threshold heuristics**\n  - Good: consistent shipping; strong ops; runway supports ≥ 18–24 months of base-case plan.\n  - Neutral: execution history mixed; ops improving.\n  - Bad: repeated missed milestones; weak key management; short runway with opaque financing.\n- **Falsifiable hypotheses**\n  1) *“Team can deliver milestone M by date T.”*  \n     - Confirm: interim milestones hit; testnet/prod deltas converge to plan.  \n     - Disconfirm: repeated slips without credible technical blockers; key staff churn.  \n     - Time window: until T (plus post-launch stability period).", "tags": []}
{"fragment_id": "F_R4_1407_1428", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1407-L1428", "text": "---\n\n**Valuation approach (relative, cashflow-like, utility, comparable networks)**\n\n- **Measurable criteria**\n  - Choose valuation lens consistent with value accrual:  \n    - fee/revenue-like (if token captures fees),  \n    - utility (if token is required for usage),  \n    - monetary premium/store-of-value (if applicable),  \n    - comparable networks (peer multiples).\n  - Sensitivity to assumptions: adoption, fee rates, take rate, emission schedule, discount rate proxy.\n  - Cross-checks: on-chain “value” proxies (e.g., value-to-activity metrics) but adjusted for manipulation.\n- **Threshold heuristics**\n  - Good: valuation triangulates multiple methods and stress-tests assumptions.\n  - Neutral: one method used with sensitivity ranges.\n  - Bad: valuation is price-anchored (“it used to be higher”).\n- **Falsifiable hypotheses**\n  1) *“Under base assumptions, implied valuation multiple is defensible vs peers.”*  \n     - Confirm: peer-adjusted multiples with plausible growth/risks; sensitivity shows robust value.  \n     - Disconfirm: valuation relies on extreme growth or ignores dilution/unlocks.  \n     - Time window: quarterly re-evaluation.", "tags": []}
{"fragment_id": "F_R4_1429_1448", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1429-L1448", "text": "(Research on cryptoasset valuation frameworks and factor-like interpretations is developed by industry research houses; use as triangulation, not as sole proof. citeturn2search11)\n\n---\n\n**Catalysts & timeline**\n\n- **Measurable criteria**\n  - Catalyst list with dates/windows: upgrades, fee switches, unlock cliffs, regulatory decisions, listings.\n  - Catalyst directionality: what must happen for the catalyst to be positive vs negative.\n  - Pre/post metrics: define what changes you expect in usage, fees, security, decentralization.\n- **Threshold heuristics**\n  - Good: catalysts are specific and testable; expectations are quantified with “what would disappoint.”\n  - Neutral: catalysts exist but impact unclear.\n  - Bad: “catalyst” is vague narrative momentum.\n- **Falsifiable hypotheses**\n  1) *“Catalyst C increases metric K by ≥ X% without increasing risk R.”*  \n     - Confirm: measured uplift and stable risk indicators.  \n     - Disconfirm: no uplift or risk spikes (incidents, churn, governance controversy).  \n     - Time window: 1–3 months post-catalyst.", "tags": []}
{"fragment_id": "F_R4_1449_1450", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1449-L1450", "text": "---", "tags": []}
{"fragment_id": "F_R4_1451_1452", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1451-L1452", "text": "## Bias Checks and Red Flags", "tags": []}
{"fragment_id": "F_R4_1453_1473", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1453-L1473", "text": "### Bias check module\n\n**Narrative bias checks (story-first failure modes)**  \nUse these as **required “pre-mortem gates”** before scoring:\n\n- Story-first reasoning: thesis has conclusions before evidence; evidence is selected post-hoc.\n- Charismatic founder bias: confidence derived from personality/network rather than shipped artifacts.\n- Meme momentum bias: attention conflated with adoption; price appreciation treated as validation.\n- Survivorship bias: only studying winners; ignoring base rates of failure in similar designs.\n- Techno-solutionism: assuming “better tech” guarantees adoption despite distribution and incentives.\n- Single-cause fallacy: one driver explains everything (e.g., “fees = value”) despite leakage channels.\n\n**Data bias checks (measurement failure modes)**\n\n- Wash trading / fake volume: venue volume not equal to executable liquidity. citeturn9search0turn9search4\n- Sybil activity: user counts inflated via multi-wallet farming (especially around airdrops/incentives). citeturn9search5turn9search1\n- Incentive-driven usage: TVL/tx spikes caused by rewards, not PMF (cliffs after incentives end).\n- API/vendor bias: dashboards choose definitions that flatter the project (metric definition drift).\n- Cherry-picked windows: starting after the bottom or excluding incident periods.\n- On-chain attribution errors: mislabeling exchanges/bridges/internal routing as “user demand.”", "tags": []}
{"fragment_id": "F_R4_1474_1475", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1474-L1475", "text": "image_group{\"layout\":\"carousel\",\"aspect_ratio\":\"16:9\",\"query\":[\"MEV sandwich attack diagram ethereum\",\"token unlock schedule cliff vesting chart crypto\",\"DAO governance timelock upgrade process diagram\"],\"num_per_query\":1}", "tags": []}
{"fragment_id": "F_R4_1476_1486", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1476-L1486", "text": "### Stoplight severity system and required mitigations\n\n- **Green:** issue is explainable, measured, and bounded.  \n  **Mitigation:** record definition + monitoring metric; proceed.\n\n- **Amber:** plausible risk that could flip the thesis; uncertainty is material.  \n  **Mitigation:** require at least one independent verification source + a sensitivity test + explicit disconfirming evidence triggers before proceeding.\n\n- **Red:** structural risk that can cause permanent loss, delisting, or thesis invalidation.  \n  **Mitigation:** *no-go* unless risk is eliminated (not merely “managed”) or position is re-scoped to a strictly bounded tactical trade.", "tags": []}
{"fragment_id": "F_R4_1487_1502", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1487-L1502", "text": "### Red-flag catalog (grouped; ≥ 25)\n\n| Theme | Red flag | Severity | Required mitigation |\n|---|---|---|---|\n| Narrative | “Replaces X” with no user/segment specificity | Amber | Force segmentation + measure retention/fees per cohort |\n| Narrative | “Partnerships” are announcements without integrations shipped | Amber | Verify production integrations + usage attributable to them |\n| Narrative | Roadmap is only slides; no shipped milestones | Amber | Map to repo/releases + independent evidence |\n| Narrative | “Community” is mostly incentivized shilling | Amber | Separate organic vs paid; require retention after incentives |\n| Narrative | Price-anchored valuation (“down 90% so cheap”) | Amber | Rebuild valuation from accrual + dilution + peers |\n| Data integrity | CEX volume huge but shallow order books | Red | Liquidity sampling + slippage tests; downweight/ignore volume |\n| Data integrity | Wash trading indicators; volume decoupled from traffic/funds | Red | Cross-validate with independent sources; exclude venues citeturn9search0turn9search4 |\n| Data integrity | TVL spikes coincide with rewards; cliffs after rewards end | Amber | Incentive-adjusted metrics + cohort retention |\n| Data integrity | User counts dominated by Sybil clusters | Red | Sybil detection + adjusted KPIs citeturn9search5turn9search9 |\n| Data integrity | Metric definitions change mid-analysis | Amber | Freeze definitions; re-run history with consistent schema |\n| Tokenomics | Opaque unlock schedule; “unknown future emissions” | Red | Full unlock calendar + contract-level verification |\n| Tokenomics | Discretionary mint/blacklist powers without constraints | Red | Verify controls; require timelock/governance constraints citeturn10search18 |", "tags": []}
{"fragment_id": "F_R4_1503_1515", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1503-L1515", "text": "| Tokenomics | Extreme concentration in top holders with governance control | Red | Concentration analysis + capture simulations |\n| Tokenomics | “Fee switch someday” is the main value story | Amber | Require governance path + precedent + timeline |\n| Incentives | Profitable oracle manipulation path exists | Red | Dependency map + oracle stress tests; require mitigations |\n| Incentives | Persistent MEV extraction harming users | Amber/Red | Measure MEV impact; require countermeasures citeturn0search1turn8search3 |\n| Incentives | Liquidity is mercenary (LPs churn quickly) | Amber | Measure LP retention; stress fee reductions |\n| Governance | Upgradable contracts without timelock | Red | Timelock or immutability requirement citeturn10search18 |\n| Governance | Governance turnout near zero; insiders decide | Amber | Delegate analysis + quorum reforms evidence |\n| Governance | Emergency powers are broad and opaque | Red | Narrow scope + documented policy + monitoring |\n| Security | No independent audits for core contracts | Red | Require audits + remediation proof |\n| Security | Repeated critical incidents without credible postmortems | Red | Verify root cause fixed; assess ops maturity |\n| Security | Weak bounty / no response process | Amber | Require credible bounty + SLA + triage process citeturn10search3 |\n| Architecture | Single dependency can freeze funds (bridge/oracle/sequencer) | Red | Dependency map + failover design |\n| Decentralization | Few entities can censor/finalize/upgrade | Red | Quantify; require dispersion improvements citeturn2search2turn2search10 |", "tags": []}
{"fragment_id": "F_R4_1516_1522", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1516-L1522", "text": "| Competitive | Product is a forkable commodity + incentives-only moat | Amber | Identify structural moat or price as tactical-only |\n| Regulatory | High-risk AML/sanctions exposure with no controls | Red | Screening + policy + legal constraints citeturn7search10turn1search11 |\n| Regulatory | Key jurisdictions likely classify token adversely | Amber/Red | Obtain scoped legal analysis + contingency planning |\n| Market structure | Massive near-term unlock vs real liquidity | Red | Unlock stress test; position sizing constraints |\n| Execution | Team runway short and financing opaque | Amber | Treasury/runway modeling + disclosure requirements |\n| Execution | Key-person risk extreme; governance/ops depend on one actor | Amber | Org redundancy + documented processes |", "tags": []}
{"fragment_id": "F_R4_1523_1524", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1523-L1524", "text": "## Expanded Institutional Framework and Scoring Rubric", "tags": []}
{"fragment_id": "F_R4_1525_1534", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1525-L1534", "text": "### Scoring rubric (0–5) anchored to evidence quality and risk\n\n**Score meaning (uniform across sections)**  \n- **0 — Unscorable:** missing data or unverifiable claims.  \n- **1 — Weak:** mostly narrative; minimal primary sources; high uncertainty.  \n- **2 — Partial:** some primary evidence, but gaps in key risks; limited falsifiability.  \n- **3 — IC-ready minimum:** primary sources + quantified metrics + explicit disconfirming evidence; risks mapped.  \n- **4 — Strong:** multiple independent evidence streams; stress tests run; bias checks passed; mitigations credible.  \n- **5 — Best-in-class:** adversarial thinking proven in practice; reproducible analysis; continuous monitoring plan; clear “anti-thesis” and pre-committed exit rules.", "tags": []}
{"fragment_id": "F_R4_1535_1549", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1535-L1549", "text": "### Checklist (per section) with “what earns a 5” and “red flags”\n\nUse this as the final institutional checklist (each section gets a 0–5 score):\n\n| Section | Checklist minimum (≥3) | What earns a 5 | Automatic score cap (cannot exceed) |\n|---|---|---|---|\n| Thesis & macro | Defined regime where thesis fails; exposures quantified | Regime playbook + hedges + disconfirm triggers | Cap at 2 if thesis is non-falsifiable |\n| Problem/PMF | Clear segment; cohort/retention and WTP proxies | Organic PMF proven without subsidies | Cap at 2 if “user = everyone” |\n| Value accrual | Value capture map + leakage analysis | Durable, protocol-enforced capture with stress tests | Cap at 2 if capture is purely narrative |\n| Tokenomics | Emissions/unlocks verified; dilution modeled | Supply + sinks resilient; Sybil-resistant distribution | Cap at 1 if discretionary minting opaque |\n| Incentives | Actor/payoff model; attack profitability evaluated | Attacks provably unprofitable or mitigated in practice | Cap at 2 if profitable attacks unaddressed |\n| Governance | Upgrade process mapped; timelocks verified | Capture simulations + strong dispersion | Cap at 1 if no timelock on upgrades |\n| Tech architecture | Trust/dependencies explicit; liveness analyzed | Formal dependency map + failure drills | Cap at 2 if single point of failure catastrophic |\n| Security posture | Audits + testing + bounty + incidents reviewed | Multiple audits + invariants/fuzz + fast IR maturity | Cap at 2 if unaudited critical path |\n| Decentralization | Multi-dimensional metrics; trends monitored | Decentralization improves with scale | Cap at 2 if centralization is structural |", "tags": []}
{"fragment_id": "F_R4_1550_1558", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1550-L1558", "text": "| Ecosystem/distribution | Integrations verified; dev health measured | Compound distribution with diversified channels | Cap at 2 if “partners” aren’t using it |\n| Competition/moat | Competitor map + switching cost analysis | Structural moat survives subsidy wars | Cap at 2 if commodity + incentives-only |\n| On-chain metrics | Leading indicators defined; manipulation controls | Sybil/incentive-adjusted dashboards + alerts | Cap at 2 if only lagging vanity metrics |\n| Liquidity/market structure | Real liquidity + unlock stress test | Execution proven at target size under stress | Cap at 2 if volume integrity fails |\n| Regulatory/legal | Jurisdiction map; AML/sanctions posture | Clear pathways for custody/listing/distribution | Cap at 2 if high probability of delist/enforcement |\n| Execution risk | Runway modeled; shipping track record | Org maturity + redundancy + IR playbooks | Cap at 2 if repeated missed milestones |\n| Valuation | Method consistent with accrual; sensitivity ranges | Triangulated valuation + scenario consistency | Cap at 2 if price-anchored |\n| Catalysts/timeline | Dated catalysts + measurable expectations | Pre/post KPI commitments + exit triggers | Cap at 2 if catalysts are vague |", "tags": []}
{"fragment_id": "F_R4_1559_1582", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1559-L1582", "text": "### Weighting scheme by strategy type\n\nWeights are **defaults**; adjust for mandate constraints. (Rows sum to 100 per strategy.)\n\n| Section | Liquid long-only | Swing trade | Venture-style | Market-neutral |\n|---|---:|---:|---:|---:|\n| Thesis & macro | 6 | 15 | 3 | 9 |\n| Problem/PMF | 4 | 3 | 13 | 3 |\n| Value accrual | 9 | 5 | 7 | 5 |\n| Tokenomics | 9 | 4 | 8 | 5 |\n| Incentives & game theory | 6 | 4 | 6 | 5 |\n| Governance & upgrade risk | 5 | 3 | 5 | 3 |\n| Technical architecture | 4 | 3 | 9 | 4 |\n| Security posture | 9 | 6 | 7 | 8 |\n| Decentralization | 5 | 2 | 5 | 3 |\n| Ecosystem & distribution | 5 | 3 | 8 | 3 |\n| Competition & moats | 5 | 3 | 7 | 3 |\n| On-chain/usage metrics | 6 | 10 | 3 | 8 |\n| Liquidity & market structure | 9 | 15 | 2 | 17 |\n| Regulatory & legal | 7 | 5 | 6 | 8 |\n| Execution risk (team/ops) | 4 | 4 | 6 | 4 |\n| Valuation | 6 | 6 | 4 | 6 |\n| Catalysts & timeline | 1 | 9 | 1 | 6 |", "tags": []}
{"fragment_id": "F_R4_1583_1597", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1583-L1597", "text": "### Minimum pass thresholds and kill criteria\n\n**Minimum pass (default):**\n- Weighted score **≥ 3.2/5** for the strategy type, **and**\n- No “Red” kill criteria triggered, **and**\n- No section scored **0** in any of: Security posture, Tokenomics, Regulatory/legal, Governance & upgrade risk.\n\n**Kill criteria (automatic no-go unless eliminated)**\n- Upgrade authority can change core logic **without timelock** or credible constraint. citeturn10search18  \n- Critical-path contracts lack independent audits and/or have unresolved critical findings.  \n- Discretionary minting/blacklisting/pause powers exist with opaque scope and no governance constraint.  \n- Liquidity is materially fake (wash trading / non-executable volume), making sizing impossible. citeturn9search0turn9search4  \n- High probability of delisting/enforcement in target jurisdiction(s) with no viable mitigation path (mandate-dependent). citeturn0search3turn1search11  \n- Proven profitable exploit/attack class remains open (oracle manipulation, MEV-driven extraction with user harm) with no deployed mitigations. citeturn0search1turn8search3", "tags": []}
{"fragment_id": "F_R4_1598_1654", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1598-L1654", "text": "### One-page IC summary template\n\n```text\nIC SUMMARY (One Page)\n\nAsset / Protocol:\nStrategy Type: (Liquid long-only / Swing / Venture-style / Market-neutral)\nDecision: (Approve / Watchlist / Reject)\nPosition Constraints: (Max size, liquidity limits, jurisdiction limits)\n\nThesis (1–3 sentences):\n- Core claim:\n- Why now:\n- Anti-thesis (when this fails):\n\nKey Evidence (bullet, cite primary sources):\n- PMF:\n- Value accrual:\n- Tokenomics:\n- Security:\n- Regulatory:\n\nTop Risks (ranked, with mitigations):\n1)\n2)\n3)\n\nCatalysts (dated windows) + What would disappoint:\n- Catalyst A (window):\n  Expected KPI change:\n  Disconfirm trigger:\n- Catalyst B (window):\n\nValuation (method + key assumptions):\n- Method:\n- Base assumptions:\n- Sensitivities that break the case:\n\nScorecard (0–5 each, weighted):\n- Thesis/macro:\n- PMF:\n- Value accrual:\n- Tokenomics:\n- Incentives:\n- Governance:\n- Tech:\n- Security:\n- Decentralization:\n- Ecosystem:\n- Competition:\n- On-chain metrics:\n- Liquidity/market structure:\n- Regulatory:\n- Execution risk:\n- Valuation:\n- Catalysts:", "tags": []}
{"fragment_id": "F_R4_1655_1661", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1655-L1661", "text": "Disconfirming Evidence Checklist (pre-committed exit / no-go triggers):\n- (3–8 items)\n\nCitations log:\n- (links / doc hashes / snapshots)\n```", "tags": []}
{"fragment_id": "F_R4_1662_1663", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1662-L1663", "text": "## Deep Research Workflow", "tags": []}
{"fragment_id": "F_R4_1664_1681", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1664-L1681", "text": "### Step-by-step workflow (IC-ready)\n\n**Step 0 — Declare constraints (before reading the docs)**  \n- Strategy type + holding period + max position size + jurisdiction/custody constraints.  \n- What would force a *no* regardless of upside (kill criteria list you will not waive).\n\n**Step 1 — Build the evidence tree (sections + hypotheses first)**  \n- Create a document with the 17 sections above.  \n- For each section, write **1–3 falsifiable hypotheses** and pre-commit disconfirming evidence triggers (from the earlier section).\n\n**Step 2 — Collect primary sources (freeze snapshots)**  \nPrimary sources to prefer:\n- Protocol whitepaper / technical docs; on-chain contracts; emitted parameters at specific block heights.\n- Code repositories (release tags, commit hashes) and dependency manifests.\n- Audit reports and formal verification artifacts (if any).\n- Governance posts and executed proposals (with timestamps and payloads).\n- Token distribution/unlock contracts and schedules.", "tags": []}
{"fragment_id": "F_R4_1682_1691", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1682-L1691", "text": "**Step 3 — Collect measurement sources (on-chain, off-chain, market)**  \n- On-chain analytics (SQL/decoded event logs, not screenshots).  \n  - Example: entity[\"company\",\"Dune\",\"blockchain analytics platform\"] query workflows and definitions. citeturn8search10turn8search14  \n  - Example: entity[\"organization\",\"The Graph\",\"decentralized indexing protocol\"] for indexed on-chain datasets when appropriate. citeturn8search11turn8search1\n- Market data: consolidated spot/perp order books, funding/OI, venue quality checks, borrow rates (as available).\n- Regulatory sources: jurisdictional rule texts, regulator statements, AML/sanctions guidance.  \n  - entity[\"organization\",\"FATF\",\"global aml standard setter\"] virtual asset guidance / implementation updates. citeturn1search3turn1search11  \n  - entity[\"organization\",\"ESMA\",\"eu securities authority\"] MiCA references and implementation context. citeturn0search3turn0search7  \n  - entity[\"organization\",\"OFAC\",\"us sanctions authority\"] virtual currency sanctions guidance. citeturn7search10", "tags": []}
{"fragment_id": "F_R4_1692_1702", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1692-L1702", "text": "**Step 4 — Run section tests (examples you should standardize)**  \n- Token unlock stress test: shock free float + simulate slippage at size Q; compare to unlock calendar.\n- Security dependency map: enumerate oracles/bridges/admin keys; classify each dependency by failure mode.\n- Governance capture simulation: compute how many entities (and which) can pass proposals; simulate bribery thresholds.\n- Liquidity shock test: widen spreads / reduce depth and re-run execution assumptions (tactical + risk sizing).\n- MEV exposure scan (if DEX): identify sandwichable flows and measure effective execution degradation. citeturn0search1turn8search3\n\n**Step 5 — Apply the bias-check module before scoring**  \n- Run narrative checks: force a “steelman bear case” and require it to be evidence-backed.  \n- Run data checks: wash trading screens + Sybil clustering + incentive-adjusted KPIs. citeturn9search0turn9search5", "tags": []}
{"fragment_id": "F_R4_1703_1729", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1703-L1729", "text": "**Step 6 — Score, weight, decide**  \n- Score each section 0–5 with citations.  \n- Compute weighted score for your strategy type.  \n- Enforce kill criteria with no waivers unless the criterion is eliminated.\n\n**Step 7 — Reproducibility and logging (make it replayable)**  \nBorrow the baseline framework’s discipline: snapshot inputs, version schemas, and make results reproducible. fileciteturn0file0L255-L337  \nConcrete logging practices:\n- Notes as a structured “evidence ledger”: every claim links to (source, date, hash/snapshot).\n- Dataset freeze: block heights for on-chain pulls; query IDs; exchange snapshots with timestamps.\n- Assumption registry: every threshold you used and why; what would change your conclusion.\n\n**Workflow process diagram (template)**\n\n```mermaid\nflowchart TD\n  A[Intake: mandate & constraints] --> B[Hypotheses per section]\n  B --> C[Primary source collection + snapshot]\n  C --> D[On-chain & market data collection]\n  D --> E[Section tests + stress tests]\n  E --> F[Bias checks module]\n  F --> G[Scoring + weighting]\n  G --> H{Kill criteria triggered?}\n  H -- Yes --> I[Reject / Tactical-only scope]\n  H -- No --> J[IC memo + monitoring plan]\n```", "tags": []}
{"fragment_id": "F_R4_1730_1740", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1730-L1740", "text": "### Disconfirming evidence checklist (thesis abandonment triggers)\n\nUse these as **pre-committed exits** (tailor per asset/strategy):\n- Evidence that usage is primarily Sybil/incentive driven (post-incentive collapse).\n- Discovery of unmitigated upgrade/admin control that can seize/brick funds.\n- New critical security incident revealing unknown class risk or weak ops.\n- Material regulatory change that blocks custody/listing for your investor base.\n- Token unlocks + weak liquidity imply unavoidable dilution overhang at your target size.\n- Governance capture event (single coalition repeatedly passes self-serving proposals).\n- Value accrual changes against tokenholder thesis (take rate collapses or value leaks externally).", "tags": []}
{"fragment_id": "F_R4_1741_1760", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1741-L1760", "text": "### Diligence plan by month (30/60/90-day)\n\n**Day 0–30 (Foundations + kill criteria scans)**\n- Freeze primary docs, contracts, and audit history.\n- Build dependency map + admin/upgradeability review.\n- Create token unlock calendar + float model.\n- Stand up baseline on-chain dashboards (leading vs lagging metrics).\n- Run wash trading / liquidity integrity screens. citeturn9search0turn9search4\n\n**Day 31–60 (Deep tests + scenario modeling)**\n- Incentive/game-theory attack surface review (oracle, MEV, governance capture). citeturn0search1turn8search3  \n- Governance and decentralization metrics (multi-dimensional). citeturn2search2turn2search10  \n- Valuation triangulation (3 methods) + sensitivity ranges.\n- Regulatory exposure mapping + distribution constraints (jurisdictional). citeturn0search7turn1search11\n\n**Day 61–90 (IC memo + monitoring system)**\n- Final scoring + weighted decision + explicit disconfirm triggers.\n- “Live monitoring” plan: dashboards + alerts + monthly re-score of top 5 risk sections.\n- Operational plan: execution constraints, custody, venue selection, hedging policy.", "tags": []}
{"fragment_id": "F_R4_1761_1774", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1761-L1774", "text": "**Analyst task checklist (copy/paste)**\n- [ ] Mandate constraints declared; kill criteria written (no waivers).  \n- [ ] Section hypotheses written (confirm/disconfirm/time window).  \n- [ ] Primary sources snapshotted; contracts verified at block heights.  \n- [ ] Tokenomics model: emissions + unlocks + sinks + concentration.  \n- [ ] Security: audits + testing depth + admin/upgradeability + bounty.  \n- [ ] Dependency map: oracles/bridges/infra; failure-mode classification.  \n- [ ] On-chain metrics: leading indicators defined; Sybil/incentive adjustments applied.  \n- [ ] Liquidity integrity: executable depth/spread; wash trading screens completed. citeturn9search0turn9search4  \n- [ ] Regulatory map completed; distribution constraints captured. citeturn0search3turn1search11  \n- [ ] Scoring completed; weighted score computed; IC summary drafted.\n\n---", "tags": []}
{"fragment_id": "F_R4_1775_1780", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1775-L1780", "text": "## FULL SOURCE [R4]\n\nThe document describes a **complete research operating system** for building systematic crypto trading strategies that are scientifically valid, execution-realistic, and reproducible. Below is a structured explanation of what it is doing, why each part exists, and how the pieces connect logically.\n\n---", "tags": []}
{"fragment_id": "F_R4_1781_1809", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1781-L1809", "text": "# 1. Core Idea (What This Framework Actually Is)\n\nThis is **not a trading strategy**.\n\nIt is a **research architecture** whose purpose is:\n\n> Convert raw market mechanics → statistically valid trading edges → executable strategies → scalable portfolios, while preventing false discoveries and unrealistic backtests.\n\nThe framework enforces four principles everywhere:\n\n1. **Point-in-time reality**\n\n   * Decisions only use information available at that moment.\n   * Prevents lookahead bias.\n\n2. **Mechanism-first modeling**\n\n   * Funding, mark price, liquidations are **market mechanics**, not indicators.\n   * Edges arise from structural constraints, not patterns.\n\n3. **Execution realism**\n\n   * Profit = after fees + slippage + impact.\n   * Paper alpha without execution viability is rejected.\n\n4. **Deterministic reproducibility**\n\n   * Every result can be replayed exactly using hashes + versions.", "tags": []}
{"fragment_id": "F_R4_1810_1816", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1810-L1816", "text": "These constraints define the entire pipeline.\n(Everything else is implementation detail.)\n\nSource definition: \n\n---", "tags": []}
{"fragment_id": "F_R4_1817_1834", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1817-L1834", "text": "# 2. The Research Objects (Conceptual Data Model)\n\nThe framework formalizes trading research into four objects:\n\n| Object             | Meaning                                          |\n| ------------------ | ------------------------------------------------ |\n| **Event**          | Something measurable happens in market structure |\n| **Candidate Edge** | A rule reacting to that event                    |\n| **Strategy Spec**  | Fully executable rule set                        |\n| **Portfolio Spec** | Allocation across strategies                     |\n\nThis is critical:\n\n> You do not search for strategies.\n> You search for **conditional reactions to events**.\n\n---", "tags": []}
{"fragment_id": "F_R4_1835_1841", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1835-L1841", "text": "# 3. Pipeline Overview (High-Level Flow)\n\nThe system runs in five stages:\n\n```\nMarket Data\n     ↓", "tags": []}
{"fragment_id": "F_R4_1842_1843", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1842-L1843", "text": "DISCOVERY\n     ↓", "tags": []}
{"fragment_id": "F_R4_1844_1845", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1844-L1845", "text": "VALIDATION\n     ↓", "tags": []}
{"fragment_id": "F_R4_1846_1849", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1846-L1849", "text": "BLUEPRINT\n     ↓\nSTRATEGY\n     ↓", "tags": []}
{"fragment_id": "F_R4_1850_1856", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1850-L1856", "text": "PORTFOLIO\n```\n\nEach stage removes a different class of error.\n\n---", "tags": []}
{"fragment_id": "F_R4_1857_1858", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1857-L1858", "text": "# 4. Phase 1 — Discovery (Finding Possible Edges)", "tags": []}
{"fragment_id": "F_R4_1859_1866", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1859-L1866", "text": "## Goal\n\nCreate **structured hypotheses** without trading yet.\n\nDiscovery builds three components:\n\n---", "tags": []}
{"fragment_id": "F_R4_1867_1899", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1867-L1899", "text": "## 4.1 Event Registry\n\nA registry is a **taxonomy of tradable situations**.\n\nExamples:\n\n* Funding dislocation\n* Liquidation cascade\n* Liquidity shock\n* Volatility regime change\n* Structural break\n\nEach event must have:\n\n* formal trigger\n* required data fields\n* deterministic labeling rule\n\nExample logic:\n\n```\nIF funding_zscore > threshold\nTHEN event = FUNDING_DISLOCATION\n```\n\nKey insight:\n\n> Events must be objectively detectable at time t₀.\n\nOtherwise research becomes narrative fitting.\n\n---", "tags": []}
{"fragment_id": "F_R4_1900_1929", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1900-L1929", "text": "## 4.2 Context Deltas\n\nMarkets behave differently depending on state.\n\nInstead of estimating:\n\n```\nE[return | event]\n```\n\nthe framework estimates:\n\n```\nE[return | event AND state change]\n```\n\nState vector includes:\n\n* volatility\n* spread\n* depth\n* order imbalance\n* open interest\n* funding\n* basis\n\nThis prevents averaging incompatible regimes.\n\n---", "tags": []}
{"fragment_id": "F_R4_1930_1952", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1930-L1952", "text": "## 4.3 Invariants (Most Important Concept)\n\nInvariants = relationships markets try to maintain.\n\nExamples:\n\n* Perpetual price anchored to spot via funding\n* Forward vs spot parity\n* Cross-rate consistency\n\nEdges occur when invariants temporarily break **beyond friction bounds**.\n\nCritical rule:\n\n```\nSmall violation → noise\nLarge violation → candidate opportunity\n```\n\nThis avoids overfitting micro fluctuations.\n\n---", "tags": []}
{"fragment_id": "F_R4_1953_1970", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1953-L1970", "text": "# 5. Labeling Logic (How Learning Happens)\n\nEach detected event receives a future outcome label:\n\n[\ny = \\log(P_{t_0+H}/P_{t_0})\n]\n\nImportant separation:\n\n| Allowed                  | Forbidden               |\n| ------------------------ | ----------------------- |\n| Future prices for labels | Future data in features |\n\nThis preserves causal direction.\n\n---", "tags": []}
{"fragment_id": "F_R4_1971_1983", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1971-L1983", "text": "# 6. Phase 2 — Validation (Where Most Ideas Die)\n\nDiscovery generates hypotheses.\nValidation determines whether they survive reality.\n\nA **candidate edge** becomes:\n\n```\n(Event + Context) → Action → Exit → Risk → Parameters\n```\n\n---", "tags": []}
{"fragment_id": "F_R4_1984_2015", "source_id": "R4", "locator": "unified_crypto_research_master.md:L1984-L2015", "text": "## 6.1 Execution Cost Modeling\n\nProfit is measured using **implementation shortfall**:\n\n```\nCost = execution price − decision price + fees\n```\n\nExecution price model:\n\n```\nmid\n+ half spread\n+ slippage\n+ market impact\n```\n\nImpact scales roughly as:\n\n[\nimpact \\propto \\sigma \\sqrt{Q / ADV}\n]\n\nMeaning:\n\n* doubling size does NOT double cost\n* but cost grows nonlinearly\n\nThis introduces capacity limits.\n\n---", "tags": []}
{"fragment_id": "F_R4_2016_2019", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2016-L2019", "text": "## 6.2 Statistical Validity Controls\n\nBecause many ideas are tested:", "tags": []}
{"fragment_id": "F_R4_2020_2038", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2020-L2038", "text": "### Problems addressed\n\n* data snooping\n* multiple testing\n* overfitting\n\nSolutions:\n\n* purged walk-forward splits\n* embargo windows\n* False Discovery Rate correction\n* Deflated Sharpe ratios\n\nInterpretation:\n\n> The framework assumes most discovered edges are false until proven otherwise.\n\n---", "tags": []}
{"fragment_id": "F_R4_2039_2054", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2039-L2054", "text": "## 6.3 Capacity Constraints\n\nAn edge must survive scaling.\n\nConstraint:\n\n[\nQ / Volume \\le \\rho_{max}\n]\n\nIf trading size destroys expectancy → reject.\n\nThis converts research from academic to deployable.\n\n---", "tags": []}
{"fragment_id": "F_R4_2055_2078", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2055-L2078", "text": "# 7. Blueprint Stage (Reproducibility Layer)\n\nBlueprint = executable research contract.\n\nIt records:\n\n* dataset hash\n* registry version\n* parameter bounds\n* code commit\n* RNG seeds\n* fee schedules\n* execution model\n\nResult:\n\n> Any experiment becomes replayable exactly.\n\nThis solves a common failure:\n\n“Strategy works but nobody knows why.”\n\n---", "tags": []}
{"fragment_id": "F_R4_2079_2105", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2079-L2105", "text": "# 8. Strategy Stage (Single Edge Engineering)\n\nNow one validated edge becomes a strategy.\n\nRequirements:\n\n* event-triggered execution\n* time-ordered training\n* sensitivity surfaces\n* stress testing\n\nKey outputs:\n\n* after-cost Sharpe\n* drawdown\n* implementation shortfall\n* regime stability\n* parameter robustness\n\nImportant shift:\n\n```\nOptimization → Robustness measurement\n```\n\n---", "tags": []}
{"fragment_id": "F_R4_2106_2119", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2106-L2119", "text": "# 9. Stress Testing (Crypto-Specific)\n\nStrategies must survive known exchange failures:\n\n* spread widening\n* depth collapse\n* volatility expansion\n* venue outage\n* liquidation cascades\n\nThese are modeled explicitly because crypto microstructure frequently breaks.\n\n---", "tags": []}
{"fragment_id": "F_R4_2120_2144", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2120-L2144", "text": "# 10. Portfolio Stage (Combining Strategies)\n\nStrategies become assets.\n\nPortfolio optimization:\n\n[\n\\max (\\text{return} - risk - transaction_cost)\n]\n\nMethods allowed:\n\n* convex optimization (primary)\n* risk parity\n* capped growth-optimal sizing\n\nAdditional controls:\n\n* correlation shrinkage\n* regime-dependent covariance\n* venue exposure caps\n* liquidity gates\n\n---", "tags": []}
{"fragment_id": "F_R4_2145_2171", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2145-L2171", "text": "# 11. What This Framework Solves (Meta-Level)\n\nTraditional retail quant workflow:\n\n```\nindicator → backtest → optimize → fail live\n```\n\nThis framework replaces it with:\n\n```\nmechanism → event → validated edge → executable system\n```\n\nIt systematically eliminates:\n\n| Failure Mode            | Where Fixed                     |\n| ----------------------- | ------------------------------- |\n| Lookahead bias          | Discovery rules                 |\n| Overfitting             | Validation multiplicity control |\n| Unrealistic fills       | Execution model                 |\n| Non-scalable alpha      | Capacity constraints            |\n| Irreproducible research | Blueprint hashing               |\n| Portfolio fragility     | Regime-aware allocation         |\n\n---", "tags": []}
{"fragment_id": "F_R4_2172_2179", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2172-L2179", "text": "# 12. Conceptual Summary (One Sentence)\n\nThe framework treats trading research as a **controlled scientific experiment pipeline** where:\n\n> market mechanics generate events, events generate hypotheses, hypotheses must survive execution reality and statistical correction before capital allocation.\n\n---", "tags": []}
{"fragment_id": "F_R4_2180_2197", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2180-L2197", "text": "# 13. Structural Insight (Why This Matches Your Existing System)\n\nYour current pipeline (Bridge-First, Phase1→Phase2→Strategy→Portfolio) already mirrors this architecture:\n\n| Document Concept | Your System         |\n| ---------------- | ------------------- |\n| Event Registry   | Phase1 discovery    |\n| Validation       | Phase2 + Bridge     |\n| Blueprint        | compiled candidates |\n| Strategy         | walkforward         |\n| Portfolio        | allocator           |\n\nMeaning:\n\nYou are effectively implementing a **production instantiation** of this theoretical framework.\n\n---", "tags": []}
{"fragment_id": "F_R4_2198_2225", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2198-L2225", "text": "# Next Logical Analysis Step\n\nThe deepest leverage point now is:\n\n**Quantify research efficiency**\n\nSpecifically:\n\n1. Measure:\n\n   ```\n   events detected\n   → candidates tested\n   → edges validated\n   → strategies surviving OOS\n   ```\n2. Estimate false discovery rate empirically.\n3. Set a hard research budget:\n\n   ```\n   max hypotheses per dataset snapshot\n   ```\n\nThis converts the framework from methodology into an optimized research engine.\n\n\n---", "tags": []}
{"fragment_id": "F_R4_2226_2243", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2226-L2243", "text": "## FULL SOURCE [R5]\n\nYou do **not** research “trading strategies.”\nYou research **repeatable market behaviors that can become measurable events**.\n\nYour confusion comes from researching at the wrong abstraction level.\n\nCorrect hierarchy:\n\n```\nMarket behavior  →  Event  →  Edge  →  Strategy\n(research here)              (NOT here)\n```\n\nYou are currently trying to research the last layer first.\n\n---", "tags": []}
{"fragment_id": "F_R4_2244_2257", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2244-L2257", "text": "# 1. What You Are Actually Researching\n\nYour research objective:\n\n> Identify **situations where market behavior changes predictably**.\n\nNot entries.\nNot indicators.\nNot setups.\n\nYou are researching **conditional market physics**.\n\n---", "tags": []}
{"fragment_id": "F_R4_2258_2263", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2258-L2263", "text": "# 2. The Five Research Domains (Only These Matter)\n\nEverything useful in trading discovery fits into these categories.\n\n---", "tags": []}
{"fragment_id": "F_R4_2264_2290", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2264-L2290", "text": "## A. Volatility State Transitions (Highest Priority)\n\nMarkets alternate between:\n\n```\ncompression → expansion → exhaustion → reset\n```\n\nResearch questions:\n\n* When does volatility compress?\n* How long does compression persist?\n* What typically happens after compression?\n* Does expansion direction depend on prior trend?\n\nThings to study:\n\n* range contraction\n* ATR percentile regimes\n* realized volatility clustering\n* session volatility differences\n\nWhy this matters:\nMost edges originate from volatility regime change.\n\n---", "tags": []}
{"fragment_id": "F_R4_2291_2312", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2291-L2312", "text": "## B. Session Microstructure\n\nMarkets behave differently across sessions.\n\nResearch:\n\n* Asian session range characteristics\n* London open displacement\n* NY continuation vs reversal\n* session overlap effects\n* liquidity arrival timing\n\nQuestions:\n\n```\nDoes behavior after NY open depend on Asian range size?\n```\n\nYou are looking for conditional effects.\n\n---", "tags": []}
{"fragment_id": "F_R4_2313_2332", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2313-L2332", "text": "## C. Liquidity & Mean Reversion Dynamics\n\nMarkets frequently move to areas where orders exist.\n\nResearch:\n\n* prior high/low sweeps\n* equal highs/lows\n* overnight highs/lows\n* VWAP distance behavior\n* reversion probability after extension\n\nKey question:\n\n```\nAfter liquidity sweep, what distribution change occurs?\n```\n\n---", "tags": []}
{"fragment_id": "F_R4_2333_2351", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2333-L2351", "text": "## D. Trend Exhaustion / Persistence\n\nTrend is not binary.\n\nResearch:\n\n* trend age\n* slope decay\n* pullback depth statistics\n* continuation probability vs trend maturity\n\nExample question:\n\n```\nAfter 5 consecutive directional candles, what happens next?\n```\n\n---", "tags": []}
{"fragment_id": "F_R4_2352_2367", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2352-L2367", "text": "## E. Market Context Interaction (Advanced)\n\nEdges rarely exist alone.\n\nResearch interactions:\n\n```\nlow volatility + session change\ntrend + liquidity sweep\ncompression + macro session open\n```\n\nEdges often appear only when conditions combine.\n\n---", "tags": []}
{"fragment_id": "F_R4_2368_2382", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2368-L2382", "text": "# 3. What NOT to Research (Huge Time Saver)\n\nAvoid:\n\n❌ indicators (RSI, MACD, etc.)\n❌ entry techniques\n❌ YouTube strategies\n❌ parameter optimization\n❌ ML price prediction papers\n❌ signal generation methods\n\nThose belong AFTER edge discovery.\n\n---", "tags": []}
{"fragment_id": "F_R4_2383_2407", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2383-L2407", "text": "# 4. How Research Should Look (Correct Form)\n\nBad research:\n\n> “Breakouts work in NY.”\n\nGood research:\n\n```\nObservation:\nWhen Asian range percentile < 30,\nNY session produces larger-than-average expansion.\n\nVariables:\n- asian_range_percentile\n- NY_session_flag\n\nHypothesis:\nconditional_range_expansion\n```\n\nResearch output must be convertible into variables.\n\n---", "tags": []}
{"fragment_id": "F_R4_2408_2415", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2408-L2415", "text": "# 5. Research Template (Use This Exactly)\n\nEvery research note should follow:\n\n```\nBEHAVIOR:\nWhat market phenomenon exists?", "tags": []}
{"fragment_id": "F_R4_2416_2418", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2416-L2418", "text": "CONDITION:\nUnder what measurable state?", "tags": []}
{"fragment_id": "F_R4_2419_2424", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2419-L2424", "text": "MEASUREMENT:\nWhat changes afterward?\n\nHORIZON:\nHow far into future?", "tags": []}
{"fragment_id": "F_R4_2425_2448", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2425-L2448", "text": "EXPECTED EFFECT:\nExpansion / reversion / drift / volatility change\n```\n\nExample:\n\n```\nBehavior:\nVolatility expansion after compression\n\nCondition:\nATR_20 percentile < 0.25\n\nMeasurement:\nForward 32-bar range\n\nExpected:\nRange increase\n```\n\nIf you cannot fill all fields → not usable research.\n\n---", "tags": []}
{"fragment_id": "F_R4_2449_2452", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2449-L2452", "text": "# 6. Sources Worth Studying (High Signal Only)\n\nFocus on structural knowledge, not strategies.", "tags": []}
{"fragment_id": "F_R4_2453_2458", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2453-L2458", "text": "### Microstructure & Volatility\n\n* volatility clustering research\n* market regime switching\n* realized volatility studies", "tags": []}
{"fragment_id": "F_R4_2459_2464", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2459-L2464", "text": "### Quantitative Trading Concepts\n\n* conditional return distributions\n* event studies in finance\n* regime-based models", "tags": []}
{"fragment_id": "F_R4_2465_2474", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2465-L2474", "text": "### Practical Data Observations\n\n* session statistics\n* range behavior\n* trend persistence analysis\n\nYou are extracting *ideas for measurable conditions*, not copying methods.\n\n---", "tags": []}
{"fragment_id": "F_R4_2475_2496", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2475-L2496", "text": "# 7. What Your First Research Week Should Produce\n\nGoal is NOT profit ideas.\n\nGoal output:\n\n```\n10–20 hypotheses like:\n```\n\n```\nH1: Low Asian volatility → NY expansion\nH2: Liquidity sweep → short-term reversal\nH3: Extreme VWAP distance → mean reversion\nH4: Compression duration predicts expansion size\nH5: Trend maturity reduces continuation probability\n```\n\nThat alone is enough to build the system.\n\n---", "tags": []}
{"fragment_id": "F_R4_2497_2514", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2497-L2514", "text": "# 8. Why This Works\n\nYou are separating:\n\n```\nDiscovery (science)\nfrom\nExecution (engineering)\n```\n\nMost traders mix them and overfit.\n\nYou instead measure market laws first.\n\nStrategies become trivial later.\n\n---", "tags": []}
{"fragment_id": "F_R4_2515_2528", "source_id": "R4", "locator": "unified_crypto_research_master.md:L2515-L2528", "text": "# 9. Practical Starting List (Research Immediately)\n\nStart investigating these **five concrete behaviors**:\n\n1. Asian session range percentile vs NY range expansion\n2. ATR compression duration vs breakout magnitude\n3. Distance from VWAP vs reversion probability\n4. Prior high/low sweep vs next 16-bar return\n5. Consecutive directional candles vs reversal probability\n\nEach maps cleanly into event discovery.\n\n---", "tags": []}
{"fragment_id": "F_R5_1_4", "source_id": "R5", "locator": "chattrading.txt:L1-L4", "text": "The document describes a **complete research operating system** for building systematic crypto trading strategies that are scientifically valid, execution-realistic, and reproducible. Below is a structured explanation of what it is doing, why each part exists, and how the pieces connect logically.\n\n---", "tags": []}
{"fragment_id": "F_R5_5_33", "source_id": "R5", "locator": "chattrading.txt:L5-L33", "text": "# 1. Core Idea (What This Framework Actually Is)\n\nThis is **not a trading strategy**.\n\nIt is a **research architecture** whose purpose is:\n\n> Convert raw market mechanics → statistically valid trading edges → executable strategies → scalable portfolios, while preventing false discoveries and unrealistic backtests.\n\nThe framework enforces four principles everywhere:\n\n1. **Point-in-time reality**\n\n   * Decisions only use information available at that moment.\n   * Prevents lookahead bias.\n\n2. **Mechanism-first modeling**\n\n   * Funding, mark price, liquidations are **market mechanics**, not indicators.\n   * Edges arise from structural constraints, not patterns.\n\n3. **Execution realism**\n\n   * Profit = after fees + slippage + impact.\n   * Paper alpha without execution viability is rejected.\n\n4. **Deterministic reproducibility**\n\n   * Every result can be replayed exactly using hashes + versions.", "tags": []}
{"fragment_id": "F_R5_34_40", "source_id": "R5", "locator": "chattrading.txt:L34-L40", "text": "These constraints define the entire pipeline.\n(Everything else is implementation detail.)\n\nSource definition: \n\n---", "tags": []}
{"fragment_id": "F_R5_41_58", "source_id": "R5", "locator": "chattrading.txt:L41-L58", "text": "# 2. The Research Objects (Conceptual Data Model)\n\nThe framework formalizes trading research into four objects:\n\n| Object             | Meaning                                          |\n| ------------------ | ------------------------------------------------ |\n| **Event**          | Something measurable happens in market structure |\n| **Candidate Edge** | A rule reacting to that event                    |\n| **Strategy Spec**  | Fully executable rule set                        |\n| **Portfolio Spec** | Allocation across strategies                     |\n\nThis is critical:\n\n> You do not search for strategies.\n> You search for **conditional reactions to events**.\n\n---", "tags": []}
{"fragment_id": "F_R5_59_65", "source_id": "R5", "locator": "chattrading.txt:L59-L65", "text": "# 3. Pipeline Overview (High-Level Flow)\n\nThe system runs in five stages:\n\n```\nMarket Data\n     ↓", "tags": []}
{"fragment_id": "F_R5_66_67", "source_id": "R5", "locator": "chattrading.txt:L66-L67", "text": "DISCOVERY\n     ↓", "tags": []}
{"fragment_id": "F_R5_68_69", "source_id": "R5", "locator": "chattrading.txt:L68-L69", "text": "VALIDATION\n     ↓", "tags": []}
{"fragment_id": "F_R5_70_73", "source_id": "R5", "locator": "chattrading.txt:L70-L73", "text": "BLUEPRINT\n     ↓\nSTRATEGY\n     ↓", "tags": []}
{"fragment_id": "F_R5_74_80", "source_id": "R5", "locator": "chattrading.txt:L74-L80", "text": "PORTFOLIO\n```\n\nEach stage removes a different class of error.\n\n---", "tags": []}
{"fragment_id": "F_R5_81_82", "source_id": "R5", "locator": "chattrading.txt:L81-L82", "text": "# 4. Phase 1 — Discovery (Finding Possible Edges)", "tags": []}
{"fragment_id": "F_R5_83_90", "source_id": "R5", "locator": "chattrading.txt:L83-L90", "text": "## Goal\n\nCreate **structured hypotheses** without trading yet.\n\nDiscovery builds three components:\n\n---", "tags": []}
{"fragment_id": "F_R5_91_123", "source_id": "R5", "locator": "chattrading.txt:L91-L123", "text": "## 4.1 Event Registry\n\nA registry is a **taxonomy of tradable situations**.\n\nExamples:\n\n* Funding dislocation\n* Liquidation cascade\n* Liquidity shock\n* Volatility regime change\n* Structural break\n\nEach event must have:\n\n* formal trigger\n* required data fields\n* deterministic labeling rule\n\nExample logic:\n\n```\nIF funding_zscore > threshold\nTHEN event = FUNDING_DISLOCATION\n```\n\nKey insight:\n\n> Events must be objectively detectable at time t₀.\n\nOtherwise research becomes narrative fitting.\n\n---", "tags": []}
{"fragment_id": "F_R5_124_153", "source_id": "R5", "locator": "chattrading.txt:L124-L153", "text": "## 4.2 Context Deltas\n\nMarkets behave differently depending on state.\n\nInstead of estimating:\n\n```\nE[return | event]\n```\n\nthe framework estimates:\n\n```\nE[return | event AND state change]\n```\n\nState vector includes:\n\n* volatility\n* spread\n* depth\n* order imbalance\n* open interest\n* funding\n* basis\n\nThis prevents averaging incompatible regimes.\n\n---", "tags": []}
{"fragment_id": "F_R5_154_176", "source_id": "R5", "locator": "chattrading.txt:L154-L176", "text": "## 4.3 Invariants (Most Important Concept)\n\nInvariants = relationships markets try to maintain.\n\nExamples:\n\n* Perpetual price anchored to spot via funding\n* Forward vs spot parity\n* Cross-rate consistency\n\nEdges occur when invariants temporarily break **beyond friction bounds**.\n\nCritical rule:\n\n```\nSmall violation → noise\nLarge violation → candidate opportunity\n```\n\nThis avoids overfitting micro fluctuations.\n\n---", "tags": []}
{"fragment_id": "F_R5_177_194", "source_id": "R5", "locator": "chattrading.txt:L177-L194", "text": "# 5. Labeling Logic (How Learning Happens)\n\nEach detected event receives a future outcome label:\n\n[\ny = \\log(P_{t_0+H}/P_{t_0})\n]\n\nImportant separation:\n\n| Allowed                  | Forbidden               |\n| ------------------------ | ----------------------- |\n| Future prices for labels | Future data in features |\n\nThis preserves causal direction.\n\n---", "tags": []}
{"fragment_id": "F_R5_195_207", "source_id": "R5", "locator": "chattrading.txt:L195-L207", "text": "# 6. Phase 2 — Validation (Where Most Ideas Die)\n\nDiscovery generates hypotheses.\nValidation determines whether they survive reality.\n\nA **candidate edge** becomes:\n\n```\n(Event + Context) → Action → Exit → Risk → Parameters\n```\n\n---", "tags": []}
{"fragment_id": "F_R5_208_239", "source_id": "R5", "locator": "chattrading.txt:L208-L239", "text": "## 6.1 Execution Cost Modeling\n\nProfit is measured using **implementation shortfall**:\n\n```\nCost = execution price − decision price + fees\n```\n\nExecution price model:\n\n```\nmid\n+ half spread\n+ slippage\n+ market impact\n```\n\nImpact scales roughly as:\n\n[\nimpact \\propto \\sigma \\sqrt{Q / ADV}\n]\n\nMeaning:\n\n* doubling size does NOT double cost\n* but cost grows nonlinearly\n\nThis introduces capacity limits.\n\n---", "tags": []}
{"fragment_id": "F_R5_240_243", "source_id": "R5", "locator": "chattrading.txt:L240-L243", "text": "## 6.2 Statistical Validity Controls\n\nBecause many ideas are tested:", "tags": []}
{"fragment_id": "F_R5_244_262", "source_id": "R5", "locator": "chattrading.txt:L244-L262", "text": "### Problems addressed\n\n* data snooping\n* multiple testing\n* overfitting\n\nSolutions:\n\n* purged walk-forward splits\n* embargo windows\n* False Discovery Rate correction\n* Deflated Sharpe ratios\n\nInterpretation:\n\n> The framework assumes most discovered edges are false until proven otherwise.\n\n---", "tags": []}
{"fragment_id": "F_R5_263_278", "source_id": "R5", "locator": "chattrading.txt:L263-L278", "text": "## 6.3 Capacity Constraints\n\nAn edge must survive scaling.\n\nConstraint:\n\n[\nQ / Volume \\le \\rho_{max}\n]\n\nIf trading size destroys expectancy → reject.\n\nThis converts research from academic to deployable.\n\n---", "tags": []}
{"fragment_id": "F_R5_279_302", "source_id": "R5", "locator": "chattrading.txt:L279-L302", "text": "# 7. Blueprint Stage (Reproducibility Layer)\n\nBlueprint = executable research contract.\n\nIt records:\n\n* dataset hash\n* registry version\n* parameter bounds\n* code commit\n* RNG seeds\n* fee schedules\n* execution model\n\nResult:\n\n> Any experiment becomes replayable exactly.\n\nThis solves a common failure:\n\n“Strategy works but nobody knows why.”\n\n---", "tags": []}
{"fragment_id": "F_R5_303_329", "source_id": "R5", "locator": "chattrading.txt:L303-L329", "text": "# 8. Strategy Stage (Single Edge Engineering)\n\nNow one validated edge becomes a strategy.\n\nRequirements:\n\n* event-triggered execution\n* time-ordered training\n* sensitivity surfaces\n* stress testing\n\nKey outputs:\n\n* after-cost Sharpe\n* drawdown\n* implementation shortfall\n* regime stability\n* parameter robustness\n\nImportant shift:\n\n```\nOptimization → Robustness measurement\n```\n\n---", "tags": []}
{"fragment_id": "F_R5_330_343", "source_id": "R5", "locator": "chattrading.txt:L330-L343", "text": "# 9. Stress Testing (Crypto-Specific)\n\nStrategies must survive known exchange failures:\n\n* spread widening\n* depth collapse\n* volatility expansion\n* venue outage\n* liquidation cascades\n\nThese are modeled explicitly because crypto microstructure frequently breaks.\n\n---", "tags": []}
{"fragment_id": "F_R5_344_368", "source_id": "R5", "locator": "chattrading.txt:L344-L368", "text": "# 10. Portfolio Stage (Combining Strategies)\n\nStrategies become assets.\n\nPortfolio optimization:\n\n[\n\\max (\\text{return} - risk - transaction_cost)\n]\n\nMethods allowed:\n\n* convex optimization (primary)\n* risk parity\n* capped growth-optimal sizing\n\nAdditional controls:\n\n* correlation shrinkage\n* regime-dependent covariance\n* venue exposure caps\n* liquidity gates\n\n---", "tags": []}
{"fragment_id": "F_R5_369_395", "source_id": "R5", "locator": "chattrading.txt:L369-L395", "text": "# 11. What This Framework Solves (Meta-Level)\n\nTraditional retail quant workflow:\n\n```\nindicator → backtest → optimize → fail live\n```\n\nThis framework replaces it with:\n\n```\nmechanism → event → validated edge → executable system\n```\n\nIt systematically eliminates:\n\n| Failure Mode            | Where Fixed                     |\n| ----------------------- | ------------------------------- |\n| Lookahead bias          | Discovery rules                 |\n| Overfitting             | Validation multiplicity control |\n| Unrealistic fills       | Execution model                 |\n| Non-scalable alpha      | Capacity constraints            |\n| Irreproducible research | Blueprint hashing               |\n| Portfolio fragility     | Regime-aware allocation         |\n\n---", "tags": []}
{"fragment_id": "F_R5_396_403", "source_id": "R5", "locator": "chattrading.txt:L396-L403", "text": "# 12. Conceptual Summary (One Sentence)\n\nThe framework treats trading research as a **controlled scientific experiment pipeline** where:\n\n> market mechanics generate events, events generate hypotheses, hypotheses must survive execution reality and statistical correction before capital allocation.\n\n---", "tags": []}
{"fragment_id": "F_R5_404_421", "source_id": "R5", "locator": "chattrading.txt:L404-L421", "text": "# 13. Structural Insight (Why This Matches Your Existing System)\n\nYour current pipeline (Bridge-First, Phase1→Phase2→Strategy→Portfolio) already mirrors this architecture:\n\n| Document Concept | Your System         |\n| ---------------- | ------------------- |\n| Event Registry   | Phase1 discovery    |\n| Validation       | Phase2 + Bridge     |\n| Blueprint        | compiled candidates |\n| Strategy         | walkforward         |\n| Portfolio        | allocator           |\n\nMeaning:\n\nYou are effectively implementing a **production instantiation** of this theoretical framework.\n\n---", "tags": []}
{"fragment_id": "F_R5_422_445", "source_id": "R5", "locator": "chattrading.txt:L422-L445", "text": "# Next Logical Analysis Step\n\nThe deepest leverage point now is:\n\n**Quantify research efficiency**\n\nSpecifically:\n\n1. Measure:\n\n   ```\n   events detected\n   → candidates tested\n   → edges validated\n   → strategies surviving OOS\n   ```\n2. Estimate false discovery rate empirically.\n3. Set a hard research budget:\n\n   ```\n   max hypotheses per dataset snapshot\n   ```\n\nThis converts the framework from methodology into an optimized research engine.", "tags": []}
{"fragment_id": "F_R6_1_8", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L1-L8", "text": "Full‑Stack Research Framework for Systematic Crypto Portfolios\n1 System Architecture\n1.1 End‑to‑end components\n\nA small, systematic crypto operator requires a pipeline that transforms raw market events into validated, capacity‑aware strategies and finally allocates them to a portfolio. The architecture must be point‑in‑time (PIT) with no look‑ahead; all features at decision time must use only data available at time $t_0$; labels use data after $t_0$ but are never leaked into features. Components include:\n\nIngestion/Canonicalization – Collect raw trade, order book and funding data from multiple venues (CEX mandatory; DEX optional). Canonicalize into a time‑sorted event stream with uniform identifiers and time stamps (e.g., UTC timestamps truncated/rounded to the exchange’s minimum latency). Store raw fields for spot and perpetual contracts (see §2). Include venue‑specific fee schedules and tier changes as time‑stamped records.", "tags": []}
{"fragment_id": "F_R6_9_12", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L9-L12", "text": "PIT feature engine – Compute features only from events with timestamps ≤ $t_0$. For example, order‑book imbalance $\\text{Imb}_t=(V^b_t - V^a_t)/(V^b_t + V^a_t)$ as the difference between best‑bid and best‑ask volumes divided by their sum. Funding‑related features (premium index, mark‑index deviation, etc.) rely on canonical definitions (§2). Derive microstructure features such as spreads, depth, volume buckets, realized volatility, liquidation count, OI changes, etc.\n\nEvent registry – A versioned database that defines event types (funding dislocation, basis dislocation, liquidation cascade, volatility shock, liquidity shock, structural break, regime shift, microstructure imbalance shock). Each event type has deterministic triggers expressed as functions of PIT features (e.g., funding dislocation when premium index + interest exceeds a threshold; order‑book imbalance shock when $|\\text{Imb}_t|>0.6$). Each registry entry contains the required PIT fields and triggers, the look‑back horizon for context, and a unique ID.", "tags": []}
{"fragment_id": "F_R6_13_18", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L13-L18", "text": "Labeling module – For each event $e$ at time $t_0$, compute horizon‑explicit labels $y(e,H)=\\log(P_{t_0+H}/P_{t_0})$ with $P$ using spot or mark price; optionally compute path diagnostics (max drawdown, realized volatility). The labeling uses data strictly after $t_0$.\n\nValidation/backtesting engine – Evaluate candidate strategies under realistic execution and capacity constraints (§4). Use purged and embargoed cross‑validation to avoid look‑ahead and overlapping horizons. Implement deterministic simulation of market/IOC orders with explicit and implicit costs (half‑spread crossing, slippage buckets, square‑root impact cost). Version run parameters (code commit hash, container digest, dataset snapshot hash, RNG seeds) to ensure deterministic replay.\n\nPortfolio allocator – Combine validated strategies into a portfolio subject to capacity, liquidity and risk gates (§6). Use a cost‑aware optimisation (e.g., convex risk parity or turnover‑penalised mean‑variance) that incorporates expected after‑cost returns and covariance. Enforce deterministic risk gates (exposure caps, venue‑mechanism limits, liquidity gates).", "tags": []}
{"fragment_id": "F_R6_19_24", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L19-L24", "text": "Execution simulator – Simulate order execution at various venues, modelling maker/taker fees, latency, partial fills and slippage. Use point‑in‑time fee schedules and square‑root impact models: the market impact of a meta‑order of size $n$ shares with volatility $\\sigma$ and daily turnover $\\nu$ is $\\Delta P = c,\\sigma,\\sqrt{n/\\nu}$. Include participation constraints: participation rate $\\rho=|Q|/V(t_0, t_0+T_{\\mathrm{exec}})$ must be ≤ $\\rho_{\\max}$.\n\nReporting/audit – Generate run manifests with dataset hashes, registry versions, strategy parameters, and results. Store result tables with after‑cost returns, risk, capacity, sensitivity maps and stress‑test diagnostics. Provide acceptance tests for “no future reads” (fail if a feature references $t>t_0$), and deterministic replay tests (re‑run pipeline with identical seeds yields identical output).\n\n1.2 Deterministic replay and versioning", "tags": []}
{"fragment_id": "F_R6_25_36", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L25-L36", "text": "To enforce reproducibility and auditability:\n\nDataset snapshot – Each dataset (spot, perp, funding, order book) is versioned by a snapshot ID and hash. Snapshots are immutable; new data are appended as new versions.\n\nEvent registry version – Each registry has a version number and a hash of event definitions. Changing triggers increments the version.\n\nCode and environment – Record the git commit ID, container image digest, and library versions. Freeze external dependencies.\n\nRun manifest – Create a YAML/JSON manifest capturing dataset versions, registry version, parameter grid, cost model configuration, capacity constraints, RNG seeds and run timestamp. Compute a run hash from these fields.\n\nAcceptance tests – Unit tests assert that (a) no feature uses data after the decision timestamp; (b) repeated runs with the same manifest produce identical outputs; (c) event triggers produce the same set of events across runs.", "tags": []}
{"fragment_id": "F_R6_37_85", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L37-L85", "text": "2 Data Layer & Venue Contracts\n2.1 Normalized schema\n\nA unified schema should support spot instruments (BTC, ETH, stablecoins) and perpetual contracts. Each record contains:\n\nField\tType\tDescription\nts\tdatetime (UTC)\tevent timestamp (microsecond resolution)\nvenue_id\tstring\texchange identifier (e.g., Binance, Coinbase, Bybit)\ninstrument_id\tstring\tunique symbol (e.g., BTC‑USDT spot or BTC‑PERP)\ntype\tenum {trade, book, funding, index, mark, oi, liquidation, fee_change}\tcategory of event\nprice_bid, price_ask\tfloat\tbest bid/ask price (spot or perp)\nprice_last\tfloat\tlast traded price\nprice_mid\tfloat\tmid‑price (mean of best bid and ask)\nprice_mark\tfloat (perps)\tmark price used for PnL and liquidation; computed from index price + funding basis\nprice_index\tfloat (perps)\tindex price: weighted average of spot prices across venues\nspread\tfloat\tbid–ask spread (ask – bid)\ndepth_bid, depth_ask\tfloat\taggregated volume at best bid/ask\nobi (order‑book imbalance)\tfloat\t$(V^b_t - V^a_t)/(V^b_t + V^a_t)$\nvolume_traded\tfloat\ttrade volume in base currency\nopen_interest\tfloat (perps)\ttotal open interest\nfunding_rate\tfloat (perps)\tperiodic funding rate; positive when longs pay shorts\npremium_index\tfloat (perps)\tdifference between mark and index price; formula: \n𝑃\n=\nmax\n⁡\n(\n0\n,\nImpactBid\n−\nIndex\n)\n−\nmax\n⁡\n(\n0\n,\nIndex\n−\nImpactAsk\n)\nIndex\nP=\nIndex\nmax(0,ImpactBid−Index)−max(0,Index−ImpactAsk)\n\t​", "tags": []}
{"fragment_id": "F_R6_86_98", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L86-L98", "text": "funding_interval\tinteger (minutes)\tinterval at which funding is exchanged (e.g., 8 h)\nfee_tiers\tobject\tmaker/taker fee schedule applicable at time ts\nliquidations\tinteger\tnumber of liquidation events in interval\noi_change\tfloat\tchange in open interest\nevent_metadata\tJSON\traw fields not covered above (e.g., impact bid/ask price, index constituents, instrument‑specific parameters)\n2.2 Venue contract handling\n\nRaw fields + normalization – Store raw venue messages (trade, order book changes, funding updates). Apply venue‑specific normalization to unify naming conventions and decimals; do not derive derived values (e.g., funding rate or mark price) from formulas; instead, store the values published by the venue along with reference formulas/metadata for reproducibility.\n\nMaker/taker fees – Capture the tiered fee schedule as a time‑series table with fields: ts_start, ts_end, tier, maker_fee, taker_fee. Link to accounts via volume tiers.", "tags": []}
{"fragment_id": "F_R6_99_109", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L99-L109", "text": "Perp anchoring & carry parity bounds – Check that the mark price is close to the index price adjusted for funding basis. A large deviation indicates a candidate event (funding dislocation). For a fair perpetual, $\\text{mark price} \\approx \\text{index price}+\\text{funding basis}$.\n\nTriangular consistency – For multi‑asset pairs, ensure that price relationships (e.g., BTC/USDT × ETH/BTC = ETH/USDT) hold within tolerance; flag arbitrage events when broken.\n\nBounds‑first gating – Each data feed enters through a bounds check: fields must lie within plausible ranges (e.g., spreads ≥ 0, funding rates within ±5 bp per interval). Violations may indicate data errors (to be cleaned) or extreme market events. Decision rules should classify observations failing bounds but meeting event triggers as candidate events requiring manual review.\n\n3 Phase 1 — Discovery\n3.1 Versioned event registry\n\nDefine a schema for the event registry. Each entry has:", "tags": []}
{"fragment_id": "F_R6_110_131", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L110-L131", "text": "event_id: unique identifier\nname: string\ntrigger_logic: expression using PIT features\ncontext_fields: list of PIT fields needed\nlookback: duration to compute context state vector x_{i,v}(t)\nlabel_horizons: list of forecast horizons H (e.g., [1h, 4h, 1d])\nnotes: description and references\n\n\nEvent types (examples):\n\nID\tEvent type\tTrigger example\nFND_DISLOC\tFunding dislocation\tPremium index + interest rate > threshold or < –threshold\nBASIS_DISLOC\tSpot–perp basis shock\t(Perp mark – spot index)/spot index beyond ±k * roll yield\nLIQ_CASCADE\tLiquidation cascade\tNumber of liquidations in a short window > quantile threshold; OI drops sharply\nVOL_SHOCK\tVolatility shock\tRealized volatility > percentile; implied vol skew jumps\nLIQ_SHOCK\tLiquidity shock\tSpread × depth ratio increases; order‑book imbalance absolute value > 0.6\nSTRUCT_BREAK\tStructural break\tStatistical test (e.g., Chow test) signals break point in price/funding/regime\nREGIME_SHIFT\tLatent regime shift\tHidden Markov model or Markov‑switching detection of new regime\nMICRO_IMBAL\tMicrostructure imbalance\tOrder‑book imbalance crosses regimes; queue imbalance persists beyond look‑back period\n3.2 Context state vector and labeling", "tags": []}
{"fragment_id": "F_R6_132_141", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L132-L141", "text": "For each instrument $i$ and venue $v$ at time $t$, define a context state vector $x_{i,v}(t)$ containing features such as spreads, depth, order‑book imbalance, realized volatility, funding rates, premium index, open interest, and cross‑asset signals. Also compute the delta $\\Delta x(t_0;\\tau) = x(t_0) - x(t_0 - \\tau)$ for multiple look‑back windows ($\\tau$ may vary from minutes to days). The event registry specifies which features to compute.\n\nLabel each event $e$ with horizon‑explicit labels:\n\nPrice label: $y(e;H) = \\log(P_{t_0+H}/P_{t_0})$ using the mark price for perps or mid price for spot.\n\nPath diagnostics: maximum adverse excursion, maximum favourable excursion, realized volatility and volume during $[t_0, t_0+H]$.\n\nRegime label: assign deterministic regimes by break point detection or Markov‑switching (e.g., low‑vol, high‑vol, trending). The context features feed into this classification.", "tags": []}
{"fragment_id": "F_R6_142_142", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L142-L142", "text": "3.3 Pseudocode for PIT feature computation", "tags": []}
{"fragment_id": "F_R6_143_143", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L143-L143", "text": "# Inputs: event time t0, lookback window L, raw time-sorted data D", "tags": []}
{"fragment_id": "F_R6_144_146", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L144-L146", "text": "# Output: feature vector features[t0] without lookahead\n\ndef compute_features(t0, L, D):", "tags": []}
{"fragment_id": "F_R6_147_148", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L147-L148", "text": "# slice data up to decision time\n    data_past = D[D.ts <= t0]", "tags": []}
{"fragment_id": "F_R6_149_151", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L149-L151", "text": "# compute aggregated microstructure features\n    past_window = data_past[data_past.ts >= t0 - L]\n    features = {}", "tags": []}
{"fragment_id": "F_R6_152_155", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L152-L155", "text": "# order book imbalance at t0\n    Vb = past_window.last().depth_bid\n    Va = past_window.last().depth_ask\n    features['obi'] = (Vb - Va) / (Vb + Va)", "tags": []}
{"fragment_id": "F_R6_156_158", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L156-L158", "text": "# spread, depth, realized vol\n    features['spread'] = past_window.last().price_ask - past_window.last().price_bid\n    features['depth_ratio'] = (Vb + Va) / max(1e-9, past_window['volume_traded'].rolling(L).sum())", "tags": []}
{"fragment_id": "F_R6_159_161", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L159-L161", "text": "# funding and premium\n    features['funding_rate'] = past_window.last().funding_rate\n    features['premium_index'] = past_window.last().premium_index", "tags": []}
{"fragment_id": "F_R6_162_184", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L162-L184", "text": "# realized volatility over window\n    returns = np.log(past_window.price_mark).diff().dropna()\n    features['realized_vol'] = returns.std() * sqrt(len(returns))\n    return features\n\n\nThis pattern ensures that only data with ts ≤ t0 are used. Labels are computed in a separate pass using future data.\n\n4 Phase 2 — Validation\n4.1 Candidate edge tuple\n\nEach candidate strategy is defined by a tuple $c=(E,C,A,X,R,\\theta)$ where:\n\nE – event type from registry.\n\nC – context conditions (e.g., region of order‑book imbalance, funding extremes, regime assignment).\n\nA – action (long, short, hedge, do nothing) with entry delay and order type (market/IOC; maker orders optional due to small size). Order quantity may be fixed or proportional to historical volatility/volume.\n\nX – instrument(s) and venue(s) to trade.\n\nR – risk management rules (stop loss, take profit, time stop, position limit, notional cap).", "tags": []}
{"fragment_id": "F_R6_185_198", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L185-L198", "text": "θ – parameter set (thresholds, delays, scaling factors) bounded by pre‑registered ranges.\n\nAll parameters and event types must be pre‑registered before research to avoid p‑hacking. Parameter ranges are specified in the run manifest.\n\n4.2 Splitting and cross‑validation\n\nUse purged and embargoed walk‑forward splitting to avoid overlap and look‑ahead. Purging removes from training any observation whose time interval overlaps the label formation window of test observations. Embargoing applies a temporal buffer after each test fold to prevent spill‑over effects. For each horizon $H$, split the event timeline into $k$ sequential folds. Training uses earlier folds excluding purged intervals; test uses the current fold.\n\n4.3 Execution realism and cost modelling\n\nImplementation shortfall is measured relative to the mid price at decision time. The execution simulator applies:\n\nExplicit fees – Maker/taker fees at the chosen venue (point‑in‑time schedule). If market orders are used, apply taker fee; if maker orders are used, apply maker fee but account for fill probability and opportunity cost.", "tags": []}
{"fragment_id": "F_R6_199_210", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L199-L210", "text": "Spread crossing – Market orders cross half the bid–ask spread. Maker orders attempt to earn the spread but may suffer partial fills.\n\nSlippage buckets – Model slippage as a function of order size relative to recent traded volume; calibrate from historical market impact data.\n\nSquare‑root impact – For larger orders, apply the square‑root law: $\\Delta P = c \\sigma \\sqrt{n/\\nu}$. Parameter $c$ is estimated from historical impact curves; $\\sigma$ is recent volatility; $\\nu$ is average daily turnover. The law is concave; small trades have disproportionate impact.\n\nParticipation constraint – Constrain the participation rate $\\rho = |Q| / V(t_0, t_0+T_{\\mathrm{exec}})$ ≤ $\\rho_{\\max}$ (e.g., 5–10%). If capacity is insufficient, scale down or skip trades.\n\n4.4 Multiplicity and data‑snooping controls\n\nFalse discovery rate (FDR) – Use Benjamini–Hochberg procedure to control the expected proportion of false positives when testing many strategies. FDR allows more power than family‑wise error control; the threshold depends on both the number of tests and the acceptable false‑discovery proportion. For example, testing 1000 strategies at 5% FWER would require t‑statistics > 4, which is overly conservative; FDR allows a relaxed threshold while controlling the proportion of false discoveries.", "tags": []}
{"fragment_id": "F_R6_211_216", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L211-L216", "text": "Reality‑check bootstrap – Apply the White (2000) reality‑check or Hansen (2005) step‑wise bootstrap: simulate the null distribution of performance differences under no edge. Select the best strategy only if its performance exceeds the maximum of bootstrap draws at the desired confidence level.\n\nDeflated Sharpe ratio (DSR) – Adjust Sharpe ratios for multiple testing and non‑normal returns. The DSR corrects for selection bias by estimating the expected maximum Sharpe ratio under $N$ independent trials; it adjusts the observed Sharpe ratio downward. The deflated Sharpe ratio is computed using the probabilistic Sharpe ratio and the variance of Sharpe estimates across trials. Only strategies with DSR above a threshold are retained.\n\nPre‑registration and FDR step‑up – Pre‑register all event types and parameter grids in the manifest; specify the number of hypotheses $N$. Apply FDR step‑up on p‑values across candidates. When performing parameter sweeps for a single event, treat each parameter combination as a separate hypothesis; compute FDR accordingly.", "tags": []}
{"fragment_id": "F_R6_217_236", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L217-L236", "text": "4.5 Rejection criteria\n\nReject a candidate strategy if any of the following holds:\n\nAfter‑cost expected return ≤ 0 in key regimes (bull, bear, high‑vol) using training data.\n\nFails multiplicity control: p‑value adjusted by FDR > α (e.g., 0.05) or DSR below threshold.\n\nCapacity erases more than X% (e.g., 50%) of expectancy when scaled to intended capital. Evaluate capacity by simulating increasing participation rates and measuring impact costs.\n\nFragile under moderate liquidity shocks: stress tests (spread × $k_s$, depth × $k_d$, volatility scaling) show negative expectancy.\n\n5 Strategy Artifact\n\nFor each accepted strategy, produce a formal specification:\n\nEvent‑triggered orders – On event $e$ at time $t_0$, schedule an entry after a deterministic delay (e.g., 1 minute). Choose order type (market/IOC or simple maker). For exit, use time‑based exit (horizon $H$) or stop conditions.\n\nParameter sweeps – Evaluate the strategy across a grid of parameters (thresholds, delays, position size scalars). Record performance metrics and produce stability heatmaps: matrix of parameters vs after‑cost returns and Sharpe ratios. Compute a neighbourhood robustness score: fraction of parameter combinations within the neighbourhood of the optimum that meet selection criteria. Prefer strategies with broad robustness rather than point estimates.", "tags": []}
{"fragment_id": "F_R6_237_251", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L237-L251", "text": "Stress tests – Simulate extreme conditions relevant to crypto:\n\nMultiply spreads by $k_s$ (e.g., 2×, 3×) and depth by $k_d$ (e.g., 0.5×) to mimic liquidity droughts.\n\nIncrease volatility in the impact model (raise $\\sigma$) to test slippage sensitivity.\n\nRandomly drop fills (venue outage) and simulate partial execution.\n\nApply forced deleveraging/ADL scenarios where positions are reduced at adverse prices.\n\nResult table schema – Store results with fields: strategy_id, run_id, parameter_set, horizon, mean_return_after_cost, volatility, max_drawdown, Sharpe, deflated_sharpe, p_value, capacity_adjusted_return, participation_rate, num_trades, hit_ratio, slippage_cost, impact_cost, test_fold_id, regime. The diagnostics checklist includes tests for stationarity, autocorrelation of returns, stability across regimes and time periods, and normality of residuals.\n\n6 Portfolio Layer (Small Systematic Emphasis)\n6.1 Allocation method", "tags": []}
{"fragment_id": "F_R6_252_308", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L252-L308", "text": "Use a cost‑aware convex optimisation or risk‑parity with turnover penalty to allocate capital across strategies. Inputs are expected after‑cost returns $\\mu_i$, covariance matrix $\\Sigma$ and cost coefficients $\\kappa_i$. Solve:\n\nmin\n⁡\n𝑤\n  \n𝑤\n𝑇\nΣ\n𝑤\n−\n𝜆\n \n𝑤\n𝑇\n𝜇\n+\n𝛾\n∑\n𝑖\n𝜅\n𝑖\n∣\nΔ\n𝑤\n𝑖\n∣\nmin\nw\n\t​\n\nw\nT\nΣw−λw\nT\nμ+γ∑\ni\n\t​\n\nκ\ni\n\t​\n\n∣Δw\ni\n\t​\n\n∣\n\nsubject to $\\sum_i w_i = 1$, $0 ≤ w_i ≤ w^{\\max}_i$. Here $\\lambda$ trades off return vs risk; $\\gamma$ penalises turnover; $w^{\\max}_i$ enforces capacity and exposure caps.\n\nAlternatively, implement regime‑conditioned risk parity: estimate separate covariance matrices for each regime (low‑vol, high‑vol) and allocate using the worst‑regime covariance to ensure resilience. Shrink covariance estimates using Ledoit–Wolf or similar shrinkage; incorporate open‑interest‑weighted scaling.\n\n6.2 Portfolio capacity and aggregation\n\nAggregate per‑strategy capacity constraints considering shared instruments and venues. Compute the sum of expected participation across strategies for each instrument and ensure it remains below the venue’s participation cap. When multiple strategies trigger on the same event, schedule them sequentially or allocate a meta‑order across venues to reduce impact. Use dynamic scaling: if aggregated participation exceeds $\\rho_{\\max}$, down‑scale all strategies proportionally.", "tags": []}
{"fragment_id": "F_R6_309_345", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L309-L345", "text": "6.3 Deterministic risk gates\n\nExposure caps – Limit notional exposure per instrument and per side (long/short). For instance, no more than 25% of portfolio NAV in BTC, 25% in ETH; 50% total long or short.\n\nLiquidity gates – Skip trades if instantaneous spread or order‑book depth falls below thresholds. Use signals like spread/depth ratio and obi to determine if the market is sufficiently liquid.\n\nVenue‑mechanism risk gates – Restrict exposure to venues with high funding volatility or poor execution reliability. Implement circuit breakers for events like liquidation cascades.\n\nRebalance friction control – Constrain portfolio turnover by incorporating transaction costs directly into the optimiser and imposing minimum holding periods.\n\n7 Executable Blueprint (YAML Template)\n\nA template for research runs can be encoded in YAML as follows:\n\nblueprint_version: \"1.0\"\nregistry_version: \"2026-02-18\"  # version/date of event definitions\nregistry_hash: \"<sha256-of-registry>\"\ndataset_id: \"crypto_data_snapshot_2026-02-17\"\ndataset_hash: \"<sha256-of-snapshot>\"\nparameters:\n  event_types: [FND_DISLOC, BASIS_DISLOC, LIQ_CASCADE, VOL_SHOCK, LIQ_SHOCK, STRUCT_BREAK, REGIME_SHIFT, MICRO_IMBAL]\n  horizons: [1h, 4h, 1d]\n  theta_bounds:\n    funding_threshold: [-50bp, 50bp]\n    obi_threshold: [0.2, 0.8]\n    delay: [0m, 5m]\n    position_scaler: [0.1, 1.0]\n  split_specs:\n    k_folds: 5\n    purging_window: \"H\"\n    embargo_fraction: 0.05\n  multiplicity_method: FDR  # options: FDR, reality_check, DSR\n  alpha: 0.05\nexecution_cost_model:\n  spread_cross: true\n  slippage_buckets: true\n  square_root_impact:", "tags": []}
{"fragment_id": "F_R6_346_368", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L346-L368", "text": "enabled: true\n    impact_coefficient: 0.5  # calibrated constant c\ncapacity_constraints:\n  participation_max: 0.05\n  max_notional_per_trade: 0.02  # as fraction of daily volume\n  max_aggregate_position: 0.25\nstress_tests:\n  spread_multipliers: [1, 2, 3]\n  depth_multipliers: [1, 0.5, 0.25]\n  vol_multipliers: [1, 1.5, 2]\n  venue_outage_probability: 0.05\nreproducibility:\n  code_commit: \"<git sha>\"\n  container_digest: \"<docker digest>\"\n  run_config_hash: \"<sha256-of-this-file>\"\n  rng_seeds: 123456\n\n\nThe run script reads this YAML, loads the corresponding dataset snapshot and event registry, performs PIT feature computation, executes the validation pipeline with specified splits and cost model, applies multiplicity corrections, and produces a report with strategy artefacts and portfolio allocation.\n\n8 Research Budget & Taxonomy Breadth Analysis\n8.1 Quantifying hypothesis space expansion", "tags": []}
{"fragment_id": "F_R6_369_376", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L369-L376", "text": "Expanding the number of event types and parameter sweeps increases the number of hypotheses $N$. Under the Benjamini–Hochberg FDR procedure, the expected number of false discoveries is $\\alpha \\cdot N / m$ where $m$ is the number of true positives. The deflated Sharpe ratio paper shows that the expected maximum Sharpe ratio increases logarithmically with the number of independent trials; thus the threshold for significance must grow with $\\sqrt{\\ln N}$. When more event types or parameters are tested, selection bias inflates observed performance; DSR and reality‑check corrections reduce this inflation.\n\nTo evaluate the effect of taxonomy breadth:\n\nEstimate independent trials $N$ – Multiply the number of event types by the number of parameter combinations per event (grid size). Dependencies (e.g., overlapping triggers) reduce effective $N$; adjust using correlation estimates.\n\nCompute expected false discoveries – Under FDR with level α, expected false positives ≈ α·$N$. Use this to determine how many candidate strategies can be investigated before the risk of false discovery becomes unacceptable.", "tags": []}
{"fragment_id": "F_R6_377_386", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L377-L386", "text": "Capacity‑adjusted opportunity set – For each candidate, compute capacity‑adjusted expected return. The number of feasible strategies is limited by capacity and correlation; adding more events may not increase opportunity if they share underlying liquidity.\n\n8.2 Max hypotheses per dataset and governance\n\nDefine a research budget: a maximum number of hypotheses $N_{\\max}$ per dataset snapshot. For example, limit $N_{\\max}$ to 200 trials per monthly snapshot. Pre‑registration requires researchers to list event types, parameter grids and evaluation criteria in a registry (manifest) before accessing labels. Each hypothesis consumes budget credits; once the budget is exhausted, new hypotheses must wait for the next dataset snapshot or require justification (e.g., new event type from market evolution). Track usage and remaining budget in a governance log.\n\nGovernance process:\n\nPre‑registration – Before running experiments, submit a manifest with event types and parameter ranges. The manifest is hashed and time‑stamped.", "tags": []}
{"fragment_id": "F_R6_387_402", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L387-L402", "text": "Approval – An internal committee or automated check verifies that proposed experiments fit within the budget and follow pre‑defined bounds.\n\nExecution – Run the research pipeline using the manifest. Record results and update budget usage.\n\nReview – At periodic intervals, review performance and budget consumption. Adjust budgets based on capacity and business priorities.\n\nThis process disciplines research, reduces data‑snooping risk, and ensures that the limited capacity of a small systematic operator is directed toward promising hypothesis spaces.\n\nFurther directions\n\nThe proposed framework is intentionally modular. Future enhancements may include:\n\nIntegrating DEX order book data to handle on‑chain liquidity and gas costs; this requires separate latency modelling and cost schedules.\n\nEmploying reinforcement learning for event‑triggered policies while respecting PIT constraints and multiplicity controls.", "tags": []}
{"fragment_id": "F_R6_403_407", "source_id": "R6", "locator": "Full‑Stack Research Framework for crypto.txt:L403-L407", "text": "Using Bayesian hierarchical models to share information across similar events (e.g., funding dislocation in BTC and ETH) while controlling false discoveries.\n\nExtending the event registry with macro‑economic or on‑chain analytics (e.g., whale movements, stablecoin flows) as additional context features.\n\nThis framework provides a rigorous, deterministic, capacity‑aware research process designed for a small systematic crypto operation. It emphasises reproducibility, microstructure realism, multiple‑testing control and robust portfolio construction.", "tags": []}
{"fragment_id": "F_R7_1_16", "source_id": "R7", "locator": "crpytochat.txt:L1-L16", "text": "You do **not** research “trading strategies.”\nYou research **repeatable market behaviors that can become measurable events**.\n\nYour confusion comes from researching at the wrong abstraction level.\n\nCorrect hierarchy:\n\n```\nMarket behavior  →  Event  →  Edge  →  Strategy\n(research here)              (NOT here)\n```\n\nYou are currently trying to research the last layer first.\n\n---", "tags": []}
{"fragment_id": "F_R7_17_30", "source_id": "R7", "locator": "crpytochat.txt:L17-L30", "text": "# 1. What You Are Actually Researching\n\nYour research objective:\n\n> Identify **situations where market behavior changes predictably**.\n\nNot entries.\nNot indicators.\nNot setups.\n\nYou are researching **conditional market physics**.\n\n---", "tags": []}
{"fragment_id": "F_R7_31_36", "source_id": "R7", "locator": "crpytochat.txt:L31-L36", "text": "# 2. The Five Research Domains (Only These Matter)\n\nEverything useful in trading discovery fits into these categories.\n\n---", "tags": []}
{"fragment_id": "F_R7_37_63", "source_id": "R7", "locator": "crpytochat.txt:L37-L63", "text": "## A. Volatility State Transitions (Highest Priority)\n\nMarkets alternate between:\n\n```\ncompression → expansion → exhaustion → reset\n```\n\nResearch questions:\n\n* When does volatility compress?\n* How long does compression persist?\n* What typically happens after compression?\n* Does expansion direction depend on prior trend?\n\nThings to study:\n\n* range contraction\n* ATR percentile regimes\n* realized volatility clustering\n* session volatility differences\n\nWhy this matters:\nMost edges originate from volatility regime change.\n\n---", "tags": []}
{"fragment_id": "F_R7_64_85", "source_id": "R7", "locator": "crpytochat.txt:L64-L85", "text": "## B. Session Microstructure\n\nMarkets behave differently across sessions.\n\nResearch:\n\n* Asian session range characteristics\n* London open displacement\n* NY continuation vs reversal\n* session overlap effects\n* liquidity arrival timing\n\nQuestions:\n\n```\nDoes behavior after NY open depend on Asian range size?\n```\n\nYou are looking for conditional effects.\n\n---", "tags": []}
{"fragment_id": "F_R7_86_105", "source_id": "R7", "locator": "crpytochat.txt:L86-L105", "text": "## C. Liquidity & Mean Reversion Dynamics\n\nMarkets frequently move to areas where orders exist.\n\nResearch:\n\n* prior high/low sweeps\n* equal highs/lows\n* overnight highs/lows\n* VWAP distance behavior\n* reversion probability after extension\n\nKey question:\n\n```\nAfter liquidity sweep, what distribution change occurs?\n```\n\n---", "tags": []}
{"fragment_id": "F_R7_106_124", "source_id": "R7", "locator": "crpytochat.txt:L106-L124", "text": "## D. Trend Exhaustion / Persistence\n\nTrend is not binary.\n\nResearch:\n\n* trend age\n* slope decay\n* pullback depth statistics\n* continuation probability vs trend maturity\n\nExample question:\n\n```\nAfter 5 consecutive directional candles, what happens next?\n```\n\n---", "tags": []}
{"fragment_id": "F_R7_125_140", "source_id": "R7", "locator": "crpytochat.txt:L125-L140", "text": "## E. Market Context Interaction (Advanced)\n\nEdges rarely exist alone.\n\nResearch interactions:\n\n```\nlow volatility + session change\ntrend + liquidity sweep\ncompression + macro session open\n```\n\nEdges often appear only when conditions combine.\n\n---", "tags": []}
{"fragment_id": "F_R7_141_155", "source_id": "R7", "locator": "crpytochat.txt:L141-L155", "text": "# 3. What NOT to Research (Huge Time Saver)\n\nAvoid:\n\n❌ indicators (RSI, MACD, etc.)\n❌ entry techniques\n❌ YouTube strategies\n❌ parameter optimization\n❌ ML price prediction papers\n❌ signal generation methods\n\nThose belong AFTER edge discovery.\n\n---", "tags": []}
{"fragment_id": "F_R7_156_180", "source_id": "R7", "locator": "crpytochat.txt:L156-L180", "text": "# 4. How Research Should Look (Correct Form)\n\nBad research:\n\n> “Breakouts work in NY.”\n\nGood research:\n\n```\nObservation:\nWhen Asian range percentile < 30,\nNY session produces larger-than-average expansion.\n\nVariables:\n- asian_range_percentile\n- NY_session_flag\n\nHypothesis:\nconditional_range_expansion\n```\n\nResearch output must be convertible into variables.\n\n---", "tags": []}
{"fragment_id": "F_R7_181_188", "source_id": "R7", "locator": "crpytochat.txt:L181-L188", "text": "# 5. Research Template (Use This Exactly)\n\nEvery research note should follow:\n\n```\nBEHAVIOR:\nWhat market phenomenon exists?", "tags": []}
{"fragment_id": "F_R7_189_191", "source_id": "R7", "locator": "crpytochat.txt:L189-L191", "text": "CONDITION:\nUnder what measurable state?", "tags": []}
{"fragment_id": "F_R7_192_197", "source_id": "R7", "locator": "crpytochat.txt:L192-L197", "text": "MEASUREMENT:\nWhat changes afterward?\n\nHORIZON:\nHow far into future?", "tags": []}
{"fragment_id": "F_R7_198_221", "source_id": "R7", "locator": "crpytochat.txt:L198-L221", "text": "EXPECTED EFFECT:\nExpansion / reversion / drift / volatility change\n```\n\nExample:\n\n```\nBehavior:\nVolatility expansion after compression\n\nCondition:\nATR_20 percentile < 0.25\n\nMeasurement:\nForward 32-bar range\n\nExpected:\nRange increase\n```\n\nIf you cannot fill all fields → not usable research.\n\n---", "tags": []}
{"fragment_id": "F_R7_222_225", "source_id": "R7", "locator": "crpytochat.txt:L222-L225", "text": "# 6. Sources Worth Studying (High Signal Only)\n\nFocus on structural knowledge, not strategies.", "tags": []}
{"fragment_id": "F_R7_226_231", "source_id": "R7", "locator": "crpytochat.txt:L226-L231", "text": "### Microstructure & Volatility\n\n* volatility clustering research\n* market regime switching\n* realized volatility studies", "tags": []}
{"fragment_id": "F_R7_232_237", "source_id": "R7", "locator": "crpytochat.txt:L232-L237", "text": "### Quantitative Trading Concepts\n\n* conditional return distributions\n* event studies in finance\n* regime-based models", "tags": []}
{"fragment_id": "F_R7_238_247", "source_id": "R7", "locator": "crpytochat.txt:L238-L247", "text": "### Practical Data Observations\n\n* session statistics\n* range behavior\n* trend persistence analysis\n\nYou are extracting *ideas for measurable conditions*, not copying methods.\n\n---", "tags": []}
{"fragment_id": "F_R7_248_269", "source_id": "R7", "locator": "crpytochat.txt:L248-L269", "text": "# 7. What Your First Research Week Should Produce\n\nGoal is NOT profit ideas.\n\nGoal output:\n\n```\n10–20 hypotheses like:\n```\n\n```\nH1: Low Asian volatility → NY expansion\nH2: Liquidity sweep → short-term reversal\nH3: Extreme VWAP distance → mean reversion\nH4: Compression duration predicts expansion size\nH5: Trend maturity reduces continuation probability\n```\n\nThat alone is enough to build the system.\n\n---", "tags": []}
{"fragment_id": "F_R7_270_287", "source_id": "R7", "locator": "crpytochat.txt:L270-L287", "text": "# 8. Why This Works\n\nYou are separating:\n\n```\nDiscovery (science)\nfrom\nExecution (engineering)\n```\n\nMost traders mix them and overfit.\n\nYou instead measure market laws first.\n\nStrategies become trivial later.\n\n---", "tags": []}
{"fragment_id": "F_R7_288_301", "source_id": "R7", "locator": "crpytochat.txt:L288-L301", "text": "# 9. Practical Starting List (Research Immediately)\n\nStart investigating these **five concrete behaviors**:\n\n1. Asian session range percentile vs NY range expansion\n2. ATR compression duration vs breakout magnitude\n3. Distance from VWAP vs reversion probability\n4. Prior high/low sweep vs next 16-bar return\n5. Consecutive directional candles vs reversal probability\n\nEach maps cleanly into event discovery.\n\n---", "tags": []}
{"fragment_id": "F_R8_1_2", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L1-L2", "text": "# Institutional Crypto Research Framework Audit and Expansion", "tags": []}
{"fragment_id": "F_R8_3_6", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L3-L6", "text": "## Framework Audit\n\n**Baseline artifact being audited:** the provided “Full-Stack Research Framework for Systematic Crypto Portfolios” focuses on **systematic trading research** driven by point‑in‑time market state, explicit transaction-cost/capacity modeling, and deterministic replay (dataset/spec hashes, blueprinting). fileciteturn0file0L5-L17 fileciteturn0file0L255-L337", "tags": []}
{"fragment_id": "F_R8_7_17", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L7-L17", "text": "### Coverage\n\n**What it covers well (strengths to preserve and reuse in the institutional version)**\n\n- **Point-in-time rigor + reproducibility as first-class constraints.** The framework explicitly enforces no lookahead, in-sample/out-of-sample separation, and deterministic artifact versioning (dataset hashes, blueprint specs). fileciteturn0file0L13-L17 fileciteturn0file0L255-L337  \n  This directly addresses well-known failure modes in quantitative research where reuse of the same dataset across many trials inflates false discoveries and overfitting risk. citeturn6search1turn6search8turn6search3\n\n- **Clear “unit of research” definitions.** The event instance → candidate edge → strategy spec → portfolio spec progression is explicit and operational, enabling auditability of what was tested and what was deployed. fileciteturn0file0L19-L23\n\n- **Event-driven discovery with formal triggers.** A versioned event registry (funding dislocations, liquidation cascades, liquidity shocks, structural breaks, regime shifts) is a strong mechanism to avoid ad-hoc “signal soup.” fileciteturn0file0L25-L47", "tags": []}
{"fragment_id": "F_R8_18_22", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L18-L22", "text": "- **Execution realism via implementation shortfall and capacity modeling.** The explicit “arrival price / implementation shortfall” framing and a parameterized impact model make PnL claims falsifiable under explicit trading frictions. fileciteturn0file0L174-L207  \n  Implementation shortfall is a standard TCA objective and helps prevent paper alpha that disappears under realistic execution. citeturn10search8turn10search0\n\n- **Multiplicity controls are explicitly acknowledged.** The framework requires multiple-testing adjustments (FDR/reality check) and selection-bias-aware statistics (deflated Sharpe) when many variants are tried. fileciteturn0file0L217-L223 citeturn6search3turn6search8turn6search1", "tags": []}
{"fragment_id": "F_R8_23_36", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L23-L36", "text": "### Gaps\n\n**What’s missing relative to an institutional “fundamentals + risk” crypto research framework**\n\nThe baseline is optimized for **tradable edges and portfolio construction**. It largely omits the “why will this asset be valuable and survivable?” layers:\n\n- **Thesis/macro/cycle positioning** is mostly absent (beyond market regime handling for returns), so it can’t answer: *what macro regime is this asset structurally long/short?*\n\n- **Problem/PMF** is not evaluated (who is the user, why now, what switching costs). This is central for long-only or venture-style decisions.\n\n- **Value accrual** is not formalized: where does durable value concentrate (token, equity, sequencer fees, MEV, off-chain capture).\n\n- **Tokenomics, incentives, governance, security, decentralization** are not treated as scored diligence categories. These are particularly important because DeFi systems have both **technical security** and **economic security** (incentive/game design risk). citeturn0search0turn0search4", "tags": []}
{"fragment_id": "F_R8_37_40", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L37-L40", "text": "- **DEX-specific adverse selection / MEV** is only implicitly present (execution realism), but not elevated to a first-order threat model. Transaction reordering/frontrunning and MEV are empirically documented as core structural risks in on-chain markets (and can propagate to consensus-layer incentives). citeturn0search1turn8search3\n\n- **Regulatory/compliance risk** is absent. For institutions, eligibility and distribution are constrained by AML/sanctions, securities/commodities classification, and jurisdictional regimes (e.g., EU MiCA, FATF Travel Rule expectations). citeturn0search3turn0search7turn1search3turn1search11", "tags": []}
{"fragment_id": "F_R8_41_48", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L41-L48", "text": "### Vague or underspecified terms\n\nWhere the baseline is operational but still ambiguous for institutional use:\n\n- **“Event thresholds” and “regime definitions”** (z-scores, percentiles, breakpoint tests) are parameterized but not anchored to an explicit *research budget* (max hypotheses per snapshot) and not tied to decision objectives (long-horizon vs tactical). fileciteturn0file0L219-L223 fileciteturn0file0L460-L460\n\n- **“DEX optional”** understates that DEX execution is not a minor routing variant: it introduces MEV, gas auctions, private orderflow, sandwich risk, and on-chain liquidity fragmentation. citeturn0search1turn8search3", "tags": []}
{"fragment_id": "F_R8_49_58", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L49-L58", "text": "### Hidden assumptions\n\n**Assumptions embedded in the baseline (explicit + implicit)**\n\nTrading/system assumptions (baseline-specific):\n- **Liquid, continuous markets exist** for the universe (spot/perps) on venues you can access, with stable APIs and sufficient depth. fileciteturn0file0L7-L12\n- **Point-in-time data is obtainable** (fees, OI, funding, liquidation prints, order books) and can be normalized across venues without silent schema drift. fileciteturn0file0L50-L56 fileciteturn0file0L255-L337\n- **Execution model fidelity is adequate**: impact model form (often √Q) and slippage buckets approximate real fills at the intended scale. fileciteturn0file0L189-L207 citeturn10search17turn10search1\n- **Research process controls are sufficient** to manage multiple testing and overlapping labels (purging/embargo). fileciteturn0file0L215-L223 citeturn6search1turn6search8", "tags": []}
{"fragment_id": "F_R8_59_64", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L59-L64", "text": "Institutional/fundamentals assumptions (commonly made unless explicitly denied; must be forced into the open):\n- “**Token price tracks usage**” (or some monotone relation exists), despite possible value leakage to off-chain actors, MEV, centralized sequencers, or equity.  \n- “**Team can ship**” (delivery risk is ignored), including ability to handle incidents and governance complexity.  \n- “**Regulatory neutrality**” (asset remains tradeable, listable, and custodiable across target jurisdictions), despite evolving frameworks and AML/sanctions requirements. citeturn0search3turn1search3turn7search10\n- “**On-chain activity is real demand**,” not subsidized or Sybil-driven—an assumption frequently violated in practice. citeturn9search1turn9search5", "tags": []}
{"fragment_id": "F_R8_65_75", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L65-L75", "text": "### Failure modes\n\n**Where the baseline would greenlight bad projects**\n- **“Tradability masking insolvency.”** The framework can approve a tradeable asset/venue regime with statistically robust microstructure edges while ignoring existential protocol risks (admin-key upgradeability, governance capture, audit history, legal exposure).\n- **“Volume/liq mirage.”** If venue-reported volume is inflated (wash trading), capacity and cost assumptions can be catastrophically wrong. citeturn9search0turn9search4\n- **“DEX execution blind spot.”** Treating DEX as “optional routing” can greenlight strategies that are structurally MEV-dominated (sandwichable flow, toxic orderflow). citeturn0search1turn8search3\n\n**Where it would reject good projects**\n- **Early-stage/wedge-phase networks** with weak current liquidity but strong PMF/architecture/security could be rejected because they are not yet “tradable” or lack perps/OI/funding regimes.\n- **Non-financial utility networks** (where value accrues via adoption or off-chain services) may be rejected because microstructure edges are not the right objective function.", "tags": []}
{"fragment_id": "F_R8_76_77", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L76-L77", "text": "## Measurable Criteria, Thresholds, and Falsifiable Hypotheses", "tags": []}
{"fragment_id": "F_R8_78_101", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L78-L101", "text": "### How to read this section\n\n- **Criteria:** 3–8 measurable items per section (quantitative when possible; otherwise operational proxies).  \n- **Thresholds:** “Good / Neutral / Bad” are **default institutional heuristics**; they must be calibrated by sector (L1/L2, DeFi, infra, privacy) and strategy type (long-only vs market-neutral).  \n- **Hypotheses:** Each includes **confirm**, **disconfirm**, and **time window**. Use placeholders where project inputs are missing.\n\n---\n\n**Thesis & macro/cycle positioning**\n\n- **Measurable criteria**\n  - Risk sensitivity: rolling β to crypto “market” proxy (e.g., BTC index), and correlation structure under stress.\n  - Liquidity regime sensitivity: bid–ask/spread and depth behavior when volatility rises (stress elasticity).\n  - Reflexivity exposure: % of demand driven by leverage (perp OI / spot volume proxies); liquidation sensitivity.\n  - Narrative cyclicality: share of attention driven by social/media vs usage (proxy: engagement-to-usage ratio).\n- **Threshold heuristics**\n  - Good: thesis specifies *when it should underperform* (explicit “anti-thesis” regime) and provides hedge plan.\n  - Neutral: thesis is regime-aware but lacks quantified exposures.\n  - Bad: thesis is always-true (no falsifiable macro claims).\n- **Falsifiable hypotheses**\n  1) *“Asset outperforms in regime R because driver D is non-cyclical.”*  \n     - Confirm: excess returns and usage/fee driver persistence during ≥2 risk-off episodes.  \n     - Disconfirm: driver collapses with market regime; returns indistinguishable from β exposure.  \n     - Time window: 6–18 months (or ≥2 regime shifts).", "tags": []}
{"fragment_id": "F_R8_102_121", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L102-L121", "text": "---\n\n**Problem/PMF + user segment**\n\n- **Measurable criteria**\n  - Defined user segment and job-to-be-done; measurable pain (time/cost reduction vs alternatives).\n  - Retention proxy: cohort stickiness (repeat users / new users; repeat tx per address).\n  - Willingness-to-pay proxy: fee generation per active user; fee/tx stability after incentives removed.\n  - Switching costs: composability lock-in, integrations, developer tooling, capital inertia.\n- **Threshold heuristics**\n  - Good: PMF evidence persists **without** subsidies; user segment is narrow + provable.\n  - Neutral: usage exists but appears incentive-sensitive; unclear segmentation.\n  - Bad: “everyone” is the user; usage explained primarily by rewards.\n- **Falsifiable hypotheses**\n  1) *“Protocol has PMF in segment S.”*  \n     - Confirm: retention + fee/user stable or rising across 2–3 cohorts with reduced incentives.  \n     - Disconfirm: activity collapses when rewards end; high churn and low repeat usage.  \n     - Time window: 3–9 months of cohort tracking.", "tags": []}
{"fragment_id": "F_R8_122_140", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L122-L140", "text": "---\n\n**Value accrual (where value concentrates)**\n\n- **Measurable criteria**\n  - Value capture map: fees/rents go to token holders vs LPs/users vs sequencer/validators vs treasury/equity.\n  - Capture durability: is capture enforced at protocol level (hard-coded) or governance-toggle (optional).\n  - Leakages: MEV extracted externally; off-chain intermediaries capture (front-ends/relays/validators).\n  - Unit economics: “take rate” = protocol-controlled revenue / gross economic activity.\n- **Threshold heuristics**\n  - Good: value capture is explicit, enforceable, and aligned with bearing-risk stakeholders.\n  - Neutral: capture exists but discretionary (fee switch, governance toggles).\n  - Bad: token has weak/indirect capture; economics leak mainly to external actors.\n- **Falsifiable hypotheses**\n  1) *“Token capture will increase as adoption grows.”*  \n     - Confirm: protocol-controlled revenue share rises with volume; governance execution timelines credible.  \n     - Disconfirm: competitive pressure forces fees down or shifts value to LPs/validators/MEV.  \n     - Time window: 6–18 months.", "tags": []}
{"fragment_id": "F_R8_141_164", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L141-L164", "text": "(For on-chain markets, explicitly model MEV as a value leakage and security externality. citeturn0search1turn8search3)\n\n---\n\n**Tokenomics (supply, emissions, sinks, dilution, distribution)**\n\n- **Measurable criteria**\n  - Supply schedule: circulating supply now; emissions curve; unlock calendar; discretionary mint rights.\n  - Dilution risk: expected inflation rate (annualized) and variance; conditions that accelerate emissions.\n  - Sinks: burns, lockups, staking, buybacks; are they endogenous to usage or discretionary.\n  - Distribution concentration: top holders’ share; treasury control; insider/VC unlock dominance.\n  - Airdrop integrity: Sybil resistance and distribution fairness (cluster analysis, behavioral features).\n- **Threshold heuristics**\n  - Good: emissions are bounded/predictable; sinks are usage-linked; distribution is not governance-capturable by a small group.\n  - Neutral: emissions moderate but near-term unlocks create overhang; sink mechanisms uncertain.\n  - Bad: discretionary minting or opaque lock/unlock mechanics; extreme concentration; Sybil-prone airdrops.\n- **Falsifiable hypotheses**\n  1) *“Net supply growth will be ≤ X% annually under base assumptions.”*  \n     - Confirm: observed supply + emission contracts match schedule; no emergency mints.  \n     - Disconfirm: governance/ops repeatedly change emissions upward.  \n     - Time window: 6–12 months (track releases vs schedule).\n  2) *“Airdrop distribution is Sybil-resistant.”*  \n     - Confirm: low post-drop clustering similarity; limited multi-wallet patterns.  \n     - Disconfirm: large share of allocation traceable to Sybil clusters.", "tags": []}
{"fragment_id": "F_R8_165_187", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L165-L187", "text": "- Time window: 1–3 months post-distribution. citeturn9search1turn9search5\n\n(Token models that link valuation to adoption dynamics are explicitly studied in academic tokenomics literature; use them to stress-test whether claimed adoption curves are consistent with sustainable token value. citeturn2search4turn5search1)\n\n---\n\n**Incentives & game theory (actors, payoff alignment, attack surfaces)**\n\n- **Measurable criteria**\n  - Actor map: users, LPs, borrowers/lenders, validators/sequencers, governance delegates, MEV actors.\n  - Payoff alignment: who profits when users lose? Identify “toxic positive externalities” (e.g., liquidation bots).\n  - Manipulation surfaces: oracle dependence, reentrancy/exploit incentives, governance bribery, MEV.\n  - Adversary profitability: expected profit of key attacks vs cost (capital, fees, bribery).\n- **Threshold heuristics**\n  - Good: key attacks are unprofitable or reliably mitigated; adversary costs scale faster than gains.\n  - Neutral: mitigations exist but rely on monitoring/ops.\n  - Bad: profitable attack classes exist (oracle manipulation, MEV sandwich, governance capture) with weak deterrence.\n- **Falsifiable hypotheses**\n  1) *“MEV/extraction is bounded and does not impair user outcomes.”*  \n     - Confirm: stable effective spreads/price impact after accounting for MEV; mitigations deployed.  \n     - Disconfirm: persistent sandwich/frontrun patterns; user execution degrades as activity rises.  \n     - Time window: 3–6 months. citeturn0search1turn8search3", "tags": []}
{"fragment_id": "F_R8_188_206", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L188-L206", "text": "---\n\n**Governance & upgrade risk**\n\n- **Measurable criteria**\n  - Upgradeability model: immutable vs proxy; timelocks; emergency powers; pause/blacklist features.\n  - Governance participation: voter turnout, quorum, delegate concentration, proposal throughput.\n  - Capture risk: concentration metrics on voting power; bribery markets; voter apathy.\n  - Change surface area: how many parameters can be modified; blast radius of a single proposal.\n- **Threshold heuristics**\n  - Good: upgrades are timelocked + transparent; emergency powers are narrow, audited, and monitored.\n  - Neutral: governance works but concentration high; timelocks shorter than institutional comfort.\n  - Bad: upgrade keys can change core logic without delay; governance effectively centralized.\n- **Falsifiable hypotheses**\n  1) *“Governance cannot be captured by ≤ N entities.”*  \n     - Confirm: voting power dispersion; no consistent cartel outcomes; proposal outcomes diverse.  \n     - Disconfirm: repeated wins by a small coalition; quorum depends on insiders.  \n     - Time window: 6–12 months.", "tags": []}
{"fragment_id": "F_R8_207_227", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L207-L227", "text": "(For upgradeable systems, timelocks are a standard mitigation for admin misuse risk; treat un-timelocked upgrade authority as a structural red flag. citeturn10search18)\n\n---\n\n**Technical architecture (trust assumptions, dependencies, liveness/finality)**\n\n- **Measurable criteria**\n  - Trust assumptions: honest majority? data availability? sequencer trust? multisig dependencies?\n  - Dependency map: oracles, bridges, L2/L1 settlement, off-chain relayers, RPC providers.\n  - Failure domains: what breaks if dependency fails (halt vs incorrect state transition vs fund loss).\n  - Liveness/finality: time to finality, reorg risk, withdrawal finality (for L2), downtime history.\n- **Threshold heuristics**\n  - Good: trust assumptions are explicit; dependency failures degrade gracefully; recovery plan exists.\n  - Neutral: dependencies exist but are standard; recovery is plausible.\n  - Bad: opaque dependencies; single points of failure can cause catastrophic loss or censorship.\n- **Falsifiable hypotheses**\n  1) *“System maintains liveness under stress scenario S.”*  \n     - Confirm: historical stress tests / incidents show bounded downtime; architecture supports failover.  \n     - Disconfirm: repeated halts/censorship; no credible failover.  \n     - Time window: 6–18 months (or incident-based).", "tags": []}
{"fragment_id": "F_R8_228_246", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L228-L246", "text": "---\n\n**Security posture (audits, bug bounties, incident history)**\n\n- **Measurable criteria**\n  - Audit coverage: count + recency + scope; whether audits include core + periphery + dependencies.\n  - Verification/testing depth: fuzzing/invariant tests, formal methods where warranted.\n  - Bug bounty strength: program exists, payout realism, response SLAs.\n  - Incident history: past exploits, severity, time-to-patch, user restitution.\n- **Threshold heuristics**\n  - Good: multiple independent audits; serious bounty; quantified testing; strong incident response.\n  - Neutral: some audits; bounty exists but limited; partial coverage.\n  - Bad: unaudited core or repeat critical incidents; opaque postmortems.\n- **Falsifiable hypotheses**\n  1) *“No critical exploit class remains in critical path given current code.”*  \n     - Confirm: audit + fuzz + invariant test coverage; critical findings remediated and verified.  \n     - Disconfirm: new critical findings recur; core invariants fail in testing.  \n     - Time window: continuous; formal re-check at each major release.", "tags": []}
{"fragment_id": "F_R8_247_268", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L247-L268", "text": "(Smart contract vulnerability taxonomies and tool landscapes are extensively surveyed; use them to ensure your audit/testing stack covers dominant bug classes. citeturn1search17turn1search1)  \n(Formal verification is feasible for high-criticality contracts and has been applied in major ecosystems. citeturn1search6)  \n(Bug bounty economics and reward rules vary; treat “bounty exists” as insufficient—evaluate realism. citeturn10search3)\n\n---\n\n**Decentralization & censorship resistance (where relevant)**\n\n- **Measurable criteria**\n  - Consensus concentration: validator/miner concentration; stake distribution; operator diversity.\n  - Client diversity: software diversity reduces correlated failure risk.\n  - Infrastructure concentration: hosting/provider concentration; geographic spread.\n  - Governance decentralization: proposer/delegate concentration; upgrade authority dispersion.\n  - Censorship indicators: transaction inclusion anomalies; OFAC-style compliance concentration signals.\n- **Threshold heuristics**\n  - Good: decentralization measured across subsystems; no single chokepoint dominates.\n  - Neutral: moderate concentration typical of PoS; mitigations exist (delegation diversity, client diversity).\n  - Bad: few entities can censor/finalize/upgrade; dependencies are centralized.\n- **Falsifiable hypotheses**\n  1) *“No small set of entities can halt/censor the system.”*  \n     - Confirm: decentralization metrics improve or remain stable as system scales.  \n     - Disconfirm: concentration increases with TVL/usage; coercion or outages show dependence on a few actors.", "tags": []}
{"fragment_id": "F_R8_269_291", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L269-L291", "text": "- Time window: 6–24 months.\n\n(Decentralization measurement is an active research topic; treat it as multi-dimensional, not a single “node count.” citeturn2search2turn2search10)\n\n---\n\n**Ecosystem & distribution (partners, integrations, community health)**\n\n- **Measurable criteria**\n  - Integration breadth: number/quality of credible integrations; share of usage from top N integrators.\n  - Developer momentum: repo activity, unique contributors, issue/PR velocity (normalize for spam).\n  - Community resilience: governance participation breadth, forum activity quality, concentration of discourse.\n  - Distribution channels: wallets, exchanges, on-chain routing (aggregators), enterprise partners.\n- **Threshold heuristics**\n  - Good: diversified integrations; developer activity is sustained and non-incentivized.\n  - Neutral: a few large integrators dominate.\n  - Bad: ecosystem is fragile (single distribution partner); “community” is largely paid.\n- **Falsifiable hypotheses**\n  1) *“Ecosystem growth compounds (more integrations → more usage).”*  \n     - Confirm: integrations and non-incentivized usage grow together; churn of integrators is low.  \n     - Disconfirm: integrations are shallow/marketing-only; usage doesn’t follow.  \n     - Time window: 6–12 months.", "tags": []}
{"fragment_id": "F_R8_292_314", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L292-L314", "text": "---\n\n**Competitive landscape & moats**\n\n- **Measurable criteria**\n  - Differentiation: measurable performance/security/UX advantage vs closest substitutes.\n  - Switching costs: liquidity depth, composability position, developer tooling lock-in.\n  - Sustainability: fee compression risk; multi-chain commoditization.\n  - Substitutability: can a fork + incentives replicate the product?\n- **Threshold heuristics**\n  - Good: moat is structural (network effects, deep liquidity, defensible tech).\n  - Neutral: differentiation exists but can be copied.\n  - Bad: commodity product + incentives-only moat.\n- **Falsifiable hypotheses**\n  1) *“Competitors cannot replicate advantage A without cost C.”*  \n     - Confirm: competitor attempts fail or require uneconomic subsidies.  \n     - Disconfirm: feature parity reached quickly and users migrate.  \n     - Time window: 6–18 months.\n\n---\n\n**On-chain / usage metrics (leading indicators vs lagging)**", "tags": []}
{"fragment_id": "F_R8_315_329", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L315-L329", "text": "- **Measurable criteria**\n  - Leading indicators: net new quality users (Sybil-adjusted), retained cohorts, recurring fees.\n  - Lagging indicators: TVL, raw tx count, gross volume (often incentive-manipulable).\n  - Quality adjustments: filter wash/incentive activity; cluster Sybil; remove internal routing loops.\n  - Price/usage decoupling: detect whether usage metrics are merely price-driven.\n- **Threshold heuristics**\n  - Good: leading indicators improve and remain after subsidy changes; metrics are Sybil-adjusted.\n  - Neutral: mixed; leading indicators uncertain.\n  - Bad: only lagging metrics look strong; metrics collapse when incentives change.\n- **Falsifiable hypotheses**\n  1) *“Usage growth is organic, not subsidy-driven.”*  \n     - Confirm: activity persists after incentives reduce; cohort retention remains.  \n     - Disconfirm: sharp drop coinciding with incentive changes.  \n     - Time window: 3–9 months.", "tags": []}
{"fragment_id": "F_R8_330_350", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L330-L350", "text": "(Sybil detection for airdrops is actively researched; use it as part of “quality user” measurement. citeturn9search5turn9search9)\n\n---\n\n**Liquidity & market structure (venues, float, unlocks, reflexivity)**\n\n- **Measurable criteria**\n  - Real liquidity: consolidated order book depth, effective spread, slippage for size Q across venues.\n  - Volume integrity: detect wash trading / fake volume; compare volume vs web traffic vs on-chain flows.\n  - Float dynamics: circulating float, borrow availability, unlock calendar, market maker concentration.\n  - Derivatives reflexivity: perp OI, funding regime volatility, liquidation clusters.\n- **Threshold heuristics**\n  - Good: liquidity supports target position size with modeled costs; volume integrity checks pass.\n  - Neutral: liquidity adequate but unlocks create near-term overhang.\n  - Bad: liquidity is thin or fake; unlocks are large relative to real liquidity; venue concentration extreme.\n- **Falsifiable hypotheses**\n  1) *“Reported volume reflects real liquidity.”*  \n     - Confirm: independent checks (traffic, flows, cross-venue consistency) align.  \n     - Disconfirm: wash-trading signatures; volume decoupled from traffic/funds held.  \n     - Time window: 1–3 months.", "tags": []}
{"fragment_id": "F_R8_351_371", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L351-L371", "text": "(Wash trading is empirically documented in centralized exchange settings; treat “reported volume” as untrusted until validated. citeturn9search0turn9search4)\n\n---\n\n**Regulatory & legal risk (jurisdictional exposure, classification risks)**\n\n- **Measurable criteria**\n  - Jurisdiction map: where team, foundation, key service providers, and major users are located.\n  - Token classification exposure: plausible security/derivatives/payment interpretations by jurisdiction.\n  - AML/sanctions exposure: VASP touchpoints, mixers, privacy features; sanctions screening controls.\n  - Market access: exchange listing/custody constraints; stablecoin dependencies.\n- **Threshold heuristics**\n  - Good: clear compliance posture; limited reliance on high-risk flows; credible legal opinions *with scope*.\n  - Neutral: moderate uncertainty; manageable exposure if limited distribution.\n  - Bad: high probability of enforcement or delisting in target jurisdictions; sanctions/AML red flags.\n- **Falsifiable hypotheses**\n  1) *“Asset remains accessible to target investor base in jurisdictions J.”*  \n     - Confirm: custody/listing/reg pathway exists; no major restrictions triggered.  \n     - Disconfirm: delistings, enforcement actions, or custody exclusion.  \n     - Time window: 6–24 months (monitor continuously).", "tags": []}
{"fragment_id": "F_R8_372_394", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L372-L394", "text": "(EU MiCA timelines and implementation details matter for European distribution. citeturn0search3turn0search7)  \n(FATF guidance frames global AML expectations for virtual assets and Travel Rule implementation. citeturn1search3turn1search11)  \n(US sanctions compliance expectations for virtual currency industry are explicitly published; treat as operational requirements. citeturn7search10)\n\n---\n\n**Execution risk (team, runway, ops)**\n\n- **Measurable criteria**\n  - Team capacity: release cadence vs roadmap; historical delivery variance.\n  - Runway: treasury assets, burn rate proxy, funding concentration, stablecoin exposure.\n  - Operational maturity: incident response playbooks, key management, access controls, monitoring.\n  - Vendor risk: reliance on a single infra provider (RPC, sequencer, custody).\n- **Threshold heuristics**\n  - Good: consistent shipping; strong ops; runway supports ≥ 18–24 months of base-case plan.\n  - Neutral: execution history mixed; ops improving.\n  - Bad: repeated missed milestones; weak key management; short runway with opaque financing.\n- **Falsifiable hypotheses**\n  1) *“Team can deliver milestone M by date T.”*  \n     - Confirm: interim milestones hit; testnet/prod deltas converge to plan.  \n     - Disconfirm: repeated slips without credible technical blockers; key staff churn.  \n     - Time window: until T (plus post-launch stability period).", "tags": []}
{"fragment_id": "F_R8_395_416", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L395-L416", "text": "---\n\n**Valuation approach (relative, cashflow-like, utility, comparable networks)**\n\n- **Measurable criteria**\n  - Choose valuation lens consistent with value accrual:  \n    - fee/revenue-like (if token captures fees),  \n    - utility (if token is required for usage),  \n    - monetary premium/store-of-value (if applicable),  \n    - comparable networks (peer multiples).\n  - Sensitivity to assumptions: adoption, fee rates, take rate, emission schedule, discount rate proxy.\n  - Cross-checks: on-chain “value” proxies (e.g., value-to-activity metrics) but adjusted for manipulation.\n- **Threshold heuristics**\n  - Good: valuation triangulates multiple methods and stress-tests assumptions.\n  - Neutral: one method used with sensitivity ranges.\n  - Bad: valuation is price-anchored (“it used to be higher”).\n- **Falsifiable hypotheses**\n  1) *“Under base assumptions, implied valuation multiple is defensible vs peers.”*  \n     - Confirm: peer-adjusted multiples with plausible growth/risks; sensitivity shows robust value.  \n     - Disconfirm: valuation relies on extreme growth or ignores dilution/unlocks.  \n     - Time window: quarterly re-evaluation.", "tags": []}
{"fragment_id": "F_R8_417_436", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L417-L436", "text": "(Research on cryptoasset valuation frameworks and factor-like interpretations is developed by industry research houses; use as triangulation, not as sole proof. citeturn2search11)\n\n---\n\n**Catalysts & timeline**\n\n- **Measurable criteria**\n  - Catalyst list with dates/windows: upgrades, fee switches, unlock cliffs, regulatory decisions, listings.\n  - Catalyst directionality: what must happen for the catalyst to be positive vs negative.\n  - Pre/post metrics: define what changes you expect in usage, fees, security, decentralization.\n- **Threshold heuristics**\n  - Good: catalysts are specific and testable; expectations are quantified with “what would disappoint.”\n  - Neutral: catalysts exist but impact unclear.\n  - Bad: “catalyst” is vague narrative momentum.\n- **Falsifiable hypotheses**\n  1) *“Catalyst C increases metric K by ≥ X% without increasing risk R.”*  \n     - Confirm: measured uplift and stable risk indicators.  \n     - Disconfirm: no uplift or risk spikes (incidents, churn, governance controversy).  \n     - Time window: 1–3 months post-catalyst.", "tags": []}
{"fragment_id": "F_R8_437_438", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L437-L438", "text": "---", "tags": []}
{"fragment_id": "F_R8_439_440", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L439-L440", "text": "## Bias Checks and Red Flags", "tags": []}
{"fragment_id": "F_R8_441_461", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L441-L461", "text": "### Bias check module\n\n**Narrative bias checks (story-first failure modes)**  \nUse these as **required “pre-mortem gates”** before scoring:\n\n- Story-first reasoning: thesis has conclusions before evidence; evidence is selected post-hoc.\n- Charismatic founder bias: confidence derived from personality/network rather than shipped artifacts.\n- Meme momentum bias: attention conflated with adoption; price appreciation treated as validation.\n- Survivorship bias: only studying winners; ignoring base rates of failure in similar designs.\n- Techno-solutionism: assuming “better tech” guarantees adoption despite distribution and incentives.\n- Single-cause fallacy: one driver explains everything (e.g., “fees = value”) despite leakage channels.\n\n**Data bias checks (measurement failure modes)**\n\n- Wash trading / fake volume: venue volume not equal to executable liquidity. citeturn9search0turn9search4\n- Sybil activity: user counts inflated via multi-wallet farming (especially around airdrops/incentives). citeturn9search5turn9search1\n- Incentive-driven usage: TVL/tx spikes caused by rewards, not PMF (cliffs after incentives end).\n- API/vendor bias: dashboards choose definitions that flatter the project (metric definition drift).\n- Cherry-picked windows: starting after the bottom or excluding incident periods.\n- On-chain attribution errors: mislabeling exchanges/bridges/internal routing as “user demand.”", "tags": []}
{"fragment_id": "F_R8_462_463", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L462-L463", "text": "image_group{\"layout\":\"carousel\",\"aspect_ratio\":\"16:9\",\"query\":[\"MEV sandwich attack diagram ethereum\",\"token unlock schedule cliff vesting chart crypto\",\"DAO governance timelock upgrade process diagram\"],\"num_per_query\":1}", "tags": []}
{"fragment_id": "F_R8_464_474", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L464-L474", "text": "### Stoplight severity system and required mitigations\n\n- **Green:** issue is explainable, measured, and bounded.  \n  **Mitigation:** record definition + monitoring metric; proceed.\n\n- **Amber:** plausible risk that could flip the thesis; uncertainty is material.  \n  **Mitigation:** require at least one independent verification source + a sensitivity test + explicit disconfirming evidence triggers before proceeding.\n\n- **Red:** structural risk that can cause permanent loss, delisting, or thesis invalidation.  \n  **Mitigation:** *no-go* unless risk is eliminated (not merely “managed”) or position is re-scoped to a strictly bounded tactical trade.", "tags": []}
{"fragment_id": "F_R8_475_490", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L475-L490", "text": "### Red-flag catalog (grouped; ≥ 25)\n\n| Theme | Red flag | Severity | Required mitigation |\n|---|---|---|---|\n| Narrative | “Replaces X” with no user/segment specificity | Amber | Force segmentation + measure retention/fees per cohort |\n| Narrative | “Partnerships” are announcements without integrations shipped | Amber | Verify production integrations + usage attributable to them |\n| Narrative | Roadmap is only slides; no shipped milestones | Amber | Map to repo/releases + independent evidence |\n| Narrative | “Community” is mostly incentivized shilling | Amber | Separate organic vs paid; require retention after incentives |\n| Narrative | Price-anchored valuation (“down 90% so cheap”) | Amber | Rebuild valuation from accrual + dilution + peers |\n| Data integrity | CEX volume huge but shallow order books | Red | Liquidity sampling + slippage tests; downweight/ignore volume |\n| Data integrity | Wash trading indicators; volume decoupled from traffic/funds | Red | Cross-validate with independent sources; exclude venues citeturn9search0turn9search4 |\n| Data integrity | TVL spikes coincide with rewards; cliffs after rewards end | Amber | Incentive-adjusted metrics + cohort retention |\n| Data integrity | User counts dominated by Sybil clusters | Red | Sybil detection + adjusted KPIs citeturn9search5turn9search9 |\n| Data integrity | Metric definitions change mid-analysis | Amber | Freeze definitions; re-run history with consistent schema |\n| Tokenomics | Opaque unlock schedule; “unknown future emissions” | Red | Full unlock calendar + contract-level verification |\n| Tokenomics | Discretionary mint/blacklist powers without constraints | Red | Verify controls; require timelock/governance constraints citeturn10search18 |", "tags": []}
{"fragment_id": "F_R8_491_503", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L491-L503", "text": "| Tokenomics | Extreme concentration in top holders with governance control | Red | Concentration analysis + capture simulations |\n| Tokenomics | “Fee switch someday” is the main value story | Amber | Require governance path + precedent + timeline |\n| Incentives | Profitable oracle manipulation path exists | Red | Dependency map + oracle stress tests; require mitigations |\n| Incentives | Persistent MEV extraction harming users | Amber/Red | Measure MEV impact; require countermeasures citeturn0search1turn8search3 |\n| Incentives | Liquidity is mercenary (LPs churn quickly) | Amber | Measure LP retention; stress fee reductions |\n| Governance | Upgradable contracts without timelock | Red | Timelock or immutability requirement citeturn10search18 |\n| Governance | Governance turnout near zero; insiders decide | Amber | Delegate analysis + quorum reforms evidence |\n| Governance | Emergency powers are broad and opaque | Red | Narrow scope + documented policy + monitoring |\n| Security | No independent audits for core contracts | Red | Require audits + remediation proof |\n| Security | Repeated critical incidents without credible postmortems | Red | Verify root cause fixed; assess ops maturity |\n| Security | Weak bounty / no response process | Amber | Require credible bounty + SLA + triage process citeturn10search3 |\n| Architecture | Single dependency can freeze funds (bridge/oracle/sequencer) | Red | Dependency map + failover design |\n| Decentralization | Few entities can censor/finalize/upgrade | Red | Quantify; require dispersion improvements citeturn2search2turn2search10 |", "tags": []}
{"fragment_id": "F_R8_504_510", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L504-L510", "text": "| Competitive | Product is a forkable commodity + incentives-only moat | Amber | Identify structural moat or price as tactical-only |\n| Regulatory | High-risk AML/sanctions exposure with no controls | Red | Screening + policy + legal constraints citeturn7search10turn1search11 |\n| Regulatory | Key jurisdictions likely classify token adversely | Amber/Red | Obtain scoped legal analysis + contingency planning |\n| Market structure | Massive near-term unlock vs real liquidity | Red | Unlock stress test; position sizing constraints |\n| Execution | Team runway short and financing opaque | Amber | Treasury/runway modeling + disclosure requirements |\n| Execution | Key-person risk extreme; governance/ops depend on one actor | Amber | Org redundancy + documented processes |", "tags": []}
{"fragment_id": "F_R8_511_512", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L511-L512", "text": "## Expanded Institutional Framework and Scoring Rubric", "tags": []}
{"fragment_id": "F_R8_513_522", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L513-L522", "text": "### Scoring rubric (0–5) anchored to evidence quality and risk\n\n**Score meaning (uniform across sections)**  \n- **0 — Unscorable:** missing data or unverifiable claims.  \n- **1 — Weak:** mostly narrative; minimal primary sources; high uncertainty.  \n- **2 — Partial:** some primary evidence, but gaps in key risks; limited falsifiability.  \n- **3 — IC-ready minimum:** primary sources + quantified metrics + explicit disconfirming evidence; risks mapped.  \n- **4 — Strong:** multiple independent evidence streams; stress tests run; bias checks passed; mitigations credible.  \n- **5 — Best-in-class:** adversarial thinking proven in practice; reproducible analysis; continuous monitoring plan; clear “anti-thesis” and pre-committed exit rules.", "tags": []}
{"fragment_id": "F_R8_523_537", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L523-L537", "text": "### Checklist (per section) with “what earns a 5” and “red flags”\n\nUse this as the final institutional checklist (each section gets a 0–5 score):\n\n| Section | Checklist minimum (≥3) | What earns a 5 | Automatic score cap (cannot exceed) |\n|---|---|---|---|\n| Thesis & macro | Defined regime where thesis fails; exposures quantified | Regime playbook + hedges + disconfirm triggers | Cap at 2 if thesis is non-falsifiable |\n| Problem/PMF | Clear segment; cohort/retention and WTP proxies | Organic PMF proven without subsidies | Cap at 2 if “user = everyone” |\n| Value accrual | Value capture map + leakage analysis | Durable, protocol-enforced capture with stress tests | Cap at 2 if capture is purely narrative |\n| Tokenomics | Emissions/unlocks verified; dilution modeled | Supply + sinks resilient; Sybil-resistant distribution | Cap at 1 if discretionary minting opaque |\n| Incentives | Actor/payoff model; attack profitability evaluated | Attacks provably unprofitable or mitigated in practice | Cap at 2 if profitable attacks unaddressed |\n| Governance | Upgrade process mapped; timelocks verified | Capture simulations + strong dispersion | Cap at 1 if no timelock on upgrades |\n| Tech architecture | Trust/dependencies explicit; liveness analyzed | Formal dependency map + failure drills | Cap at 2 if single point of failure catastrophic |\n| Security posture | Audits + testing + bounty + incidents reviewed | Multiple audits + invariants/fuzz + fast IR maturity | Cap at 2 if unaudited critical path |\n| Decentralization | Multi-dimensional metrics; trends monitored | Decentralization improves with scale | Cap at 2 if centralization is structural |", "tags": []}
{"fragment_id": "F_R8_538_546", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L538-L546", "text": "| Ecosystem/distribution | Integrations verified; dev health measured | Compound distribution with diversified channels | Cap at 2 if “partners” aren’t using it |\n| Competition/moat | Competitor map + switching cost analysis | Structural moat survives subsidy wars | Cap at 2 if commodity + incentives-only |\n| On-chain metrics | Leading indicators defined; manipulation controls | Sybil/incentive-adjusted dashboards + alerts | Cap at 2 if only lagging vanity metrics |\n| Liquidity/market structure | Real liquidity + unlock stress test | Execution proven at target size under stress | Cap at 2 if volume integrity fails |\n| Regulatory/legal | Jurisdiction map; AML/sanctions posture | Clear pathways for custody/listing/distribution | Cap at 2 if high probability of delist/enforcement |\n| Execution risk | Runway modeled; shipping track record | Org maturity + redundancy + IR playbooks | Cap at 2 if repeated missed milestones |\n| Valuation | Method consistent with accrual; sensitivity ranges | Triangulated valuation + scenario consistency | Cap at 2 if price-anchored |\n| Catalysts/timeline | Dated catalysts + measurable expectations | Pre/post KPI commitments + exit triggers | Cap at 2 if catalysts are vague |", "tags": []}
{"fragment_id": "F_R8_547_570", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L547-L570", "text": "### Weighting scheme by strategy type\n\nWeights are **defaults**; adjust for mandate constraints. (Rows sum to 100 per strategy.)\n\n| Section | Liquid long-only | Swing trade | Venture-style | Market-neutral |\n|---|---:|---:|---:|---:|\n| Thesis & macro | 6 | 15 | 3 | 9 |\n| Problem/PMF | 4 | 3 | 13 | 3 |\n| Value accrual | 9 | 5 | 7 | 5 |\n| Tokenomics | 9 | 4 | 8 | 5 |\n| Incentives & game theory | 6 | 4 | 6 | 5 |\n| Governance & upgrade risk | 5 | 3 | 5 | 3 |\n| Technical architecture | 4 | 3 | 9 | 4 |\n| Security posture | 9 | 6 | 7 | 8 |\n| Decentralization | 5 | 2 | 5 | 3 |\n| Ecosystem & distribution | 5 | 3 | 8 | 3 |\n| Competition & moats | 5 | 3 | 7 | 3 |\n| On-chain/usage metrics | 6 | 10 | 3 | 8 |\n| Liquidity & market structure | 9 | 15 | 2 | 17 |\n| Regulatory & legal | 7 | 5 | 6 | 8 |\n| Execution risk (team/ops) | 4 | 4 | 6 | 4 |\n| Valuation | 6 | 6 | 4 | 6 |\n| Catalysts & timeline | 1 | 9 | 1 | 6 |", "tags": []}
{"fragment_id": "F_R8_571_585", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L571-L585", "text": "### Minimum pass thresholds and kill criteria\n\n**Minimum pass (default):**\n- Weighted score **≥ 3.2/5** for the strategy type, **and**\n- No “Red” kill criteria triggered, **and**\n- No section scored **0** in any of: Security posture, Tokenomics, Regulatory/legal, Governance & upgrade risk.\n\n**Kill criteria (automatic no-go unless eliminated)**\n- Upgrade authority can change core logic **without timelock** or credible constraint. citeturn10search18  \n- Critical-path contracts lack independent audits and/or have unresolved critical findings.  \n- Discretionary minting/blacklisting/pause powers exist with opaque scope and no governance constraint.  \n- Liquidity is materially fake (wash trading / non-executable volume), making sizing impossible. citeturn9search0turn9search4  \n- High probability of delisting/enforcement in target jurisdiction(s) with no viable mitigation path (mandate-dependent). citeturn0search3turn1search11  \n- Proven profitable exploit/attack class remains open (oracle manipulation, MEV-driven extraction with user harm) with no deployed mitigations. citeturn0search1turn8search3", "tags": []}
{"fragment_id": "F_R8_586_642", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L586-L642", "text": "### One-page IC summary template\n\n```text\nIC SUMMARY (One Page)\n\nAsset / Protocol:\nStrategy Type: (Liquid long-only / Swing / Venture-style / Market-neutral)\nDecision: (Approve / Watchlist / Reject)\nPosition Constraints: (Max size, liquidity limits, jurisdiction limits)\n\nThesis (1–3 sentences):\n- Core claim:\n- Why now:\n- Anti-thesis (when this fails):\n\nKey Evidence (bullet, cite primary sources):\n- PMF:\n- Value accrual:\n- Tokenomics:\n- Security:\n- Regulatory:\n\nTop Risks (ranked, with mitigations):\n1)\n2)\n3)\n\nCatalysts (dated windows) + What would disappoint:\n- Catalyst A (window):\n  Expected KPI change:\n  Disconfirm trigger:\n- Catalyst B (window):\n\nValuation (method + key assumptions):\n- Method:\n- Base assumptions:\n- Sensitivities that break the case:\n\nScorecard (0–5 each, weighted):\n- Thesis/macro:\n- PMF:\n- Value accrual:\n- Tokenomics:\n- Incentives:\n- Governance:\n- Tech:\n- Security:\n- Decentralization:\n- Ecosystem:\n- Competition:\n- On-chain metrics:\n- Liquidity/market structure:\n- Regulatory:\n- Execution risk:\n- Valuation:\n- Catalysts:", "tags": []}
{"fragment_id": "F_R8_643_649", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L643-L649", "text": "Disconfirming Evidence Checklist (pre-committed exit / no-go triggers):\n- (3–8 items)\n\nCitations log:\n- (links / doc hashes / snapshots)\n```", "tags": []}
{"fragment_id": "F_R8_650_651", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L650-L651", "text": "## Deep Research Workflow", "tags": []}
{"fragment_id": "F_R8_652_669", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L652-L669", "text": "### Step-by-step workflow (IC-ready)\n\n**Step 0 — Declare constraints (before reading the docs)**  \n- Strategy type + holding period + max position size + jurisdiction/custody constraints.  \n- What would force a *no* regardless of upside (kill criteria list you will not waive).\n\n**Step 1 — Build the evidence tree (sections + hypotheses first)**  \n- Create a document with the 17 sections above.  \n- For each section, write **1–3 falsifiable hypotheses** and pre-commit disconfirming evidence triggers (from the earlier section).\n\n**Step 2 — Collect primary sources (freeze snapshots)**  \nPrimary sources to prefer:\n- Protocol whitepaper / technical docs; on-chain contracts; emitted parameters at specific block heights.\n- Code repositories (release tags, commit hashes) and dependency manifests.\n- Audit reports and formal verification artifacts (if any).\n- Governance posts and executed proposals (with timestamps and payloads).\n- Token distribution/unlock contracts and schedules.", "tags": []}
{"fragment_id": "F_R8_670_679", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L670-L679", "text": "**Step 3 — Collect measurement sources (on-chain, off-chain, market)**  \n- On-chain analytics (SQL/decoded event logs, not screenshots).  \n  - Example: entity[\"company\",\"Dune\",\"blockchain analytics platform\"] query workflows and definitions. citeturn8search10turn8search14  \n  - Example: entity[\"organization\",\"The Graph\",\"decentralized indexing protocol\"] for indexed on-chain datasets when appropriate. citeturn8search11turn8search1\n- Market data: consolidated spot/perp order books, funding/OI, venue quality checks, borrow rates (as available).\n- Regulatory sources: jurisdictional rule texts, regulator statements, AML/sanctions guidance.  \n  - entity[\"organization\",\"FATF\",\"global aml standard setter\"] virtual asset guidance / implementation updates. citeturn1search3turn1search11  \n  - entity[\"organization\",\"ESMA\",\"eu securities authority\"] MiCA references and implementation context. citeturn0search3turn0search7  \n  - entity[\"organization\",\"OFAC\",\"us sanctions authority\"] virtual currency sanctions guidance. citeturn7search10", "tags": []}
{"fragment_id": "F_R8_680_690", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L680-L690", "text": "**Step 4 — Run section tests (examples you should standardize)**  \n- Token unlock stress test: shock free float + simulate slippage at size Q; compare to unlock calendar.\n- Security dependency map: enumerate oracles/bridges/admin keys; classify each dependency by failure mode.\n- Governance capture simulation: compute how many entities (and which) can pass proposals; simulate bribery thresholds.\n- Liquidity shock test: widen spreads / reduce depth and re-run execution assumptions (tactical + risk sizing).\n- MEV exposure scan (if DEX): identify sandwichable flows and measure effective execution degradation. citeturn0search1turn8search3\n\n**Step 5 — Apply the bias-check module before scoring**  \n- Run narrative checks: force a “steelman bear case” and require it to be evidence-backed.  \n- Run data checks: wash trading screens + Sybil clustering + incentive-adjusted KPIs. citeturn9search0turn9search5", "tags": []}
{"fragment_id": "F_R8_691_717", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L691-L717", "text": "**Step 6 — Score, weight, decide**  \n- Score each section 0–5 with citations.  \n- Compute weighted score for your strategy type.  \n- Enforce kill criteria with no waivers unless the criterion is eliminated.\n\n**Step 7 — Reproducibility and logging (make it replayable)**  \nBorrow the baseline framework’s discipline: snapshot inputs, version schemas, and make results reproducible. fileciteturn0file0L255-L337  \nConcrete logging practices:\n- Notes as a structured “evidence ledger”: every claim links to (source, date, hash/snapshot).\n- Dataset freeze: block heights for on-chain pulls; query IDs; exchange snapshots with timestamps.\n- Assumption registry: every threshold you used and why; what would change your conclusion.\n\n**Workflow process diagram (template)**\n\n```mermaid\nflowchart TD\n  A[Intake: mandate & constraints] --> B[Hypotheses per section]\n  B --> C[Primary source collection + snapshot]\n  C --> D[On-chain & market data collection]\n  D --> E[Section tests + stress tests]\n  E --> F[Bias checks module]\n  F --> G[Scoring + weighting]\n  G --> H{Kill criteria triggered?}\n  H -- Yes --> I[Reject / Tactical-only scope]\n  H -- No --> J[IC memo + monitoring plan]\n```", "tags": []}
{"fragment_id": "F_R8_718_728", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L718-L728", "text": "### Disconfirming evidence checklist (thesis abandonment triggers)\n\nUse these as **pre-committed exits** (tailor per asset/strategy):\n- Evidence that usage is primarily Sybil/incentive driven (post-incentive collapse).\n- Discovery of unmitigated upgrade/admin control that can seize/brick funds.\n- New critical security incident revealing unknown class risk or weak ops.\n- Material regulatory change that blocks custody/listing for your investor base.\n- Token unlocks + weak liquidity imply unavoidable dilution overhang at your target size.\n- Governance capture event (single coalition repeatedly passes self-serving proposals).\n- Value accrual changes against tokenholder thesis (take rate collapses or value leaks externally).", "tags": []}
{"fragment_id": "F_R8_729_748", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L729-L748", "text": "### Diligence plan by month (30/60/90-day)\n\n**Day 0–30 (Foundations + kill criteria scans)**\n- Freeze primary docs, contracts, and audit history.\n- Build dependency map + admin/upgradeability review.\n- Create token unlock calendar + float model.\n- Stand up baseline on-chain dashboards (leading vs lagging metrics).\n- Run wash trading / liquidity integrity screens. citeturn9search0turn9search4\n\n**Day 31–60 (Deep tests + scenario modeling)**\n- Incentive/game-theory attack surface review (oracle, MEV, governance capture). citeturn0search1turn8search3  \n- Governance and decentralization metrics (multi-dimensional). citeturn2search2turn2search10  \n- Valuation triangulation (3 methods) + sensitivity ranges.\n- Regulatory exposure mapping + distribution constraints (jurisdictional). citeturn0search7turn1search11\n\n**Day 61–90 (IC memo + monitoring system)**\n- Final scoring + weighted decision + explicit disconfirm triggers.\n- “Live monitoring” plan: dashboards + alerts + monthly re-score of top 5 risk sections.\n- Operational plan: execution constraints, custody, venue selection, hedging policy.", "tags": []}
{"fragment_id": "F_R8_749_759", "source_id": "R8", "locator": "deep-research-report-audit (1).md:L749-L759", "text": "**Analyst task checklist (copy/paste)**\n- [ ] Mandate constraints declared; kill criteria written (no waivers).  \n- [ ] Section hypotheses written (confirm/disconfirm/time window).  \n- [ ] Primary sources snapshotted; contracts verified at block heights.  \n- [ ] Tokenomics model: emissions + unlocks + sinks + concentration.  \n- [ ] Security: audits + testing depth + admin/upgradeability + bounty.  \n- [ ] Dependency map: oracles/bridges/infra; failure-mode classification.  \n- [ ] On-chain metrics: leading indicators defined; Sybil/incentive adjustments applied.  \n- [ ] Liquidity integrity: executable depth/spread; wash trading screens completed. citeturn9search0turn9search4  \n- [ ] Regulatory map completed; distribution constraints captured. citeturn0search3turn1search11  \n- [ ] Scoring completed; weighted score computed; IC summary drafted.", "tags": []}
{"fragment_id": "F_R9_1_2", "source_id": "R9", "locator": "deep-research-report.md:L1-L2", "text": "# Full-Stack Research Framework for Systematic Crypto Portfolios", "tags": []}
{"fragment_id": "F_R9_3_12", "source_id": "R9", "locator": "deep-research-report.md:L3-L12", "text": "## Scope, objects, and hard constraints\n\nThis framework assumes trading decisions are generated from **point-in-time, venue-specific state** and executed under an explicit transaction cost + capacity model, with every research artifact versioned for deterministic replay. Funding-driven convergence between perpetual futures and spot, and mark/index constructs provided by venues, are treated as *mechanism primitives* rather than “signals.” citeturn6view0turn6view2turn6view3\n\n**Design knobs (if unspecified, defaults are shown):**\n- **Universe**: spot + perpetual futures (default: both; perps are required to study funding/basis regimes). citeturn6view0turn2search28turn6view3  \n- **Horizon**: intraday to multi-day (default: event-horizon dependent, e.g., minutes–hours for liquidation/vol shocks; hours–days for funding/basis). citeturn6view0turn6view4  \n- **Venue set**: centralized venues first (default), with DEX optional as an additional “routing + latency + gas” regime. Mark/index definitions differ by venue; treat as venue-specific data contracts. citeturn6view2turn5search1turn5search9  \n- **Stack**: Python research + replayable containers; event registry + datasets are immutable snapshots with hashes.", "tags": []}
{"fragment_id": "F_R9_13_18", "source_id": "R9", "locator": "deep-research-report.md:L13-L18", "text": "**Hard constraints enforced everywhere:**\n- **No lookahead bias**: features at decision time \\(t_0\\) use only information available up to \\(t_0\\); labels can use \\(t>t_0\\) but must never leak into features/splits. Look-ahead bias is explicitly defined as using future-unavailable information in a simulation. citeturn8search1turn0search19  \n- **In-sample vs out-of-sample separation**: all edge selection, parameter choosing, and multiplicity correction happen in-sample; only locked specs are evaluated out-of-sample.  \n- **Data-snooping control**: repeated reuse of the same data for model selection inflates false discoveries; therefore apply multiplicity controls and, when appropriate, backtest selection-bias diagnostics and/or “reality check” style procedures. citeturn8search2turn3search3turn0search13  \n- **Execution realism**: PnL is always computed after explicit + implicit costs, using implementation shortfall-style accounting against a decision/arrival benchmark. citeturn7search6turn2search15turn0search20", "tags": []}
{"fragment_id": "F_R9_19_24", "source_id": "R9", "locator": "deep-research-report.md:L19-L24", "text": "Key research objects used across the pipeline:\n- **Event instance** \\(e\\): \\((\\text{event\\_type}, i, v, t_0, \\text{attrs}, \\text{context})\\)\n- **Candidate edge** \\(c\\): \\((e \\rightarrow \\text{action rule})\\) + parameterization + cost/capacity constraints\n- **Strategy spec** \\(s\\): fully executable rules + parameter bounds + data snapshot hash\n- **Portfolio spec** \\(p\\): allocation + correlation control + capacity + execution simulator assumptions", "tags": []}
{"fragment_id": "F_R9_25_28", "source_id": "R9", "locator": "deep-research-report.md:L25-L28", "text": "## Phase 1 — Discovery\n\nDiscovery produces (i) a formal event registry, (ii) context-delta definitions (state transitions), and (iii) invariants that act as cross-asset/venue constraints and data-quality gates.", "tags": []}
{"fragment_id": "F_R9_29_40", "source_id": "R9", "locator": "deep-research-report.md:L29-L40", "text": "### Event registry\n\nAn **event registry** is a deterministic taxonomy of *tradable, labelable* market events with standardized fields and labeling logic. The registry must be versioned because any taxonomy change changes the candidate universe and invalidates prior multiplicity accounting. citeturn8search2turn0search13\n\nBelow is a minimal-but-complete registry suitable for systematic crypto research, with event types chosen to map to well-defined venue mechanics (funding, liquidations, mark/index) and general market microstructure (volatility shocks, liquidity regime breaks, structural breaks). citeturn6view0turn6view2turn0search6turn3search2\n\n**Event registry table (formal definitions + required data fields):**\n\n| Event type | Formal trigger (decision-time predicate) | Minimal required fields (point-in-time) | Notes on mechanism / why it’s definable |\n|---|---|---|---|\n| Funding dislocation | \\(|z(\\text{funding\\_rate}_{i,v}(t_0))| \\ge z_\\*\\) and \\(|\\Delta \\text{premium\\_index}|\\ge p_\\*\\) | perp funding rate, funding interval schedule, mark price, index/spot reference, premium index (or proxy), top-of-book, timestamp | Funding transfers are peer-to-peer and computed from notional × funding rate (venue-defined); designed to anchor perp prices to the underlying index. citeturn6view0turn2search28turn6view3 |\n| Basis / forward dislocation | \\(|\\ln(F/S)|\\ge b_\\*\\) relative to carry bounds and costs | spot mid, futures/perp mid, funding (if perp), borrow/lend proxy (if available), time-to-expiry (if dated), fees/spreads | No-arbitrage forward/spot parity uses cost-of-carry relationships; deviations only tradable if they exceed transaction + financing frictions. citeturn3search12turn3search28turn2search14turn6view4 |", "tags": []}
{"fragment_id": "F_R9_41_46", "source_id": "R9", "locator": "deep-research-report.md:L41-L46", "text": "| Liquidation cascade | liquidation prints intensity above threshold and concurrent OI drop + large return | liquidation volume (if available), open interest, mark price, returns, spread/depth | Liquidations and exchange risk backstops (insurance funds, ADL) create regime-like bursts in forced flow. citeturn4search10turn4search6turn4search22 |\n| Volatility shock | realized vol jump: \\(\\sigma_{\\text{rv}}(t_0)/\\text{MA}(\\sigma_{\\text{rv}})\\ge v_\\*\\) or implied–realized gap spike | OHLCV or trades, realized vol estimator, (optional) implied vol surface | Volatility regimes are persistent and can be modeled with regime-switching / change-point logic. citeturn3search2turn0search6turn6view4 |\n| Liquidity shock | spread widens + depth collapses: \\(s(t_0)\\uparrow\\), \\(D_{\\text{top}}(t_0)\\downarrow\\) | bid/ask, spread, depth at levels, trade/quote volume | Liquidity metrics (spread/depth/impact) are core microstructure state variables; deterioration drives slippage/impact. citeturn4search11turn2search15turn7search5 |\n| Structural break | breakpoint detected in returns/vol/liquidity process | returns series, vol series, liquidity series, breakpoint test outputs | Multiple structural change procedures formalize regime boundary detection in a statistically testable way. citeturn0search2turn0search6 |\n| Regime shift (latent) | posterior \\(P(r_t\\neq r_{t-1})\\ge \\pi_\\*\\) for an HMM/MSM | regime model state, filtered probs, observed features | Markov regime-switching models provide a tractable regime formalism for time series. citeturn3search2turn3search34 |\n| Microstructure imbalance shock | order-book imbalance beyond threshold | L2/L3 order book snapshots, imbalance metric | Order-book imbalance is a definable microstructure metric based on bid/ask queued quantities. citeturn4search11turn4search23 |", "tags": []}
{"fragment_id": "F_R9_47_56", "source_id": "R9", "locator": "deep-research-report.md:L47-L56", "text": "image_group{\"layout\":\"carousel\",\"aspect_ratio\":\"16:9\",\"query\":[\"perpetual swap funding rate diagram mark price index price\",\"crypto liquidation cascade chart open interest drop\",\"order book heatmap liquidity heatmap bid ask depth\"],\"num_per_query\":1}\n\n**Core data fields (normalized schema) required by the registry (minimum viable):**\n- **Identifiers**: `ts_event` (UTC), `instrument_id`, `venue_id`, `contract_type` (spot/perp/future), `quote_ccy`, `base_ccy`\n- **Prices**: `bid1`, `ask1`, `mid`, `last`, `mark_price`, `index_price` (if venue provides), `open/high/low/close` for bar resolutions\n- **Microstructure**: `spread = ask1-bid1`, `depth_L1..Lk` (bid/ask), `trades_count`, `trade_volume`, `vwap`\n- **Derivatives mechanics**: `funding_rate`, `funding_interval`, `premium_index` (or reconstructable proxy), `open_interest`, `liquidation_volume` (if available)\n- **Costs**: `maker_fee_bps`, `taker_fee_bps`, rebates, fee tier info (point-in-time); maker/taker definitions are venue-specific. citeturn5search12turn5search10turn5search14", "tags": []}
{"fragment_id": "F_R9_57_61", "source_id": "R9", "locator": "deep-research-report.md:L57-L61", "text": "Venue data-contract examples (to motivate the need for explicit fields):\n- On entity[\"company\",\"Binance\",\"crypto exchange\"] Futures, funding amount is computed as notional (mark × position size) × funding rate, with default 8-hour intervals; mark price is computed from index/last/funding/order-book inputs using a published formula. citeturn6view0turn6view2  \n- entity[\"company\",\"BitMEX\",\"crypto derivatives exchange\"] documents a funding mechanism composed of interest + premium/discount intended to keep the perpetual price near spot/index. citeturn2search4turn6view3  \n- entity[\"company\",\"Kraken\",\"crypto exchange\"] describes funding periodicity/payments as the mechanism that anchors perpetual prices to spot. citeturn2search28turn5search2", "tags": []}
{"fragment_id": "F_R9_62_88", "source_id": "R9", "locator": "deep-research-report.md:L62-L88", "text": "### Context deltas\n\nA **context delta** is a state transition \\(\\Delta x(t_0)\\) over a fixed “decision horizon” (e.g., last 5 minutes) that modifies the conditional distribution of outcomes given an event. The explicit goal is to avoid unconditional averaging and instead estimate \\( \\mathbb{E}[\\text{PnL} \\mid e, \\Delta x] \\). Markov switching and structural break literature motivates explicit regime/state representations. citeturn3search2turn0search6turn0search2\n\nDefine the context state vector at venue \\(v\\), instrument \\(i\\):\n\\[\nx_{i,v}(t) =\n\\Big[\n\\sigma_{\\text{rv}}(t),\\ \ns(t),\\ \nD_{\\text{top}}(t),\\ \n\\text{imbalance}(t),\\ \n\\text{volume}(t),\\ \n\\text{OI}(t),\\ \n\\text{funding}(t),\\ \n\\text{basis}(t)\n\\Big]\n\\]\nand context delta:\n\\[\n\\Delta x_{i,v}(t_0;\\tau)=x_{i,v}(t_0)-x_{i,v}(t_0-\\tau)\n\\]\n\nRegime labeling options (choose one, but make it deterministic and versioned):\n- **Breakpoint-based regimes**: detect structural changes in returns/vol/liquidity using multiple structural change methods; define regimes as segments between breakpoints. citeturn0search2turn0search6  \n- **Markov regime-switching**: fit a Markov switching model to returns/vol and use filtered probabilities to label regimes (e.g., low-vol vs high-vol). citeturn3search2turn3search34", "tags": []}
{"fragment_id": "F_R9_89_98", "source_id": "R9", "locator": "deep-research-report.md:L89-L98", "text": "### Invariants\n\nInvariants are constraints that should *approximately* hold absent frictions; they are used for (1) sanity checks, (2) generating candidate “mispricing” events, and (3) bounding expected profits by execution/financing frictions.\n\nKey invariants relevant to crypto market structure:\n- **Perp anchoring invariant (mechanism-level)**: perpetual funding is designed to anchor perpetual prices to an underlying spot/index, with funding transfers between longs/shorts computed from notional × funding rate (venue-defined). citeturn6view0turn6view3  \n- **Forward/spot parity (cost-of-carry)**: for an investment asset with no income and no storage costs, no-arbitrage forward pricing relates forward/futures price to spot via \\(F_0 = S_0 e^{rT}\\) (continuous compounding); income/yield modifies the carry term. citeturn3search12turn3search28  \n- **Triangular consistency (cross-rate)**: in currency-like markets, cross rates must align to prevent triangular arbitrage; implement as a product/ratio constraint with bounds widened by spreads/fees. citeturn4search8turn4search0  \n- **Put–call parity (options, if included)**: European put–call parity provides a replicating relationship; deviations are bounded by financing + transaction costs. citeturn4search13turn4search1", "tags": []}
{"fragment_id": "F_R9_99_100", "source_id": "R9", "locator": "deep-research-report.md:L99-L100", "text": "**Invariant enforcement rule:** treat violations as either (a) **data errors** (drop/repair) or (b) **candidate events** only if the violation magnitude exceeds conservative friction bounds (fees + spread + transfer/latency assumptions). The “bounds-first” principle prevents turning micro noise into spurious edges. citeturn8search2turn7search6turn2search15", "tags": []}
{"fragment_id": "F_R9_101_109", "source_id": "R9", "locator": "deep-research-report.md:L101-L109", "text": "### Labeling logic and pseudocode\n\nLabeling must be **event-driven** and **horizon-explicit**. For each event instance at \\(t_0\\), define a fixed label horizon \\(H\\) (in seconds/bars) and compute:\n\\[\ny(e;H) = \\ln\\frac{P(t_0+H)}{P(t_0)}\n\\]\nOptionally define **path-dependent labels** (e.g., max adverse excursion) for risk diagnostics; keep them out of the discovery-stage “edge” ranking unless you already enforce purging/embargo later. citeturn0search19turn8search1\n\n```python", "tags": []}
{"fragment_id": "F_R9_110_110", "source_id": "R9", "locator": "deep-research-report.md:L110-L110", "text": "# PSEUDO-CODE (discovery): registry-driven event labeling", "tags": []}
{"fragment_id": "F_R9_111_113", "source_id": "R9", "locator": "deep-research-report.md:L111-L113", "text": "# Assumes point-in-time feature computation, no future reads for features.\n\ndef compute_features(pt_data, t0, lookbacks):", "tags": []}
{"fragment_id": "F_R9_114_129", "source_id": "R9", "locator": "deep-research-report.md:L114-L129", "text": "# only use data with ts <= t0\n    feats = {}\n    feats[\"mid\"] = mid(pt_data.book[t0])\n    feats[\"spread\"] = pt_data.book[t0].ask1 - pt_data.book[t0].bid1\n    feats[\"depth_top\"] = pt_data.book[t0].bid_qty1 + pt_data.book[t0].ask_qty1\n    feats[\"rv\"] = realized_vol(pt_data.returns.window(end=t0, len=lookbacks[\"rv\"]))\n    feats[\"obi\"] = order_book_imbalance(pt_data.book[t0], levels=lookbacks[\"obi_levels\"])\n    feats[\"funding\"] = pt_data.funding.get_last(t0)          # perp only\n    feats[\"mark\"] = pt_data.mark.get_last(t0)                # if provided\n    feats[\"index\"] = pt_data.index.get_last(t0)              # if provided\n    feats[\"oi\"] = pt_data.open_interest.get_last(t0)         # if provided\n    feats[\"liq_vol\"] = pt_data.liquidations.sum(t0 - 300, t0) # last 5 min\n    return feats\n\ndef detect_events(feats, params):\n    events = []", "tags": []}
{"fragment_id": "F_R9_130_134", "source_id": "R9", "locator": "deep-research-report.md:L130-L134", "text": "# funding dislocation\n    if \"funding\" in feats:\n        z = zscore(feats[\"funding\"], params[\"funding_window\"])\n        if abs(z) >= params[\"funding_z_th\"]:\n            events.append({\"event_type\": \"FUNDING_DISLOCATION\", \"z\": z})", "tags": []}
{"fragment_id": "F_R9_135_137", "source_id": "R9", "locator": "deep-research-report.md:L135-L137", "text": "# liquidation cascade\n    if feats.get(\"liq_vol\", 0) >= params[\"liq_vol_th\"] and feats.get(\"oi\", 0) <= params[\"oi_drop_th\"]:\n        events.append({\"event_type\": \"LIQUIDATION_CASCADE\"})", "tags": []}
{"fragment_id": "F_R9_138_143", "source_id": "R9", "locator": "deep-research-report.md:L138-L143", "text": "# liquidity shock\n    if feats[\"spread\"] >= params[\"spread_th\"] and feats[\"depth_top\"] <= params[\"depth_th\"]:\n        events.append({\"event_type\": \"LIQUIDITY_SHOCK\"})\n    return events\n\ndef label_event(price_series, t0, H):", "tags": []}
{"fragment_id": "F_R9_144_146", "source_id": "R9", "locator": "deep-research-report.md:L144-L146", "text": "# labels are allowed to look forward, but must never feed into features\n    return log(price_series[t0 + H] / price_series[t0])", "tags": []}
{"fragment_id": "F_R9_147_155", "source_id": "R9", "locator": "deep-research-report.md:L147-L155", "text": "# main loop: generate event_instances dataset\nfor (instrument, venue) in universe:\n    for t0 in decision_times:\n        feats = compute_features(pt_data[(instrument, venue)], t0, lookbacks)\n        for ev in detect_events(feats, params):\n            y = label_event(pt_data[(instrument, venue)].mid_price, t0, H=params[\"label_horizon\"])\n            emit_event_instance(instrument, venue, t0, feats, ev, y)\n```", "tags": []}
{"fragment_id": "F_R9_156_159", "source_id": "R9", "locator": "deep-research-report.md:L156-L159", "text": "## Phase 2 — Validation\n\nValidation converts “interesting conditional returns” into **candidate edges** with explicit action rules, after-cost expectancy, statistical controls, and capacity limits.", "tags": []}
{"fragment_id": "F_R9_160_173", "source_id": "R9", "locator": "deep-research-report.md:L160-L173", "text": "### Candidate edge definition and conditional action rules\n\nA candidate edge is a tuple:\n\\[\nc = (\\text{event predicate }E,\\ \\text{context predicate }C,\\ \\text{action }A,\\ \\text{exit }X,\\ \\text{risk }R,\\ \\theta)\n\\]\nwhere \\(\\theta\\) are parameters bounded by pre-registered ranges.\n\nAction rules must be explicit about:\n- **Entry time**: \\(t_{\\text{enter}} = t_0 + \\delta\\) (to model detection + order placement latency)\n- **Entry mechanism**: market vs limit, single venue vs router, target participation rate\n- **Exit rule**: time-based \\(H\\), signal-based reversal, or risk-based stop\n- **Position sizing**: function of risk budget and capacity, not of ex-post performance", "tags": []}
{"fragment_id": "F_R9_174_188", "source_id": "R9", "locator": "deep-research-report.md:L174-L188", "text": "### After-cost expectancy with an explicit execution cost model\n\nUse an **implementation shortfall** style decomposition: compare “paper” decision price to realized execution, capturing explicit fees and implicit costs (spread, impact, delay, opportunity). This is standard transaction-cost accounting and is directly applicable to electronic markets. citeturn7search6turn2search15turn7search30\n\nDefine for each trade \\(j\\) (signed quantity \\(q_j\\), positive for buy):\n- Decision/arrival benchmark \\(p^{\\text{arr}}_j\\) (e.g., mid at signal time \\(t_0\\))\n- Execution price \\(p^{\\text{exec}}_j\\)\n- Fee rate \\(f_j\\) (maker/taker, notional-based; venue schedule is part of point-in-time data) citeturn5search12turn5search10turn5search14  \n\nImplementation shortfall (IS) in currency units:\n\\[\n\\text{IS} = \\sum_j q_j (p^{\\text{exec}}_j - p^{\\text{arr}}_j) + \\sum_j \\text{fees}_j\n\\]\nThis matches the “arrival price” framing used in futures TCA materials and broader execution literature. citeturn2search15turn7search6turn0search20", "tags": []}
{"fragment_id": "F_R9_189_202", "source_id": "R9", "locator": "deep-research-report.md:L189-L202", "text": "**Execution price model (deterministic, calibration-ready):**\n\\[\np^{\\text{exec}}(q,t)=m(t) + \\operatorname{sign}(q)\\left(\\frac{s(t)}{2} + \\text{slip}(q,t) + \\text{impact}(q,t)\\right)\n\\]\nwith:\n- \\(m(t)\\): mid price\n- \\(s(t)\\): spread\n- \\(\\text{slip}\\): short-horizon adverse selection + queue effects (can be modeled empirically per venue/order type)\n- \\(\\text{impact}\\): market impact, calibrated from historical executions or proxy models\n\nImpact modeling options (choose one, then validate it):\n- **Temporary/permanent impact optimal execution family**: foundational models separate temporary vs permanent impact and motivate cost terms used in execution simulators. citeturn0search20turn0search8turn0search0  \n- **Square-root impact scaling**: empirically, impact of large “metaorders” often scales approximately with \\(\\sqrt{Q/ADV}\\) (with volatility scaling), providing a capacity-aware penalty. citeturn7search20turn7search5turn7search0", "tags": []}
{"fragment_id": "F_R9_203_208", "source_id": "R9", "locator": "deep-research-report.md:L203-L208", "text": "A capacity-friendly parametric impact term:\n\\[\n\\text{impact}(Q) = \\eta\\ \\sigma_d\\ \\sqrt{\\frac{Q}{ADV_d}}\n\\]\nwhere \\(\\sigma_d\\) and \\(ADV_d\\) are daily volatility and daily traded value (or volume) proxies computed point-in-time (use rolling estimates). citeturn7search20turn7search5turn6view4", "tags": []}
{"fragment_id": "F_R9_209_216", "source_id": "R9", "locator": "deep-research-report.md:L209-L216", "text": "### Stability across time slices and regimes\n\nValidation must show that expectancy remains positive (after costs) across:\n- **Time slices** (e.g., yearly/quarterly)\n- **Regimes** (low/high vol, liquidity-stressed, funding-stressed), defined by the regime model specified in Discovery. citeturn3search2turn0search6  \n\nBecause labels in event-driven trading often overlap in time (multi-bar horizons), naive k-fold splits leak information. Use **purging and embargoing**: remove training samples whose label horizons overlap the test window, with an added buffer (embargo). citeturn0search19turn0search3turn0search7", "tags": []}
{"fragment_id": "F_R9_217_224", "source_id": "R9", "locator": "deep-research-report.md:L217-L224", "text": "### Multiplicity-adjusted significance\n\nDiscovery + tuning implicitly creates many hypotheses. Correct for multiple testing using at least one of:\n- **False discovery rate (FDR) control** via the step-up procedure (sort p-values \\(p_{(k)}\\), find largest \\(k\\) with \\(p_{(k)} \\le \\frac{k}{m}\\alpha\\)). citeturn0search13turn0search1  \n- **Reality-check style bootstrap for data snooping** when comparing many candidate rules to a benchmark on the same sample (addresses “best-of-many” selection). citeturn8search2turn8search25  \n\nSelection-bias in optimized backtests is also addressed by adjusted performance statistics such as the **Deflated Sharpe Ratio** when many trials/parameter sets are tested. citeturn3search3turn3search7", "tags": []}
{"fragment_id": "F_R9_225_238", "source_id": "R9", "locator": "deep-research-report.md:L225-L238", "text": "### Density and capacity constraints\n\nA candidate that “works” at tiny size but fails at realistic size is not a tradable edge. Enforce:\n- **Participation constraint**: for execution horizon \\(T_{\\text{exec}}\\), require\n\\[\n\\rho = \\frac{|Q|}{V(t_0, t_0+T_{\\text{exec}})} \\le \\rho_{\\max}\n\\]\nwith \\(V\\) measured volume (or dollar volume) in the execution window.\n- **Impact scaling constraint**: projected impact must not consume expectancy:\n\\[\n\\mathbb{E}[\\text{edge}] - \\mathbb{E}[\\text{costs}] \\ge \\epsilon_{\\min} \\quad \\text{where costs include } \\text{impact}(Q)\n\\]\nSquare-root impact scaling provides a direct way to quantify how costs increase with size. citeturn7search20turn7search0turn7search5", "tags": []}
{"fragment_id": "F_R9_239_254", "source_id": "R9", "locator": "deep-research-report.md:L239-L254", "text": "### Validation workflow and rejection criteria\n\n**Workflow (deterministic):**\n1. Build event instances from the registry (fixed version + fixed dataset hash).\n2. For each candidate rule family, pre-register parameter bounds \\(\\theta \\in [\\theta_{\\min},\\theta_{\\max}]\\).\n3. Estimate after-cost returns using the explicit execution model; compute expectancy and distributional stats.\n4. Evaluate across slices and regimes; compute multiplicity-adjusted significance.\n5. Apply capacity filter and stress cost assumptions (spread widening, reduced depth).\n6. Freeze passing candidates into the Blueprint YAML.\n\n**Minimum rejection criteria (implementation-ready):**\n- After-cost mean return \\(\\le 0\\) in aggregate **or** in any “core regime” bucket (e.g., top-2 most frequent regimes).\n- Fails FDR/reality-check threshold at target \\(\\alpha\\) after accounting for tested hypotheses. citeturn0search13turn8search2  \n- Capacity at target capital implies participation/impact costs that erase ≥X% of expectancy.\n- Performance collapses under modest execution perturbations consistent with liquidity shocks (spread/depth deterioration). citeturn2search15turn4search11turn7search5", "tags": []}
{"fragment_id": "F_R9_255_290", "source_id": "R9", "locator": "deep-research-report.md:L255-L290", "text": "## Blueprint\n\nBlueprint is the “contract” that makes research deterministic, reproducible, and auditable. It encodes all identifiers, datasets, bounds, and run controls as executable specs (not prose). Data endpoints and mechanics (e.g., funding schedules, candlestick identity) are treated as part of the dataset contract. citeturn5search1turn6view0turn5search9\n\n```yaml\nblueprint_version: \"1.0.0\"\n\nevent_registry:\n  event_registry_version: \"0.3.0\"\n  registry_hash_sha256: \"<sha256_of_registry_yaml>\"\n  definitions_source_notes:\n    - \"Perp funding/mark/index definitions are venue-specific; store raw fields + normalization.\"\n    - \"Funding payment = notional * funding_rate (venue-defined).\"\n\ndataset:\n  dataset_id: \"crypto_research_snapshot_2026-02-18\"\n  dataset_hash_sha256: \"<sha256_of_canonical_export>\"\n  canonicalization:\n    format: \"parquet\"\n    sort_keys: [\"venue_id\", \"instrument_id\", \"ts\"]\n    tz: \"UTC\"\n    null_policy: \"explicit_nulls_preserved\"\n  sources:\n    market_data:\n      - type: \"trades\"\n        fields: [\"ts\", \"price\", \"qty\", \"side\"]\n      - type: \"l2_book\"\n        fields: [\"ts\", \"bid_px_1\", \"bid_qty_1\", \"ask_px_1\", \"ask_qty_1\", \"depth_levels_k\"]\n      - type: \"mark_index\"\n        fields: [\"ts\", \"mark_price\", \"index_price\"]\n      - type: \"funding_open_interest\"\n        fields: [\"ts\", \"funding_rate\", \"funding_interval\", \"open_interest\"]\n    cost_data:\n      - type: \"fee_schedule_point_in_time\"\n        fields: [\"ts_effective\", \"maker_fee_bps\", \"taker_fee_bps\", \"rebates\", \"tier_rule_id\"]", "tags": []}
{"fragment_id": "F_R9_291_330", "source_id": "R9", "locator": "deep-research-report.md:L291-L330", "text": "candidate_id:\n  schema: \"C-{event_type}-{instrument_id}-{venue_id}-{horizon}-{direction}-{param_hash}-{spec_version}\"\n  example: \"C-FUNDING_DISLOCATION-BTCUSDT-PERP-VENUEA-H8H-LONG-<phash>-v1\"\n  determinism:\n    param_hash: \"sha256(json_canonical(params))\"\n    code_commit: \"<git_commit>\"\n    rng_seed: 0\n\nparameter_bounds:\n  funding_dislocation:\n    funding_window_hours: [24, 720]\n    funding_z_th: [1.5, 5.0]\n    premium_proxy_th: [0.0001, 0.01]\n    entry_delay_seconds: [0, 10]\n  liquidation_cascade:\n    liq_vol_th_percentile: [0.90, 0.999]\n    oi_drop_th_percentile: [0.80, 0.99]\n    entry_delay_seconds: [0, 10]\n  liquidity_shock:\n    spread_th_bps: [2, 200]\n    depth_drop_percentile: [0.01, 0.20]\n\nvalidation_spec:\n  in_sample:\n    split_method: \"purged_walk_forward\"\n    embargo_fraction: 0.05\n    min_train_days: 365\n    test_block_days: 30\n  out_of_sample:\n    locked_params: true\n    no_refitting: true\n  multiplicity_control:\n    method: \"FDR_step_up\"\n    alpha: 0.05\n  execution_cost_model:\n    fees: \"maker_taker_point_in_time\"\n    spread: \"half_spread_crossing\"\n    impact: \"sqrt(Q/ADV)_vol_scaled\"\n    slippage: \"empirical_bucket_model\"", "tags": []}
{"fragment_id": "F_R9_331_338", "source_id": "R9", "locator": "deep-research-report.md:L331-L338", "text": "reproducibility_checklist:\n  - \"All features computed with ts <= decision_ts.\"\n  - \"All fee schedules are point-in-time (ts_effective <= decision_ts).\"\n  - \"All splits are time-ordered; purging/embargo applied for overlapping horizons.\"\n  - \"Store: dataset_hash, registry_hash, code_commit, container_digest, run_config_hash.\"\n  - \"Store full random seeds and any sampling bootstrap seeds.\"\n```", "tags": []}
{"fragment_id": "F_R9_339_342", "source_id": "R9", "locator": "deep-research-report.md:L339-L342", "text": "## Strategy\n\nStrategy turns each validated candidate into a single-strategy research artifact with a clean train/test split, sensitivity surfaces, and stress tests. Overfitting risk rises with parameter search; therefore the strategy workflow must report robustness, not just point estimates. citeturn3search3turn8search2turn0search13", "tags": []}
{"fragment_id": "F_R9_343_350", "source_id": "R9", "locator": "deep-research-report.md:L343-L350", "text": "### Single-strategy backtest specs\n\n**Backtest unit of simulation:** event-triggered orders with explicit entry latency, order type, and execution simulator. Execution quality is measured using implementation shortfall framing against an arrival benchmark. citeturn2search15turn7search6turn0search20\n\n**Train/test split (deterministic):**\n- Use **time-ordered** splits; never random shuffle. citeturn8search1turn0search19  \n- Default: **purged walk-forward** with embargo (defined in Blueprint) to avoid label overlap leakage. citeturn0search19turn0search3", "tags": []}
{"fragment_id": "F_R9_351_358", "source_id": "R9", "locator": "deep-research-report.md:L351-L358", "text": "### Parameter sweeps and sensitivity maps\n\nFor each parameter in \\(\\theta\\), compute a sensitivity grid and report:\n- Heatmap data: \\(\\text{Sharpe}_{\\text{after-cost}}(\\theta)\\), hit-rate, turnover, max drawdown, tail loss\n- *Stability score*: fraction of parameter neighborhood with positive after-cost expectancy and significant under multiplicity control\n\nWhen many parameter sets are tried, report a selection-bias-aware statistic (e.g., deflated Sharpe) or a data-snooping-aware procedure alongside conventional metrics. citeturn3search3turn8search2", "tags": []}
{"fragment_id": "F_R9_359_366", "source_id": "R9", "locator": "deep-research-report.md:L359-L366", "text": "### Stress tests\n\nStress tests must map directly to known crypto venue failure modes and microstructure deterioration:\n\n- **Liquidity shock**: multiply spread by \\(k_s\\), reduce top-of-book depth by \\(k_d\\), and re-simulate fills (captures deterioration in post-trade costs). citeturn4search11turn7search5turn2search15  \n- **Volatility expansion**: scale realized volatility used in impact model; square-root impact law implies higher volatility increases impact cost for a given \\(Q/ADV\\). citeturn7search20turn7search0  \n- **Exchange outage / forced deleveraging regime**: simulate a no-fill window for a venue and/or forced position reduction; exchanges may use insurance funds and auto-deleveraging mechanisms in extreme conditions. citeturn4search10turn4search6turn4search22", "tags": []}
{"fragment_id": "F_R9_367_388", "source_id": "R9", "locator": "deep-research-report.md:L367-L388", "text": "### Output performance table and robustness diagnostics (template)\n\n**Performance table schema (populate with your computed values):**\n\n| Metric | In-sample | Out-of-sample | Notes |\n|---|---:|---:|---|\n| Mean after-cost return per trade |  |  | After all modeled costs |\n| Sharpe (after-cost) |  |  | Also report deflated Sharpe if many trials |\n| Hit rate |  |  | Conditional on event triggers |\n| Avg / p95 implementation shortfall |  |  | Arrival-price benchmark |\n| Turnover (notional/day) |  |  | Drives cost + capacity |\n| Max drawdown |  |  | Use equity curve from simulated fills |\n| Capacity at \\(\\rho_{\\max}\\) |  |  | Participation/impact constrained |\n| Regime stability score |  |  | Fraction of regimes with positive expectancy |\n| Stress delta (liq shock) |  |  | Degradation under spread/depth shock |\n| Stress delta (venue outage) |  |  | Exposure to venue availability |\n\nRobustness diagnostics to store per strategy:\n- Multiplicity-adjusted significance result (FDR / reality-check outcome). citeturn0search13turn8search2  \n- Parameter neighborhood stability and “edge half-life” across rolling windows.  \n- Execution sensitivity: outcomes under fee tier changes, maker vs taker mix (maker/taker schedules are venue-defined). citeturn5search12turn5search14", "tags": []}
{"fragment_id": "F_R9_389_392", "source_id": "R9", "locator": "deep-research-report.md:L389-L392", "text": "## Portfolio\n\nPortfolio combines validated strategies into a single allocation + execution system with correlation control, capacity modeling, and cross-venue execution simulation. Portfolio optimization with transaction costs and constraints is naturally expressed in convex optimization form when costs/constraints are convex (or approximated as such). citeturn7search3turn1search3turn7search11", "tags": []}
{"fragment_id": "F_R9_393_409", "source_id": "R9", "locator": "deep-research-report.md:L393-L409", "text": "### Allocation method\n\nRepresent each strategy \\(s\\) by an after-cost return series \\(r_s(t)\\) and (optional) a forecast \\(\\hat{\\mu}_s(t)\\). Portfolio weights \\(w(t)\\) can be set by one of these deterministic methods (choose one as the “primary,” keep others as ablations):\n\n**Convex optimization (cost-aware, constraint-first):**\n\\[\n\\max_{w}\\ \\hat{\\mu}^\\top w - \\lambda w^\\top \\Sigma w - \\gamma \\, \\text{TC}(w, w_{-1})\n\\]\nsubject to bounds:\n\\[\nw_{\\min}\\le w \\le w_{\\max},\\quad \\|w\\|_1 \\le L_{\\max},\\quad \\text{exposure/venue caps}\n\\]\nConvex optimization is the standard framework for efficiently solving such constrained problems, and portfolio optimization with linear/fixed transaction costs has established convex formulations/relaxations. citeturn1search3turn7search3turn7search11  \n\n**Risk parity / equal risk contribution (forecast-light):**\nSolve for weights such that each component contributes equally to total portfolio risk (ex-ante), reducing reliance on fragile mean estimates. citeturn1search0turn1search34", "tags": []}
{"fragment_id": "F_R9_410_412", "source_id": "R9", "locator": "deep-research-report.md:L410-L412", "text": "**Growth-optimal (log-utility) sizing, capped:**\nUse growth-optimal sizing logic as an input, then cap per-strategy leverage/exposure to control estimation error; the original growth-optimal principle maximizes expected log wealth under repeated betting assumptions. citeturn1search1turn1search21", "tags": []}
{"fragment_id": "F_R9_413_425", "source_id": "R9", "locator": "deep-research-report.md:L413-L425", "text": "### Correlation control and covariance estimation\n\n**Rolling covariance** is necessary but noisy; improve stability with **shrinkage** toward a structured target (e.g., identity) to obtain a better-conditioned estimator in higher dimensions. citeturn1search12turn1search4turn1search24  \n\nRegime-conditioned correlation:\n- Compute regimes \\(r(t)\\) from the Discovery regime model.\n- Estimate \\(\\Sigma^{(k)}\\) within each regime \\(k\\).\n- Allocate under a “worst-regime” or weighted-regime risk objective:\n\\[\n\\min_{w} \\ \\max_k \\ w^\\top \\Sigma^{(k)} w\n\\]\nThis explicitly controls correlation spikes in stress regimes instead of assuming stationarity. citeturn3search2turn0search6turn1search3", "tags": []}
{"fragment_id": "F_R9_426_439", "source_id": "R9", "locator": "deep-research-report.md:L426-L439", "text": "### Capacity modeling\n\nCapacity is computed per strategy and then aggregated with portfolio-level constraints.\n\nPer strategy \\(s\\):\n1. Determine feasible participation \\(\\rho_{\\max}\\) and execution window \\(T_{\\text{exec}}\\).\n2. Estimate \\(ADV\\) and \\(\\sigma\\) in the traded venue/instrument.\n3. Use an impact model (e.g., square-root) to map capital \\(K\\) to expected impact cost.\n4. Define capacity \\(K^\\*\\) as largest \\(K\\) satisfying:\n\\[\n\\mathbb{E}[\\text{edge}(K)] - \\mathbb{E}[\\text{cost}(K)] \\ge \\epsilon_{\\min}\n\\]\nSquare-root impact scaling provides a concrete way to do step (3). citeturn7search20turn7search0turn7search5", "tags": []}
{"fragment_id": "F_R9_440_451", "source_id": "R9", "locator": "deep-research-report.md:L440-L451", "text": "### Execution simulation across venues\n\nA minimal cross-venue simulator must model:\n- **Order type**: market vs limit (with queue uncertainty)\n- **Fee schedule**: maker/taker notional fees (point-in-time, tier-aware) citeturn5search12turn5search10  \n- **Benchmark**: arrival price implementation shortfall accounting citeturn2search15turn7search6  \n- **Market impact**: size-dependent cost term citeturn0search20turn7search20  \n- **Venue risk events**: liquidation/ADL/outage conditions as scenario toggles citeturn4search10turn4search22  \n\nExecution inputs derived from venue documentation and market data contracts:\n- Candlestick and index-price kline identity rules (key for deterministic bar building) are defined in venue APIs. citeturn5search1turn5search9", "tags": []}
{"fragment_id": "F_R9_452_460", "source_id": "R9", "locator": "deep-research-report.md:L452-L460", "text": "### Portfolio risk controls\n\nRisk controls are deterministic gates that operate on point-in-time observables:\n- **Exposure caps**: per instrument, per venue, per strategy, and gross/net leverage.\n- **Liquidity gates**: if spread/depth exceed thresholds (liquidity shock context), reduce or halt new risk. citeturn4search11turn7search5  \n- **Mechanism-aware venue risk gates**: monitor liquidation/ADL risk periods; exchanges describe ADL as an emergency mechanism in extreme cases. citeturn4search10turn4search22  \n- **Rebalance friction control**: incorporate transaction costs directly into allocation (convex cost-aware optimization / risk parity with turnover penalty). citeturn7search3turn7search11turn1search0  \n\n**Next deeper angle of analysis:** once registry + validation are wired, quantify how *taxonomy breadth* (number of event types and parameter sweeps) changes false discovery rates and capacity-adjusted opportunity set size—then use that to set an explicit “research budget” (maximum hypotheses per dataset snapshot) under your chosen multiplicity and data-snooping controls. citeturn8search2turn0search13turn3search3", "tags": []}
{"fragment_id": "F_R10_1_11", "source_id": "R10", "locator": "crpyto hypo.txt:L1-L11", "text": "Below is the **full conversion** of your conditional-market-physics research into a **crypto-native hypothesis set**, rewritten strictly under your framework and constraints.\n\nAssumptions (explicit):\n\n* Instruments: BTC, ETH, SOL perpetuals + spot (generalizable to liquid perps).\n* Data: 1m–15m OHLCV, funding, mark/index, OI, session clocks.\n* No results invented.\n* All hypotheses are **event-study compatible** and convertible into variables.\n\n---", "tags": []}
{"fragment_id": "F_R10_12_27", "source_id": "R10", "locator": "crpyto hypo.txt:L12-L27", "text": "# I. Core Market Physics (Crypto Version)\n\nCrypto replaces FX session structure with **mechanism-driven regime shifts**:\n\n**Primary state drivers**\n\n1. Volatility regime (compression ↔ expansion)\n2. Leverage pressure (funding + OI)\n3. Liquidity state (spread/depth)\n4. Time anchors (UTC boundaries, US hours)\n5. Position liquidation feedback loops\n\nEdges emerge when **state transitions**, not indicators.\n\n---", "tags": []}
{"fragment_id": "F_R10_28_33", "source_id": "R10", "locator": "crpyto hypo.txt:L28-L33", "text": "# II. 15 Ranked Conditional Market Physics Hypotheses (Crypto)\n\nRanking logic = expected structural strength × universality × measurability.\n\n---", "tags": []}
{"fragment_id": "F_R10_34_61", "source_id": "R10", "locator": "crpyto hypo.txt:L34-L61", "text": "## 1. Compression → Expansion at Time Anchors\n\n**BEHAVIOR:** Volatility expansion after prolonged compression.\n\n**CONDITION:**\n`atr_20_pct ≤ 0.20`\nAND `compression_duration ≥ 24 bars`\nAND `anchor_flag ∈ {utc_00, us_cash_open}`\n\n**MEASUREMENT:**\nForward realized volatility distribution shift.\n\n**HORIZON:** 16–64 bars.\n\n**EXPECTED EFFECT:** Volatility expansion.\n\n**VARIABLES:**\n`atr_20_pct`, `compression_duration`, `anchor_flag`, `fwd_rv_32`.\n\n**TEST DESIGN:**\nEvent study vs same-minute-of-week control sample.\n\n**FAILURE MODES:** Macro news releases.\n\n**NEXT STEP:** Build ATR percentile regime table.\n\n---", "tags": []}
{"fragment_id": "F_R10_62_86", "source_id": "R10", "locator": "crpyto hypo.txt:L62-L86", "text": "## 2. Extreme Funding → Basis Compression\n\n**BEHAVIOR:** Crowded positioning unwinds.\n\n**CONDITION:**\n`|funding_rate_pct| ≥ 0.90`\n\n**MEASUREMENT:**\nChange in spot–perp basis distribution.\n\n**HORIZON:** 1–3 funding intervals.\n\n**EXPECTED EFFECT:** Mean reversion.\n\n**VARIABLES:**\n`funding_rate_pct`, `basis`, `basis_change_fwd`.\n\n**TEST DESIGN:** Conditional distribution vs median funding periods.\n\n**FAILURE MODES:** Persistent directional trend regimes.\n\n**NEXT STEP:** Align funding timestamps precisely.\n\n---", "tags": []}
{"fragment_id": "F_R10_87_112", "source_id": "R10", "locator": "crpyto hypo.txt:L87-L112", "text": "## 3. Funding Flip + Rising OI → Continuation Risk\n\n**BEHAVIOR:** New leverage enters trend.\n\n**CONDITION:**\n`funding_sign_flip = 1`\nAND `Δoi_z ≥ +1.5`.\n\n**MEASUREMENT:**\nRight-tail expansion of forward returns.\n\n**HORIZON:** 8–48 bars.\n\n**EXPECTED EFFECT:** Drift continuation.\n\n**VARIABLES:**\n`funding_sign`, `oi`, `oi_z`, `ret_fwd`.\n\n**TEST DESIGN:** Compare flip+OI vs flip-only.\n\n**FAILURE MODES:** Low liquidity regimes.\n\n**NEXT STEP:** Compute rolling OI z-score.\n\n---", "tags": []}
{"fragment_id": "F_R10_113_138", "source_id": "R10", "locator": "crpyto hypo.txt:L113-L138", "text": "## 4. Liquidation Cascade → Volatility Overshoot then Reset\n\n**BEHAVIOR:** Forced deleveraging expands volatility then normalizes.\n\n**CONDITION:**\n`|return| ≥ 3σ`\nAND `spread_pct ≥ 0.90`.\n\n**MEASUREMENT:**\nForward realized vol path.\n\n**HORIZON:** 4–24 bars.\n\n**EXPECTED EFFECT:** Expansion → reset.\n\n**VARIABLES:**\n`return_z`, `spread_pct`, `rv_fwd`.\n\n**TEST DESIGN:** Two-stage labeling (shock vs post-normalization).\n\n**FAILURE MODES:** News-driven repricing.\n\n**NEXT STEP:** Build spread percentile regime.\n\n---", "tags": []}
{"fragment_id": "F_R10_139_164", "source_id": "R10", "locator": "crpyto hypo.txt:L139-L164", "text": "## 5. OI Drop During Price Drop → Exhaustion\n\n**BEHAVIOR:** Deleveraging exhaustion.\n\n**CONDITION:**\n`ret_W ≤ -2σ`\nAND `ΔOI_W ≤ -1.5σ`.\n\n**MEASUREMENT:**\nForward return median shift.\n\n**HORIZON:** 8–32 bars.\n\n**EXPECTED EFFECT:** Reversion.\n\n**VARIABLES:**\n`ret_W`, `Δoi_z`.\n\n**TEST DESIGN:** Compare price-drop events with vs without OI decline.\n\n**FAILURE MODES:** Macro trend breaks.\n\n**NEXT STEP:** Joint price/OI event labeling.\n\n---", "tags": []}
{"fragment_id": "F_R10_165_189", "source_id": "R10", "locator": "crpyto hypo.txt:L165-L189", "text": "## 6. VWAP Distance Extreme → Reversion Probability Increase\n\n**BEHAVIOR:** Inventory imbalance normalization.\n\n**CONDITION:**\n`|price − VWAP| / ATR ≥ 2`.\n\n**MEASUREMENT:**\nProbability of VWAP touch.\n\n**HORIZON:** 8–24 bars.\n\n**EXPECTED EFFECT:** Mean reversion.\n\n**VARIABLES:**\n`vwap_dist_atr`, `touch_vwap_flag`.\n\n**TEST DESIGN:** Hazard-rate estimation.\n\n**FAILURE MODES:** Trend acceleration regimes.\n\n**NEXT STEP:** Compute VWAP normalized distance.\n\n---", "tags": []}
{"fragment_id": "F_R10_190_214", "source_id": "R10", "locator": "crpyto hypo.txt:L190-L214", "text": "## 7. Prior High/Low Sweep → Distribution Skew Shift\n\n**BEHAVIOR:** Liquidity removal alters return distribution.\n\n**CONDITION:**\nHigh/low breached by ≥ 0.25 ATR then closes back inside.\n\n**MEASUREMENT:**\nForward skewness + drift direction.\n\n**HORIZON:** 16 bars.\n\n**EXPECTED EFFECT:** Regime-dependent drift.\n\n**VARIABLES:**\n`sweep_flag`, `close_position`.\n\n**TEST DESIGN:** Event-aligned returns.\n\n**FAILURE MODES:** Strong trend days.\n\n**NEXT STEP:** Implement sweep detector.\n\n---", "tags": []}
{"fragment_id": "F_R10_215_239", "source_id": "R10", "locator": "crpyto hypo.txt:L215-L239", "text": "## 8. Compression Duration vs Breakout Magnitude\n\n**BEHAVIOR:** Stored volatility releases proportionally.\n\n**CONDITION:**\n`atr_pct ≤ 0.25`.\n\n**MEASUREMENT:**\nCorrelation between compression length and forward range.\n\n**HORIZON:** Next expansion event.\n\n**EXPECTED EFFECT:** Larger expansion tails.\n\n**VARIABLES:**\n`compression_len`, `fwd_range`.\n\n**TEST DESIGN:** Conditional quantile regression (descriptive).\n\n**FAILURE MODES:** Regime shifts.\n\n**NEXT STEP:** Build compression clock.\n\n---", "tags": []}
{"fragment_id": "F_R10_240_264", "source_id": "R10", "locator": "crpyto hypo.txt:L240-L264", "text": "## 9. Weekend Liquidity Regime → Tail Expansion\n\n**BEHAVIOR:** Thin liquidity amplifies moves.\n\n**CONDITION:**\n`weekend_flag = 1`.\n\n**MEASUREMENT:**\nTail percentile comparison.\n\n**HORIZON:** 12–48 bars.\n\n**EXPECTED EFFECT:** Volatility increase.\n\n**VARIABLES:**\n`weekend_flag`, `range_pct`.\n\n**TEST DESIGN:** Same-time weekday controls.\n\n**FAILURE MODES:** Major announcements.\n\n**NEXT STEP:** Tag weekend periods.\n\n---", "tags": []}
{"fragment_id": "F_R10_265_275", "source_id": "R10", "locator": "crpyto hypo.txt:L265-L275", "text": "## 10. Mark–Index Gap Spike → Microstructure Reversion\n\n**CONDITION:**\n`gap_z ≥ 2`.\n\n**MEASUREMENT:** Gap closure probability.\n\n**EXPECTED EFFECT:** Reversion.\n\n---", "tags": []}
{"fragment_id": "F_R10_276_283", "source_id": "R10", "locator": "crpyto hypo.txt:L276-L283", "text": "## 11. Low Vol + Funding Extreme → Expansion Trigger\n\nInteraction hypothesis.\n\n**EXPECTED EFFECT:** Expansion.\n\n---", "tags": []}
{"fragment_id": "F_R10_284_289", "source_id": "R10", "locator": "crpyto hypo.txt:L284-L289", "text": "## 12. Trend Age ≥ Percentile 80 → Continuation Decay\n\n**MEASUREMENT:** Conditional continuation probability.\n\n---", "tags": []}
{"fragment_id": "F_R10_290_297", "source_id": "R10", "locator": "crpyto hypo.txt:L290-L297", "text": "## 13. Consecutive Directional Bars → Reversal Hazard Increase\n\n**CONDITION:** ≥5 same-direction closes.\n\n**MEASUREMENT:** Reversal probability curve.\n\n---", "tags": []}
{"fragment_id": "F_R10_298_303", "source_id": "R10", "locator": "crpyto hypo.txt:L298-L303", "text": "## 14. OI Expansion Without Price Movement → Latent Break Risk\n\n**EXPECTED:** Future volatility increase.\n\n---", "tags": []}
{"fragment_id": "F_R10_304_309", "source_id": "R10", "locator": "crpyto hypo.txt:L304-L309", "text": "## 15. Basis Widening + Compression → Violent Convergence\n\n**EXPECTED:** Expansion + basis collapse.\n\n---", "tags": []}
{"fragment_id": "F_R10_310_339", "source_id": "R10", "locator": "crpyto hypo.txt:L310-L339", "text": "# III. Variable Glossary (Top 25)\n\n| Variable             | Definition               |\n| -------------------- | ------------------------ |\n| atr_20_pct           | ATR percentile           |\n| compression_duration | bars since low-vol start |\n| rv                   | realized volatility      |\n| rv_pct               | RV percentile            |\n| funding_rate         | perp funding             |\n| funding_rate_pct     | percentile               |\n| funding_sign_flip    | sign change              |\n| basis                | perp − spot              |\n| basis_change         | forward change           |\n| oi                   | open interest            |\n| Δoi                  | change                   |\n| oi_z                 | standardized change      |\n| spread               | bid-ask                  |\n| spread_pct           | percentile               |\n| vwap                 | rolling session VWAP     |\n| vwap_dist_atr        | normalized distance      |\n| sweep_flag           | liquidity sweep          |\n| anchor_flag          | time anchor              |\n| weekend_flag         | weekend regime           |\n| return_z             | standardized return      |\n| fwd_range            | forward range            |\n| fwd_rv               | forward RV               |\n| trend_age            | bars in trend            |\n| directional_streak   | consecutive closes       |\n| gap_z                | mark-index divergence    |", "tags": []}
{"fragment_id": "F_R10_340_341", "source_id": "R10", "locator": "crpyto hypo.txt:L340-L341", "text": "---", "tags": []}
{"fragment_id": "F_R10_342_356", "source_id": "R10", "locator": "crpyto hypo.txt:L342-L356", "text": "# IV. Priority Scoreboard\n\n| Hypothesis                   | Edge Strength | Universality | Ease | Total  |\n| ---------------------------- | ------------- | ------------ | ---- | ------ |\n| Compression→Anchor Expansion | 5             | 5            | 5    | **15** |\n| Funding Extreme Reversion    | 5             | 5            | 4    | **14** |\n| VWAP Distance Reversion      | 4             | 5            | 5    | **14** |\n| OI Drop Exhaustion           | 4             | 4            | 4    | 12     |\n| Compression Length Effect    | 4             | 5            | 3    | 12     |\n| Funding Flip + OI            | 4             | 4            | 3    | 11     |\n| Liquidation Cascade          | 5             | 3            | 3    | 11     |\n| Sweep Distribution Shift     | 4             | 4            | 3    | 11     |\n| Weekend Regime               | 3             | 5            | 5    | 13     |\n| Remaining hypotheses         | 3–4           | 3–4          | 3–4  | 9–11   |", "tags": []}
{"fragment_id": "F_R10_357_358", "source_id": "R10", "locator": "crpyto hypo.txt:L357-L358", "text": "---", "tags": []}
{"fragment_id": "F_R10_359_377", "source_id": "R10", "locator": "crpyto hypo.txt:L359-L377", "text": "# V. Immediate Implementation Order (critical)\n\nYour pipeline already fits this architecture. Do **only this next**:\n\n1. Build unified **event table**\n\n   ```\n   event_id\n   timestamp\n   condition_flags\n   regime_features\n   forward_labels\n   ```\n2. Compute forward distributions once.\n3. Reuse labels across hypotheses.\n4. Evaluate conditional shifts — not PnL.\n\n---", "tags": []}
{"fragment_id": "F_R10_378_381", "source_id": "R10", "locator": "crpyto hypo.txt:L378-L381", "text": "## Next deeper step\n\nConstruct the **minimal universal labeling schema** that allows *all 15 hypotheses* to be evaluated from one dataset pass (this is the single highest leverage structural improvement).\nIf desired, the next step is a schema + SQL/Parquet layout matching your Backtest repo so hypotheses become one-line queries.", "tags": []}
{"fragment_id": "F_R10_382_385", "source_id": "R10", "locator": "crpyto hypo.txt:L382-L385", "text": "## Minimal universal labeling schema for all 15 hypotheses (crypto perps)\n\nGoal: one dataset pass produces (a) **state at decision time t₀** and (b) **forward distribution labels**. Hypotheses become simple filters over an `events` table joined to `bars` and `forward_labels`.", "tags": []}
{"fragment_id": "F_R10_386_394", "source_id": "R10", "locator": "crpyto hypo.txt:L386-L394", "text": "### Design principles\n\n1. **Two timestamps per row**: `t_close` (bar close time) and `t0` (= decision time). Define `t0 = t_close` for close-to-next-bar execution research, or `t0 = t_open` if you prefer open-to-close. Pick one and keep it invariant.\n2. **All features must be computable with `ts ≤ t0`** (strict PIT).\n3. **Forward labels are derived only from future bars** and stored separately (no accidental reuse in features).\n4. **Events are just Boolean predicates over features**; do not compute separate event datasets per hypothesis.\n\n---", "tags": []}
{"fragment_id": "F_R10_395_396", "source_id": "R10", "locator": "crpyto hypo.txt:L395-L396", "text": "# A) Data model (Parquet/SQL friendly)", "tags": []}
{"fragment_id": "F_R10_397_411", "source_id": "R10", "locator": "crpyto hypo.txt:L397-L411", "text": "## 1) `bars` (base intraday)\n\n**Primary key:** `(venue, symbol, tf, ts)`\n\nColumns:\n\n* `venue` (e.g., binance, bybit)\n* `symbol` (e.g., BTCUSDT)\n* `tf` (1m/5m/15m)\n* `ts` (bar end timestamp, UTC)\n* `open, high, low, close`\n* `volume` (base or quote; keep raw + standardized if needed)\n* `trades` (optional)\n* `vwap_bar` (optional if you have trade-level; otherwise omit)", "tags": []}
{"fragment_id": "F_R10_412_424", "source_id": "R10", "locator": "crpyto hypo.txt:L412-L424", "text": "## 2) `perp_mechanics` (mechanism primitives)\n\n**Primary key:** `(venue, symbol, ts)`\n\n* `funding_rate` (rate for upcoming interval; align correctly to decision time)\n* `funding_interval_start, funding_interval_end`\n* `mark_price` (at ts)\n* `index_price` (at ts)\n* `open_interest` (OI at ts)\n* `basis = mark_price - index_price` (or perp mid - spot mid; define explicitly)\n* `tick_size, contract_mult` (static; can go in ref table)\n* `fee_bps_maker, fee_bps_taker` (point-in-time if available)", "tags": []}
{"fragment_id": "F_R10_425_436", "source_id": "R10", "locator": "crpyto hypo.txt:L425-L436", "text": "## 3) `micro_liquidity` (if available; otherwise approximate)\n\n**Primary key:** `(venue, symbol, ts)`\n\n* `spread_bps` (mid-based)\n* `depth_1pct` / `depth_10bps` (optional)\n* `book_imbalance` (optional)\n\nIf you lack L2, approximate with:\n\n* `spread_proxy = high-low` at 1m, or rolling `|close-open|` etc. (but keep it clearly labeled as proxy).", "tags": []}
{"fragment_id": "F_R10_437_446", "source_id": "R10", "locator": "crpyto hypo.txt:L437-L446", "text": "## 4) `calendar` (time anchors / regimes)\n\nKey: `ts`\n\n* `dow`, `hour_utc`, `minute`\n* `weekend_flag` (Sat/Sun UTC or your chosen definition)\n* `anchor_flag` categorical: `{utc_00, utc_08, utc_13, us_cash_open, funding_settlement, ...}`\n* `holiday_flag` (optional)\n* `macro_news_flag` (optional; if you have an events calendar)", "tags": []}
{"fragment_id": "F_R10_447_454", "source_id": "R10", "locator": "crpyto hypo.txt:L447-L454", "text": "## 5) `features_pt` (point-in-time features at t₀)\n\n**Primary key:** `(venue, symbol, tf, ts)` where `ts` is the decision time index.\n\nThis is the “single pass” product: everything needed to define conditions.\n\nCore columns (minimal but sufficient):", "tags": []}
{"fragment_id": "F_R10_455_463", "source_id": "R10", "locator": "crpyto hypo.txt:L455-L463", "text": "### Volatility state\n\n* `atr_20` (computed from bars up to ts)\n* `atr_20_pct` (percentile vs trailing window, e.g., 60d same tf)\n* `rv_32` (realized vol over last 32 bars)\n* `rv_32_pct`\n* `compression_flag = (atr_20_pct ≤ q)` (store as bool for a few q, e.g., 0.20/0.25)\n* `compression_duration` (bars since entering compression_flag=1)", "tags": []}
{"fragment_id": "F_R10_464_470", "source_id": "R10", "locator": "crpyto hypo.txt:L464-L470", "text": "### Trend / persistence\n\n* `ret_1, ret_4, ret_16` (log returns trailing)\n* `trend_slope_32` (e.g., OLS slope of log price over 32 bars)\n* `trend_age` (bars since slope sign last flipped or since MA-cross regime; define explicitly)\n* `directional_streak` (consecutive same-sign returns/closes)", "tags": []}
{"fragment_id": "F_R10_471_477", "source_id": "R10", "locator": "crpyto hypo.txt:L471-L477", "text": "### Liquidity / mean reversion anchors\n\n* `vwap_session` (define session = rolling UTC day or rolling 24h; pick one)\n* `vwap_dist_atr = (close - vwap_session)/atr_20`\n* `prior_high`, `prior_low` (e.g., rolling 24h high/low or “previous UTC day”)\n* `equal_high_low_flags` (optional, can be later)", "tags": []}
{"fragment_id": "F_R10_478_491", "source_id": "R10", "locator": "crpyto hypo.txt:L478-L491", "text": "### Mechanism features\n\n* `funding_rate`\n* `funding_rate_pct` (percentile vs trailing)\n* `funding_sign`\n* `funding_sign_flip` (current sign != prior interval sign)\n* `oi`\n* `doi_1, doi_4, doi_16` (OI changes)\n* `doi_z` (standardized OI change)\n* `basis`\n* `basis_z`\n* `mark_index_gap = (mark-index)/index`\n* `mark_index_gap_z`", "tags": []}
{"fragment_id": "F_R10_492_496", "source_id": "R10", "locator": "crpyto hypo.txt:L492-L496", "text": "### Liquidity proxies\n\n* `spread_bps` (or proxy)\n* `spread_bps_pct`", "tags": []}
{"fragment_id": "F_R10_497_502", "source_id": "R10", "locator": "crpyto hypo.txt:L497-L502", "text": "### Calendar joins\n\n* `weekend_flag`\n* `anchor_flag`\n* `hour_utc`, `dow`", "tags": []}
{"fragment_id": "F_R10_503_526", "source_id": "R10", "locator": "crpyto hypo.txt:L503-L526", "text": "## 6) `forward_labels` (future-only labels, multiple horizons)\n\n**Primary key:** `(venue, symbol, tf, ts, horizon)`\n\nStore labels in long format to avoid wide tables.\n\nColumns:\n\n* `horizon` (e.g., 4, 8, 16, 32, 64 bars)\n* `ret_fwd` (log return close(ts+h)/close(ts))\n* `range_fwd = (max_high(ts+1..ts+h) - min_low(ts+1..ts+h))/close(ts)`\n* `rv_fwd` (realized vol over future window)\n* `vwap_touch` (whether price crossed vwap_session within horizon)\n* `drawdown_fwd`, `drawup_fwd` (optional)\n* `skew_proxy_fwd` (optional; usually computed at analysis time)\n\nThese labels support:\n\n* “NY range expansion” analogs (use anchor windows)\n* breakout magnitude (range_fwd)\n* reversion probability (vwap_touch)\n* sweep impact (ret_fwd / skew changes)\n* tail behavior (quantiles of range_fwd/rv_fwd)", "tags": []}
{"fragment_id": "F_R10_527_541", "source_id": "R10", "locator": "crpyto hypo.txt:L527-L541", "text": "## 7) `events` (thin event registry table)\n\n**Primary key:** `(venue, symbol, tf, ts, event_type, event_version)`\n\nColumns:\n\n* `event_type` (e.g., `compression_anchor`, `funding_extreme`, `sweep`, `liq_cascade`)\n* `event_version` (integer; freezes logic)\n* `event_strength` (continuous score if useful)\n* `meta_json` (optional: e.g., threshold used)\n\n**Important:** events are derived from `features_pt` only (PIT), never from forward labels.\n\n---", "tags": []}
{"fragment_id": "F_R10_542_568", "source_id": "R10", "locator": "crpyto hypo.txt:L542-L568", "text": "# B) Minimal event definitions covering the 4 “can it answer these” questions\n\nYou asked earlier whether research can answer:\n\n1. **(Crypto analog) Anchor window range percentile vs later expansion**\n   Replace “Asian vs NY” with **UTC window A vs anchor window B**.\n\n* Define `window_A = [00:00–08:00 UTC]` and `window_B = [13:00–16:00 UTC]` (or your preferred).\n* Add in `features_pt`:\n\n  * `range_A_pct` computed at end of window_A (PIT)\n  * `anchor_B_flag` at start of window_B\n* Label: `range_fwd` during window_B.\n\n2. **ATR compression duration vs breakout magnitude**\n\n* Already supported: `compression_duration` + `range_fwd` / `rv_fwd`.\n\n3. **Distance from VWAP vs reversion probability**\n\n* Supported: `vwap_dist_atr` + label `vwap_touch`.\n\n4. **Prior high/low sweep vs next 16-bar return**\n\n* Event: `sweep_flag` computed PIT from current bar vs `prior_high/low` and close location.\n* Label: `ret_fwd(h=16)`.", "tags": []}
{"fragment_id": "F_R10_569_575", "source_id": "R10", "locator": "crpyto hypo.txt:L569-L575", "text": "5. **Consecutive directional candles vs reversal probability**\n\n* Feature: `directional_streak`.\n* Label: reversal indicator (derive at analysis time): e.g., `sign(ret_fwd_8) != sign(ret_1)` or “hit opposite side of rolling midpoint”.\n\n---", "tags": []}
{"fragment_id": "F_R10_576_611", "source_id": "R10", "locator": "crpyto hypo.txt:L576-L611", "text": "# C) Concrete event registry (minimal set)\n\nDefine these event_types (all computed from `features_pt`):\n\n1. `compression_anchor`\n\n* `atr_20_pct ≤ 0.20 AND compression_duration ≥ 24 AND anchor_flag in {...}`\n\n2. `funding_extreme`\n\n* `abs(funding_rate_pct) ≥ 0.90`\n\n3. `funding_flip_oi_rise`\n\n* `funding_sign_flip=1 AND doi_z ≥ +1.5`\n\n4. `vwap_extreme`\n\n* `abs(vwap_dist_atr) ≥ 2.0`\n\n5. `prior_hl_sweep`\n\n* `high > prior_high + 0.25*atr_20 AND close < prior_high` OR symmetric for lows\n\n6. `directional_streak`\n\n* `directional_streak ≥ 5`\n\n7. `liq_cascade_proxy`\n\n* `abs(ret_1_z) ≥ 3 AND spread_bps_pct ≥ 0.90` (or other PIT proxy)\n\nThese 7 cover the majority of the 15 hypotheses; the rest are interactions (combine flags).\n\n---", "tags": []}
{"fragment_id": "F_R10_612_637", "source_id": "R10", "locator": "crpyto hypo.txt:L612-L637", "text": "# D) Build order (what to compute first)\n\n1. **Canonical joins and alignment**\n\n* Ensure `bars` and `perp_mechanics` align to the same `ts` grid per `tf`.\n* Funding alignment is the most common PIT bug: store funding as “rate applicable for interval (t_start,t_end)” and at `ts` use the rate known at `ts`.\n\n2. **Compute `features_pt`**\n\n* Start with ATR, RV, percentiles, compression clock.\n* Then funding/basis/OI percentiles + z-scores.\n* Then VWAP + vwap_dist_atr.\n* Then streak + sweep features.\n\n3. **Compute `forward_labels`**\n\n* For each horizon list, compute ret/range/rv/touch labels.\n* Store in long format.\n\n4. **Derive `events`**\n\n* Use only `features_pt`.\n* Version every event definition.\n\n---", "tags": []}
{"fragment_id": "F_R10_638_639", "source_id": "R10", "locator": "crpyto hypo.txt:L638-L639", "text": "# E) Example queries (analysis patterns)", "tags": []}
{"fragment_id": "F_R10_640_643", "source_id": "R10", "locator": "crpyto hypo.txt:L640-L643", "text": "### Event study: compression duration vs breakout magnitude\n\nFilter events, then compare `range_fwd` quantiles vs control.", "tags": []}
{"fragment_id": "F_R10_644_647", "source_id": "R10", "locator": "crpyto hypo.txt:L644-L647", "text": "### Reversion hazard: VWAP distance\n\nBin by `vwap_dist_atr` and compute `P(vwap_touch=1 | bin)`.", "tags": []}
{"fragment_id": "F_R10_648_653", "source_id": "R10", "locator": "crpyto hypo.txt:L648-L653", "text": "### Sweep impact: next 16-bar return\n\nCompare `ret_fwd(h=16)` distribution for `prior_hl_sweep` vs matched controls (same hour/dow, same atr_20_pct).\n\n---", "tags": []}
{"fragment_id": "F_R10_654_668", "source_id": "R10", "locator": "crpyto hypo.txt:L654-L668", "text": "# F) What “minimal” means in practice\n\nIf you implement only:\n\n* `bars`\n* `perp_mechanics`\n* `calendar`\n* `features_pt` (ATR/RV percentiles, compression clock, vwap_dist_atr, streak, prior_hl_sweep, funding/basis/OI z-scores, spread proxy)\n* `forward_labels` (ret/range/rv + vwap_touch)\n* `events`\n\n…then every hypothesis becomes: **filter rows → compare forward label distributions**.\n\n---", "tags": []}
{"fragment_id": "F_R10_669_677", "source_id": "R10", "locator": "crpyto hypo.txt:L669-L677", "text": "## Next step\n\nPick your **single canonical timeframe** (5m or 15m) and define:\n\n* trailing percentile window (e.g., 60 days)\n* horizons set (e.g., 8/16/32/64 bars)\n* anchor windows (UTC definitions)\n\nThen the first deliverable is a **schema+contract file** (column definitions + PIT rules + event versioning) that your pipeline can enforce with tests.", "tags": []}
{"fragment_id": "F_R11_1_2", "source_id": "R11", "locator": "hypothesis.md:L1-L2", "text": "# Conditional Market Physics Hypotheses for FX Majors and ES/NQ Futures", "tags": []}
{"fragment_id": "F_R11_3_10", "source_id": "R11", "locator": "hypothesis.md:L3-L10", "text": "## Framing and evidence base\n\nThe discoverable “conditional market physics” in liquid markets is primarily about **state-dependent changes in the distribution of future returns and volatility**, not about point forecasts. Two empirical constraints matter most:\n\nVolatility is **predictably time-varying (conditionally heteroskedastic)** and **clusters**: large moves tend to be followed by large moves and small by small, which is why volatility regimes exist and can be modeled rather than treated as noise. citeturn3search0turn3search5turn3search6\n\nAt intraday horizons, volatility is also **strongly periodic by time-of-day/session** and closely tied to market activity variables (volume/turnover, spreads/liquidity). This creates structurally different behavior across **Asia / Europe / US** windows and their overlaps, and it means any “regime” metric should be **time-of-day aware** (or at least controlled for). citeturn11view2turn4view0turn7view0turn12view1", "tags": []}
{"fragment_id": "F_R11_11_16", "source_id": "R11", "locator": "hypothesis.md:L11-L16", "text": "There is evidence that volatility shocks can **spill over across regional sessions within the same day** (“meteor showers” vs “heat waves”), which is exactly the kind of conditional cross-session dependency you want to exploit with variables like `asia_rv` → `london_rv` → `ny_rv`. citeturn5view0\n\nOrder-location and execution mechanics provide another high-signal driver: **stop-loss and take-profit orders cluster** (not randomly) and can create either **reversals at levels** (negative feedback) or **accelerations after crossings** (positive feedback cascades). This directly motivates “sweep → distribution shift” hypotheses. citeturn5view1turn0search2turn2search25\n\nAt very short horizons, **microstructure effects and resiliency** produce return reversals (e.g., bid–ask bounce and order-book replenishment after liquidity shocks), while at some intraday horizons **momentum-like dependence can appear**. This implies streak-based effects must be conditioned on regime and horizon (otherwise you average opposing forces). citeturn5view4turn5view3turn5view2", "tags": []}
{"fragment_id": "F_R11_17_18", "source_id": "R11", "locator": "hypothesis.md:L17-L18", "text": "Finally, **VWAP is a widely used benchmark in execution** and is embedded in institutional trading workflows; combining VWAP-distance with regime filters is a natural way to test for state-dependent mean reversion versus drift without importing “indicator logic.” citeturn8view1turn8view0turn4view4", "tags": []}
{"fragment_id": "F_R11_19_39", "source_id": "R11", "locator": "hypothesis.md:L19-L39", "text": "## Ranked testable hypotheses\n\n**Rank 1**\n\nBEHAVIOR: Volatility expansion tends to launch at liquidity arrival boundaries (session opens/overlaps) after multi-hour compression.\n\nCONDITION: `vol_state = compression` where `atr_20_pct ≤ 0.20` for ≥ `compression_dur ≥ 24` bars (use 5m bars as default), and `session_transition_flag = 1` (entering entity[\"city\",\"London\",\"uk\"] or entity[\"city\",\"New York City\",\"ny, us\"] session; or LN-overlap starts).\n\nMEASUREMENT: Shift in forward distribution of `fwd_range_32` and `fwd_rv_32` (variance and upper-tail quantiles) vs time-of-day matched controls.\n\nHORIZON: Next 32 bars or until end of the next session block (whichever first); evaluate separately for “into London” vs “into NY” transitions.\n\nEXPECTED EFFECT: Volatility change (↑) and fatter right tail of range (more large expansions).\n\nVARIABLES (minimal set):  \n- `atr_20` = ATR over 20 bars; `atr_20_pct` = rolling percentile vs last 60 trading days (same time-of-day bin).  \n- `compression_dur` = consecutive bars with `atr_20_pct ≤ 0.20`.  \n- `session_transition_flag` = 1 at first K bars of a session boundary/overlap.  \n- `fwd_range_32` = max(high) − min(low) over next 32 bars.  \n- `fwd_rv_32` = sqrt(sum(r²) next 32 bars).", "tags": []}
{"fragment_id": "F_R11_40_52", "source_id": "R11", "locator": "hypothesis.md:L40-L52", "text": "TEST DESIGN: Event study keyed on first bar of `session_transition_flag` after a compression run; control group = same clock-time bars on days with `atr_20_pct` in [0.40, 0.60] and same day-of-week; report full conditional distributions (median, IQR, 90/95/99th percentiles), not a single mean.\n\nFAILURE MODES / CONFOUNDS: Macro releases clustered at session starts; holiday-thinned liquidity; futures roll/contract change; FX “witching hour” thin liquidity causing outlier ranges; bar aggregation artifacts.\n\nNEXT STEP: Implement time-of-day-binned percentiles for `atr_20` and compute `compression_dur` + `fwd_range_32` distributions by session transition. citeturn11view2turn4view0turn7view0turn3search0turn4view5\n\n\n**Rank 2**\n\nBEHAVIOR: Breakout magnitude increases with compression *persistence* (hazard + tail dependence), not just compression depth.\n\nCONDITION: Define “compression episode” as continuous segment with `atr_20_pct ≤ 0.25`; bucket episodes by `compression_dur` quantiles (e.g., Q1/Q2/Q3/Q4). Breakout begins when price first exits `compression_range_hi/lo`.", "tags": []}
{"fragment_id": "F_R11_53_67", "source_id": "R11", "locator": "hypothesis.md:L53-L67", "text": "MEASUREMENT: `breakout_mag_max` (max excursion beyond boundary) and `time_to_breakout` (hazard) conditional on episode duration; compare tail quantiles across duration buckets.\n\nHORIZON: From breakout trigger until min(64 bars, next session boundary).\n\nEXPECTED EFFECT: Expansion (↑) in upper-tail of `breakout_mag_max` as `compression_dur` increases; hazard of breakout may increase around session boundaries even if compression persists.\n\nVARIABLES (minimal set):  \n- `compression_episode_id`, `compression_dur`.  \n- `compression_range_hi/lo` = hi/lo over the episode.  \n- `breakout_side` ∈ {up, down}.  \n- `breakout_mag_max` = max(|price − boundary|) over horizon.  \n- `time_to_breakout` = bars from episode start to first boundary breach.\n\nTEST DESIGN: Survival/hazard model with covariates {minute-of-day, day-of-week, session flag}; control for intraday periodicity by stratification; report conditional hazard curves + magnitude distributions by duration bucket.", "tags": []}
{"fragment_id": "F_R11_68_82", "source_id": "R11", "locator": "hypothesis.md:L68-L82", "text": "FAILURE MODES / CONFOUNDS: Duration is endogenous to time-of-day (Asia often quieter); volatility clustering implies episodes persist (don’t confuse persistence with “stored energy”); lookahead in defining episode end; stale FX volume proxies.\n\nNEXT STEP: Build a compression episode extractor and compute duration-binned `breakout_mag_max` distributions with time-of-day stratification. citeturn3search6turn11view2turn4view5\n\n\n**Rank 3**\n\nBEHAVIOR: Lower entity[\"city\",\"Tokyo\",\"japan\"]/Asia range days are followed by larger NY expansions (conditional on session structure).\n\nCONDITION: `asia_range_pct ≤ 0.30` (Asia session range percentile vs trailing 90 days); evaluate separately by instrument (FX majors; ES/NQ globex vs RTH).\n\nMEASUREMENT: Shift in NY session `range_first_hour`, `rv_first_hour`, and `tail(range_first_hour)` vs days with `asia_range_pct` in [0.45, 0.55].\n\nHORIZON: NY first 60 minutes (or first 12×5m bars), plus a secondary horizon “full NY session”.", "tags": []}
{"fragment_id": "F_R11_83_100", "source_id": "R11", "locator": "hypothesis.md:L83-L100", "text": "EXPECTED EFFECT: Expansion (↑) in variance and right-tail of early-NY range on low-Asia-range days.\n\nVARIABLES (minimal set):  \n- `asia_range` = Hi−Lo in Asia session; `asia_range_pct` = rolling percentile.  \n- `ny_open_flag` = first K bars after NY session start.  \n- `range_first_hour`, `rv_first_hour`.\n\nTEST DESIGN: Conditional distribution comparison with matched controls on day-of-week and (for futures) prior day realized vol; cluster-robust SE by day; report effect sizes as distribution deltas (e.g., ΔP90, Δmedian).\n\nFAILURE MODES / CONFOUNDS: FX “thin” periods (late NY/early Asia) distort “range percentile” baseline; DST session definitions; Wednesday/Friday effects; major data releases in early NY.\n\nNEXT STEP: Compute `asia_range_pct` and NY first-hour range distributions; add a variant controlling for pre-NY European drift. citeturn7view0turn4view0turn11view2\n\n\n**Rank 4**\n\nBEHAVIOR: London-open displacement escalates when preceding Asia is in a low-vol regime.", "tags": []}
{"fragment_id": "F_R11_101_118", "source_id": "R11", "locator": "hypothesis.md:L101-L118", "text": "CONDITION: `asia_rv_pct ≤ 0.20` where `asia_rv` is realized vol over Asia session; event aligned to London open window (first 30 minutes).\n\nMEASUREMENT: Distribution shift in `london_open_range_30m` and `london_open_netret_30m`, plus persistence into next hour (skew of `fwd_ret_12`).\n\nHORIZON: First 30 minutes after London open; secondary horizon next 60 minutes.\n\nEXPECTED EFFECT: Expansion (↑) in range and a more skewed return distribution (stronger displacement tails).\n\nVARIABLES (minimal set):  \n- `asia_rv`, `asia_rv_pct`.  \n- `london_open_flag`.  \n- `london_open_range_30m`, `london_open_netret_30m`.  \n- `fwd_ret_12`.\n\nTEST DESIGN: Event study centered on London open; control group = same clock-time with `asia_rv_pct` in [0.40, 0.60]; include robustness with “exclude major scheduled macro minutes” proxy windows.\n\nFAILURE MODES / CONFOUNDS: London-open overlaps with specific fix/flows; FX liquidity providers’ behavior changes in stress; futures may have different “open” definitions (cash vs globex).", "tags": []}
{"fragment_id": "F_R11_119_135", "source_id": "R11", "locator": "hypothesis.md:L119-L135", "text": "NEXT STEP: Build `asia_rv_pct` and measure London-open 30m distributions conditional on Asia vol regime. citeturn11view2turn4view0turn4view5\n\n\n**Rank 5**\n\nBEHAVIOR: Sweep-and-reject versus sweep-and-hold flips the post-event return distribution (reversion vs drift).\n\nCONDITION: Define `prior_level` = prior session high/low (or overnight high/low). A “sweep” occurs when intrabar high exceeds `prior_high` by ≥ `ε` (or low breaks `prior_low`). Classify outcome:  \n- `sweep_reject = 1` if price closes back inside the level range within ≤ 3 bars.  \n- `sweep_hold = 1` if close remains beyond level for ≥ 3 bars.\n\nMEASUREMENT: Next 16-bar return distribution (`ret_fwd_16`), plus asymmetry (skew) and tail probabilities conditional on reject vs hold.\n\nHORIZON: 16 bars after classification point; also evaluate to next session boundary.\n\nEXPECTED EFFECT: Reversion after reject; drift/continuation after hold (distribution mean shifts sign-consistent with break).", "tags": []}
{"fragment_id": "F_R11_136_153", "source_id": "R11", "locator": "hypothesis.md:L136-L153", "text": "VARIABLES (minimal set):  \n- `prior_high`, `prior_low`.  \n- `sweep_side`, `sweep_reject`, `sweep_hold`.  \n- `ret_fwd_16`, `mfe_16`, `mae_16`.\n\nTEST DESIGN: Two-event cohorts (reject vs hold) with matched controls “touch but no break”; use conditional density plots for `ret_fwd_16`; validate robustness across instruments/timezones.\n\nFAILURE MODES / CONFOUNDS: Bar-resolution hides microstructure (false sweeps); spread/quote spikes in FX; news shocks that blow through any level; futures session gaps and roll.\n\nNEXT STEP: Implement sweep detection + reject/hold classifier; compute `ret_fwd_16` distributions for each class and compare to “touch-no-break” controls. citeturn5view1turn0search2turn5view4\n\n\n**Rank 6**\n\nBEHAVIOR: Candle-streak behavior bifurcates by volatility state (short-horizon reversal in compression vs continuation in expansion).\n\nCONDITION: `streak_len ≥ 5` (consecutive returns same sign) AND classify `vol_state` by `atr_20_pct` (compression ≤0.25, expansion ≥0.75).", "tags": []}
{"fragment_id": "F_R11_154_170", "source_id": "R11", "locator": "hypothesis.md:L154-L170", "text": "MEASUREMENT: `P(reversal_next_k)` and distribution of `ret_fwd_k` for k ∈ {1, 4, 8} bars; compare streak events across `vol_state`.\n\nHORIZON: Next 1–8 bars after streak completes.\n\nEXPECTED EFFECT: Reversion probability ↑ in compression; drift/continuation probability ↑ in expansion (state-dependent sign of autocorrelation).\n\nVARIABLES (minimal set):  \n- `streak_len`, `streak_dir`.  \n- `atr_20_pct` → `vol_state`.  \n- `ret_fwd_1`, `ret_fwd_4`, `ret_fwd_8`, `reversal_flag_k`.\n\nTEST DESIGN: Conditional distribution by {streak_len bucket, vol_state}; control group = randomly sampled non-streak bars matched by time-of-day; report effect as Δhit-rate and Δmedian return.\n\nFAILURE MODES / CONFOUNDS: Bid–ask bounce creates artificial reversal at very small bars; bar construction differences (mid vs last); high-impact news creates long streaks with true drift.\n\nNEXT STEP: Build streak detector and conditionalize by `atr_20_pct`; start with 5m FX and 1m futures separately to isolate microstructure artifacts. citeturn5view3turn5view4turn3search0", "tags": []}
{"fragment_id": "F_R11_171_190", "source_id": "R11", "locator": "hypothesis.md:L171-L190", "text": "**Rank 7**\n\nBEHAVIOR: Post-expansion exhaustion (volatility reset) after extreme realized volatility.\n\nCONDITION: `rv_60m_pct ≥ 0.90` (realized vol percentile over last 60 minutes) AND expansion event not in first/last 15 minutes of a major session (avoid “open/close” structural bursts).\n\nMEASUREMENT: Forward change in volatility: `Δrv = rv_fwd_60m − rv_prev_60m`, plus contraction of forward range tails.\n\nHORIZON: Next 60 minutes; secondary = next session block.\n\nEXPECTED EFFECT: Volatility change (↓): mean reversion in realized volatility and thinner forward range distribution after an extreme.\n\nVARIABLES (minimal set):  \n- `rv_prev_60m`, `rv_60m_pct`.  \n- `rv_fwd_60m`, `fwd_range_12` (if 5m).  \n- `session_edge_excl_flag`.\n\nTEST DESIGN: Conditional expectation and conditional quantiles; strong controls for time-of-day periodicity; compare to medium-vol baseline `rv_60m_pct` in [0.45, 0.55].", "tags": []}
{"fragment_id": "F_R11_191_207", "source_id": "R11", "locator": "hypothesis.md:L191-L207", "text": "FAILURE MODES / CONFOUNDS: Volatility clustering can keep vol high (no reset) on crisis/news days; regime shifts; futures cash-close effects.\n\nNEXT STEP: Compute realized vol percentiles with time-of-day bins; test Δrv distributions after rv extremes vs baseline. citeturn3search6turn4view5turn11view2\n\n\n**Rank 8**\n\nBEHAVIOR: Cross-session volatility spillover is stronger than same-session persistence at intraday granularity.\n\nCONDITION: For each day, compute `asia_rv`, `london_rv`, `ny_rv`. Identify high-shock days where `asia_rv_pct ≥ 0.80` (or London shock). Test whether `ny_rv` shifts conditional on upstream session shocks.\n\nMEASUREMENT: Distribution shift in `ny_rv` (variance and tail quantiles) conditional on earlier-session `rv` shocks; compare to a “heat-wave” baseline using only NY lagged shocks.\n\nHORIZON: Same-day next session.\n\nEXPECTED EFFECT: Volatility change (↑) in downstream session metrics when upstream `rv` shocks are high.", "tags": []}
{"fragment_id": "F_R11_208_224", "source_id": "R11", "locator": "hypothesis.md:L208-L224", "text": "VARIABLES (minimal set):  \n- `asia_rv`, `london_rv`, `ny_rv` and percentiles.  \n- `ny_rv` conditional model inputs.\n\nTEST DESIGN: Variance causality-style test using conditional variance regressions (or simpler: stratified conditional distributions); control = time-of-day and day-of-week; evaluate separately for FX vs futures.\n\nFAILURE MODES / CONFOUNDS: Session definitions differ by instrument; macro events propagate globally and may be the real driver; volatility seasonality can mimic spillover if not controlled.\n\nNEXT STEP: Build per-session realized vol series and run stratified conditional comparisons `ny_rv | asia_rv_pct`. citeturn5view0turn11view2turn4view5\n\n\n**Rank 9**\n\nBEHAVIOR: Short-lived volatility spikes cluster (elevated “vol-of-vol”) after an initial spike.\n\nCONDITION: Detect spike when `|r_t| ≥ 3 * rv_20` (or `range_t ≥ 3 * atr_20`). Optionally restrict to “known spike times” proxy windows (e.g., top-of-hour, major open windows).", "tags": []}
{"fragment_id": "F_R11_225_245", "source_id": "R11", "locator": "hypothesis.md:L225-L245", "text": "MEASUREMENT: Forward realized vol `rv_fwd_15m` and spike recurrence probability `P(spike in next 3–6 bars)`.\n\nHORIZON: Next 15–30 minutes (3–6×5m or 15–30×1m).\n\nEXPECTED EFFECT: Volatility change (↑): conditional elevation of short-horizon realized vol and higher spike recurrence.\n\nVARIABLES (minimal set):  \n- `r_t`, `rv_20`, `atr_20`, `spike_flag`.  \n- `rv_fwd_15m`, `spike_recur_flag`.\n\nTEST DESIGN: Event study; control group = bars with large `rv_20` but no spike; compare spike recurrence and forward rv.\n\nFAILURE MODES / CONFOUNDS: Data errors/outliers; spread blowouts; limit moves in futures; aggregation hides true spike start.\n\nNEXT STEP: Implement spike detector and compute forward `rv` and recurrence conditional on spike vs no-spike baselines. citeturn3search0turn12view1turn12view0\n\n\n**Rank 10**\n\nBEHAVIOR: NY open continuation vs reversal depends on pre-NY drift and Asia range tightness.", "tags": []}
{"fragment_id": "F_R11_246_262", "source_id": "R11", "locator": "hypothesis.md:L246-L262", "text": "CONDITION: `pre_ny_drift_z` = (return from London open to NY open) standardized by realized vol; bucket into strong drift (|z| ≥ 1.5) vs weak drift (|z| ≤ 0.5). Cross with `asia_range_pct ≤ 0.30` vs ≥ 0.70.\n\nMEASUREMENT: Conditional distribution of first-hour NY return (`ret_ny_1h`) and reversal hit-rate (`sign(ret_ny_1h) = −sign(pre_ny_drift)`).\n\nHORIZON: NY first 60 minutes.\n\nEXPECTED EFFECT: In strong pre-NY drift + tight Asia, continuation probability ↑ (skew toward drift direction); in weak drift or wide Asia, reversal probability ↑.\n\nVARIABLES (minimal set):  \n- `pre_ny_drift`, `pre_ny_drift_z`.  \n- `asia_range_pct`.  \n- `ret_ny_1h`, `reversal_flag`.\n\nTEST DESIGN: 2×2 conditional distribution study with matched controls on day-of-week; robustness: exclude first 15 minutes if macro spike proxy is active.\n\nFAILURE MODES / CONFOUNDS: Macro releases at/near NY open; DST boundaries; futures transitioning between globex/cash liquidity regimes.", "tags": []}
{"fragment_id": "F_R11_263_282", "source_id": "R11", "locator": "hypothesis.md:L263-L282", "text": "NEXT STEP: Compute `pre_ny_drift_z` and run 2×2 conditional distributions for `ret_ny_1h`. citeturn7view0turn4view0turn11view2\n\n\n**Rank 11**\n\nBEHAVIOR: VWAP-distance reversion probability rises with standardized deviation in non-trending regimes.\n\nCONDITION: Use running VWAP (no lookahead): `vwap_run_t`. Define standardized deviation `vwap_z = (close_t − vwap_run_t) / atr_20`. Require `|vwap_z| ≥ 2.5` AND `trend_strength ≤ 0.30` (trend strength = |slope| percentile over recent window).\n\nMEASUREMENT: `P(hit_vwap within H)` and distribution of `time_to_vwap` + `ret_fwd_H` skew vs baseline `|vwap_z| ≤ 0.5`.\n\nHORIZON: H = 24 bars (2 hours on 5m) or until session end.\n\nEXPECTED EFFECT: Reversion (↑): higher VWAP-hit probability and negative drift back toward VWAP in non-trending regimes.\n\nVARIABLES (minimal set):  \n- `vwap_run_t`, `vwap_z`.  \n- `trend_strength` (e.g., |linear regression slope| standardized).  \n- `hit_vwap_flag_H`, `time_to_vwap`, `ret_fwd_H`.", "tags": []}
{"fragment_id": "F_R11_283_295", "source_id": "R11", "locator": "hypothesis.md:L283-L295", "text": "TEST DESIGN: Event study at first bar where condition becomes true; control group matched on time-of-day with small `vwap_z`; ensure VWAP uses only volume/price up to t.\n\nFAILURE MODES / CONFOUNDS: VWAP is path-dependent and mechanically “chases” price early in session; futures volume spikes at open distort VWAP; FX volume is proxy; strong trend regimes overwhelm reversion.\n\nNEXT STEP: Implement running VWAP + strict no-lookahead checks; start with the conditional hit-rate `P(hit_vwap in 24)` by `vwap_z` bins and `trend_strength` bins. citeturn8view1turn8view0turn4view4turn5view4\n\n\n**Rank 12**\n\nBEHAVIOR: Intraday time-series momentum: early-session return predicts late-session return (within-day persistence).\n\nCONDITION: Define “early window” (first 30 minutes of primary session) return `ret_open_30m`; classify “strong” if |z| ≥ 1 where z is standardized by recent intraday rv. Apply separately to futures cash session and major FX liquidity windows.", "tags": []}
{"fragment_id": "F_R11_296_311", "source_id": "R11", "locator": "hypothesis.md:L296-L311", "text": "MEASUREMENT: Conditional mean and hit-rate of “late window” return `ret_close_30m`, plus change in tail probability of same-direction moves.\n\nHORIZON: Same session day: open+30m → close−30m.\n\nEXPECTED EFFECT: Drift (↑): distribution mean of late return shifts toward early return sign; increased same-direction probability.\n\nVARIABLES (minimal set):  \n- `ret_open_30m`, `ret_close_30m`, `open_z`.  \n- `session_open_flag`, `session_close_flag`.\n\nTEST DESIGN: Same-day paired windows; control group = days with |open_z| ≤ 0.25; report distributions of `ret_close_30m` conditional on `sign(ret_open_30m)`.\n\nFAILURE MODES / CONFOUNDS: Futures close auction mechanics; overlapping window contamination; “open” definition differs by instrument; macro releases inside either window.\n\nNEXT STEP: Implement open/close window slicing; compute conditional `E[ret_close_30m | open_z bin]` and hit-rates across instruments. citeturn5view2turn11view2turn12view1", "tags": []}
{"fragment_id": "F_R11_312_330", "source_id": "R11", "locator": "hypothesis.md:L312-L330", "text": "**Rank 13**\n\nBEHAVIOR: Equal-high/low clustering creates discontinuous behavior at breaks (fast acceleration after crossing).\n\nCONDITION: Detect `equal_high_cluster` when ≥2 swing highs within last 120 bars are within `δ = 0.1 * atr_20` of each other (analogous for lows). Trigger when price crosses cluster level by ≥ ε.\n\nMEASUREMENT: Forward “speed” metrics: `range_fwd_8 / atr_20` and probability of a one-sided move (skew/tail) immediately after crossing vs random non-cluster highs.\n\nHORIZON: Next 8 bars after first decisive cross.\n\nEXPECTED EFFECT: Expansion (↑): fatter right tail in short-horizon range; increased probability of fast continuation after level cross.\n\nVARIABLES (minimal set):  \n- `equal_high_cluster_flag`, `cluster_level`.  \n- `cross_flag`, `range_fwd_8`.\n\nTEST DESIGN: Event study contrasting cluster-cross events vs “ordinary swing-high cross” events matched on volatility percentile and time-of-day; use tail quantile differences (ΔP95).", "tags": []}
{"fragment_id": "F_R11_331_343", "source_id": "R11", "locator": "hypothesis.md:L331-L343", "text": "FAILURE MODES / CONFOUNDS: Cluster detection depends on swing definition; spread spikes create false highs; FX round-number effects may dominate cluster logic without explicit round-number controls.\n\nNEXT STEP: Build a robust swing-point extractor + cluster detector; validate cluster frequency stability across bar sizes before testing post-cross distributions. citeturn5view1turn2search25turn11view2\n\n\n**Rank 14**\n\nBEHAVIOR: Overnight high/low sweep near primary cash open changes the return distribution (volatility burst + directional bias conditional on hold/reject).\n\nCONDITION: For futures, define `overnight_high/low` over globex overnight window. If within first 30 minutes of cash open (`cash_open_flag=1`) price breaches overnight high/low by ≥ ε, classify reject vs hold (same logic as Rank 5).\n\nMEASUREMENT: `rv_next_30m` and `ret_fwd_12` distributions conditional on breach type; compare to “touch but no breach” controls.", "tags": []}
{"fragment_id": "F_R11_344_368", "source_id": "R11", "locator": "hypothesis.md:L344-L368", "text": "HORIZON: Next 30–60 minutes from breach.\n\nEXPECTED EFFECT: Volatility change (↑) regardless; drift vs reversion depends on hold vs reject classification.\n\nVARIABLES (minimal set):  \n- `overnight_high/low`, `cash_open_flag`.  \n- `sweep_hold`, `sweep_reject`.  \n- `rv_next_30m`, `ret_fwd_12`.\n\nTEST DESIGN: Event study with strict open-window alignment; controls matched on day-of-week and prior-day volatility.\n\nFAILURE MODES / CONFOUNDS: Contract rolls; price limits; cash open definition changes; major scheduled announcements at/near open.\n\nNEXT STEP: Implement overnight window definitions and open-window breach detector; start with variance comparison `rv_next_30m` for breach vs no-breach controls. citeturn12view1turn10view1turn4view0\n\n\n**Rank 15**\n\nBEHAVIOR: Compound regime interaction: compression + session open + level sweep produces the strongest expansion tails.\n\nCONDITION: All true:  \n1) `atr_20_pct ≤ 0.20` for `compression_dur ≥ 24` bars ending within last 2 hours,  \n2) `session_open_flag = 1` (first 30 minutes of London/NY),  \n3) `sweep_hold = 1` on prior session high/low (or overnight level).", "tags": []}
{"fragment_id": "F_R11_369_386", "source_id": "R11", "locator": "hypothesis.md:L369-L386", "text": "MEASUREMENT: Upper-tail quantiles of `fwd_range_16` and sign-consistent drift probability (hit-rate that returns stay in sweep direction).\n\nHORIZON: Next 16 bars or until end of the opening window.\n\nEXPECTED EFFECT: Expansion (↑) with extreme right-tail amplification; drift (↑) in sweep direction conditional on hold.\n\nVARIABLES (minimal set):  \n- `atr_20_pct`, `compression_dur`.  \n- `session_open_flag`.  \n- `sweep_hold`, `prior_level`.  \n- `fwd_range_16`, `ret_fwd_16`.\n\nTEST DESIGN: Interaction study using 2×2×2 stratification; isolate incremental effect of the 3-way interaction via matched controls (e.g., compression+open without sweep, sweep+open without compression); report tail deltas.\n\nFAILURE MODES / CONFOUNDS: Opens coincide with macro releases; definition drift across DST; sweep detection sensitivity to bar size; FX liquidity fragmentation.\n\nNEXT STEP: Implement the three condition flags and compute `fwd_range_16` tail quantiles across all interaction cells; verify that each cell has sufficient sample size. citeturn11view2turn4view0turn5view1turn0search2", "tags": []}
{"fragment_id": "F_R11_387_387", "source_id": "R11", "locator": "hypothesis.md:L387-L387", "text": "", "tags": []}
{"fragment_id": "F_R11_388_406", "source_id": "R11", "locator": "hypothesis.md:L388-L406", "text": "## Variable glossary\n\n| Variable | Definition (backtestable) | Notes to avoid lookahead / leakage |\n|---|---|---|\n| `r_t` | Log return: ln(close_t / close_{t-1}) | Use consistent session reset rules |\n| `range_t` | High_t − Low_t | Sensitive to bad ticks |\n| `tr_t` | True Range = max(high−low, |high−prev_close|, |low−prev_close|) | Futures gaps make TR essential |\n| `atr_n` | Wilder-style ATR over n bars (e.g., n=20) | Use only past TR values citeturn6search0 |\n| `atr_n_pct` | Rolling percentile of `atr_n` (recommend 60–90 days), stratified by time-of-day bin | Controls intraday periodicity |\n| `rv_w` | Realized vol over window w: sqrt(sum_{i=1..w} r_{t-i+1}²) | Standard for intraday vol proxies citeturn4view5 |\n| `rv_w_pct` | Rolling percentile of `rv_w` | Use same clock-time bins where possible |\n| `vol_state` | Discrete: compression (≤0.25), neutral, expansion (≥0.75) based on `atr_n_pct` | Keep thresholds fixed across tests |\n| `compression_flag` | 1 if `atr_n_pct ≤ θ` (e.g., θ=0.20) | Define θ once per study |\n| `compression_dur` | Consecutive bars with `compression_flag=1` | Episode extraction must be causal |\n| `compression_range_hi/lo` | Max high / min low over the compression episode | Do not “peek” past episode end |\n| `breakout_side` | Up if price first breaches hi, down if breaches lo | Use first breach only |\n| `breakout_mag_max` | Max excursion beyond boundary over horizon | Horizon must be fixed ex ante |\n| `fwd_range_N` | Forward range over next N bars | Strictly forward, no overlap in controls |\n| `fwd_rv_N` | Forward realized vol over next N bars | Same N across all samples |", "tags": []}
{"fragment_id": "F_R11_407_423", "source_id": "R11", "locator": "hypothesis.md:L407-L423", "text": "| `session_label` | {Asia, London, NY, overlap} from calendar | Must handle DST consistently |\n| `session_transition_flag` | 1 on first K bars after a session boundary/overlap start | K fixed (e.g., 6×5m) |\n| `session_open_flag` | 1 on first 30 minutes of a session | For futures: specify cash vs globex |\n| `ny_open_flag` | 1 on first 60 minutes of NY session | Separate from overlap if needed |\n| `asia_range` | Hi−Lo over Asia session | FX: define Asia window via UTC |\n| `asia_range_pct` | Rolling percentile of `asia_range` | Compare only to same instrument |\n| `pre_ny_drift` | Return from London open to NY open | Isolates \"Europe drift\" |\n| `pre_ny_drift_z` | `pre_ny_drift` standardized by realized vol in that window | Stabilizes across regimes |\n| `vwap_run_t` | Running VWAP from session start to t: sum(price×vol)/sum(vol) using data ≤ t | Never use full-day VWAP citeturn8view1 |\n| `vwap_z` | (close_t − vwap_run_t) / atr_n | Use regime filter to avoid trend bleed |\n| `prior_high/low` | Prior session (or day) high/low | Define prior window precisely |\n| `overnight_high/low` | Futures: globex overnight high/low | Requires consistent overnight window |\n| `sweep_flag` | 1 if price trades beyond prior level by ≥ ε | ε scaled by ATR improves stability |\n| `sweep_outcome` | {reject, hold} based on closes relative to level over next m bars | Classification must be causal |\n| `streak_len` | Consecutive same-sign returns length | Define sign using close-to-close |\n| `streak_dir` | Sign of the streak (+1 / −1) | Handle zeros explicitly |", "tags": []}
{"fragment_id": "F_R11_424_442", "source_id": "R11", "locator": "hypothesis.md:L424-L442", "text": "## Priority scoreboard\n\n| Rank | Hypothesis (short name) | Edge strength (1–5) | Universality (1–5) | Ease of testing (1–5) | Product |\n|---:|---|---:|---:|---:|---:|\n| 1 | Compression → expansion at session transition | 5 | 5 | 4 | 100 |\n| 2 | Compression duration → breakout magnitude tails | 4 | 5 | 4 | 80 |\n| 3 | Asia range pct → NY first-hour expansion | 4 | 5 | 4 | 80 |\n| 4 | London open displacement conditional on Asia low vol | 4 | 5 | 4 | 80 |\n| 5 | Sweep reject vs hold → reversion vs drift | 4 | 5 | 4 | 80 |\n| 6 | Streak effects bifurcate by vol state | 3 | 5 | 5 | 75 |\n| 7 | Post-expansion exhaustion / volatility reset | 4 | 4 | 4 | 64 |\n| 8 | VWAP distance reversion conditional on regime | 3 | 5 | 4 | 60 |\n| 9 | Cross-session volatility spillover | 4 | 4 | 3 | 48 |\n| 10 | NY continuation vs reversal (pre-NY drift × Asia range) | 3 | 4 | 4 | 48 |\n| 11 | Intraday open→close momentum persistence | 3 | 4 | 4 | 48 |\n| 12 | Compression+open+sweep interaction tail amplifier | 4 | 4 | 3 | 48 |\n| 13 | Spike clustering (short-lived vol-of-vol) | 3 | 4 | 3 | 36 |\n| 14 | Equal-high/low cluster break acceleration | 3 | 4 | 3 | 36 |\n| 15 | Overnight level sweep near cash open | 3 | 4 | 3 | 36 |", "tags": []}
{"fragment_id": "F_R12_p1_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p1:1", "text": "1 \n \n \n \n \nMicrostructure and Market Dynamics in Crypto Markets \n \nDavid Easley, Maureen O’Hara, Songshan Yang , and Zhibai Zhang* \nApril, 2024 \n \nWe investigate the role of market microstructure metrics in explaining and predicting price \ndynamics for 5 major cryptocurrencies. Using machine learning, we show how microstructure \nmeasures of liquidity and price discovery have predictive power for price dynamics of interest \nfor electronic market making, dynamic hedging strategies and volatility estimation. We identify \nimportant own market and cross-market effects for BTC and ETH Roll measures and VPINs. \nOur results are little changed during crypto winter, demonstrating a stability to these effects. Our \nfindings suggest that market dynamics of cryptocurrencies can be viewed as similar to those of \nother investible asset classes.\n \n \n \n \n \n \n *David Easley, Departments of Economics and Information Science, Cornell University; \nMaureen O’Hara, Johnson College of Business, Cornell University; Songshan Yang, Center for \nApplied Statistics and Institute of Statistics and Big Data, Renmin University of China; Zhibai \nZhang, the Tandon School of Engineering, New York University \n \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p2_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p2:1", "text": "2 \n \nMicrostructure and Market Dynamics in Crypto Markets \nThe challenge confronting cryptocurrencies has long been apparent: How to make crypto an \nasset class investible by retail and institutional traders? The resurgence of crypto prices suggests \nthat some impediments to reaching this goal are receding, at least as they apply to retail investors. \nFollowing the January 2024 SEC approval of spot-market based Bitcoin ETFs, inflows to the new \nBitcoin ETFs reached almost $70 billion in just 2 months.\n1 This remarkable growth testifies to \nthe appeal that being able to buy bitcoin exposure through brokerage accounts r ather than via \ncrypto exchanges or futures markets has for retail traders. But other obstacles remain for both retail \nand institutional investors.\n2 Whether these difficulties can be overcome remains to be seen, but \ncertainly fundamental to greater particip ation is the ability to understand what drives the market \ndynamics of cryptocurrencies. \nUnderstanding these market dynamics, however, is not straightforward. As we discuss \nbelow, there is a developing literature attempting to understand crypto valuation drawing on \nstandard tools from other asset classes. Other approaches focus on crypto-specific factors such as \nunderlying users or mining costs to explain crypto valuations. Yet other researchers eschew", "tags": []}
{"fragment_id": "F_R12_p2_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p2:2", "text": "understanding the determinants of crypto value and instead turn to more “black box” machine \nlearning to simply predict future prices. While these approaches yield interesting insights, accurate \ncrypto valuation continues to prove elusive, underscoring the difficulty of making crypto more \n \n1 AUM numbers as of March 28, 2024 from https://cryptonews.com/news/assets-invested-in-crypto-etfs-and-etps-\nrise-359-in \n2024.htm#:~:text=ETFGI%2C%20an%20independent%20research%20firm%2C%20reports%20assets%20invested,\nincrease%20from%20%2415.12%20billion%20at%20end%20of%202023. See also “Bitcoin ETFs on Better Win \nStreak than 95% of Traditional Funds,” Forbes, March 25, 2024. \n2 Vanguard, for example, will not offer crypto-products arguing bitcoin is “a speculation [rather] than an \ninvestment”. JP Morgan and Goldman Sachs have expressed similar views. Institutional roadblocks include \nregulatory issues (e.g. is crypto a security?, know your customer compliance), market manipulation concerns, \nvaluation issues, market fragmentation, excessive volatility, and lack of liquidity. For discussion, see \nblog.ionixxtech.com/top-5-challenges-institutional-investors-face-in-crypto-trading/. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p3_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p3:1", "text": "3 \n \naccessible to a w ider investing audience who, understandably, want to know whether the crypto \nasset they are considering buying is over- or under-valued. \nIn this paper, we offer a different approach to investigating crypto market behavior. Our \nparticular focus is on a bas ic question: Can standard microstructure measures predict crypto \nmarket dynamics? Unlike valuation-based analyses, our approach is based more on understanding \nthe liquidity and price discovery process involved in crypto trading. Microstructure theory \nprovides various measures related to liquidity (for example, the Amihud measure), asymmetric \ninformation and toxicity (Kyle lambda, VPIN), and overall spreads and auto- correlations (Roll \nmeasure and Roll Impact measure) that have been shown to matter for liquidity and price dynamics \nin other asset markets. We estimate these variables for 5 major cryptocurrencies, giving insight \ninto how these metrics differ from those found in more standard market settings. \nWe then use these measures to predict 5 outcomes of market price dynamics of particular \ninterest for electronic market making, dynamic hedging strategies and volatility estimation. These \noutcome variables are the signs of the change in realized volatility, the change in auto-correlation \nof realized returns, the change in skewness of realized returns, the change in kurtosis of realized", "tags": []}
{"fragment_id": "F_R12_p3_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p3:2", "text": "returns, and the change in the Jarque-Bera statistic. We follow the approach taken in Easley, Lopez \nde Prado, O’Hara, and Zhang (ELOZ) (2021) of using machine learning to ask if our \nmicrostructure features can predict our outcome labels in cryptocurrency markets. \nSome readers might find it odd to care about crypto market liquidity and price dynamics \nrather than crypto valuation per se. Two reasons to do so are paramount. First, as noted in ELOZ, \nin high frequency markets how the market is structured turns out to be critical in predicting where \nthe market is going. The less “efficient” the market, the more predictable it is, so understanding \nthe efficiency of crypto trading matters. The second reason is that institutional and high frequency \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p4_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p4:1", "text": "4 \n \ntraders rely on algorithmic trading approaches to optimize trading strategies. These strategies, in \nturn, rely on predicting market dynamics to determine the optimal path for executing tr ades. \nAlgorithmic trading is widely used in crypto markets, where trade bots allow both crypto “whales” \nand retail traders alike the ability to trade dynamically.\n3 But the algorithmic strategies employed \nthere generally rely on common market indicators, and not on the underlying microstructure \nvariables that may drive the more sophisticated trade execution strategies found in more standard \nasset markets.\n4 The labels we focus on are inputs to those quantitative strategies, so understanding \ntheir predictability is fundamental to attracting such traders to the crypto space. Perhaps the \nsimplest way to characterize our interest is that we are asking: Are crypto markets really different, \nand if so, how?\n5 \nOur research design uses high frequency data from Binance (the largest crypto exchange) for \ncryptocurrencies of five major blockchain platforms: Bitcoin (BTC), Ethereum (ETH), Ripple \n(XRP), Solano (SOL), and Cardano (ADA)\n6. We are interested in both the effects of own \nmicrostructure variables for prediction (e.g. does the ADA Roll metric predict the sign of changes \nin future ADA volatility) and the cross effects for prediction (e.g. does Bitcoin VPIN predict the", "tags": []}
{"fragment_id": "F_R12_p4_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p4:2", "text": "sign of changes in future Ethereum autocorrelation). Our interest in these cross -asset effects \nreflects the reality that high frequency trading behavior often involves complex multi -asset \n \n3 For a discussion of trade bots see Coinbase, “How trade bots work? where it is noted that “some common \nparameters bots use include price, time frame, and order volume, while common market indicators include moving \naverages (MAs), relative strength index (RSI), and more”. Available at https://www.coindesk.com/learn/what -are-\ncrypto-trading-bots-and-how-do-they-work/ \n4 There is some discussion in the crypto industry of the potential use of microstructure measures, but no systematic \nevaluation of microstructure measures and their ability to predict characteristics of the distrib ution of returns. See for \nexample, https://medium.com/@kryptonlabs/vpin-the-coolest-market-metric-youve-never-heard-of-e7b3d6cbacf1 \n5 A related approach is taken by Kogan, Makarov, Niessner, and Schoar [2024] who investigate whether retail \ntrading in crypto differs from retail trading in stocks and gold. These authors find substantial differences in retail \ncrypto trading. \n6 Each blockchain has a native token in which transactions are charged. In addition, various non -native tokens can \nbe deployed on a blockchain. Here we focus the study only on the native tokens, which are represented by the", "tags": []}
{"fragment_id": "F_R12_p4_3", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p4:3", "text": "tickers in parentheses. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p5_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p5:1", "text": "5 \n \nstrategies. We use random forest machine learning to ascertain overall predictability and to \ndetermine which features (i.e. microstructure variables) are most impo rtant for understanding \nmarket dynamics. Our sample period spans January 2021 – July 2023, allowing us to investigate \nwhether the predictability of crypto markets behaved differently during the “crypto winter” period. \nOur research provides a number of results, three of which we highlight here: \nFirst, we find surprisingly high values for the Roll Measure and VPIN in crypto markets \nrelative to more standard equity and futures market settings. The greater serial correlation in crypto \nprices is consistent with more momentum- based trading, while the higher levels of VPIN are \nindicative of greater trade toxicity arising potentially from more asymmetric information. \nSecond, we find strong predictability of microstructure measures for future market price \ndynamics. Averaging across all currencies and variables, we find an AUC > .55, a very strong \nresult consistent with deviations from market efficiency.\n7 Focusing on the individual features, we \nfind similar predictability (AUC ranging from .54 - .61) with the exception of skewness where we \nfind no predictability (AUC=.50). Overall, we find that predictability is driven by the Roll measure", "tags": []}
{"fragment_id": "F_R12_p5_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p5:2", "text": "and by VPIN. Own measures of Roll and VPIN matter for most of our predicted features, with \nother microstructure features ha ving little or no importance for own market prediction. Cross \neffects are also important: BTC Roll and VPIN and ETH Roll and VPIN have strong predictive \npower for price dynamics across the other crypto currencies. \nThird, a particular concern for instit utional trading methods is the stability of these market \ndynamics. We use the natural experiment of “crypto winter” to investigate how these market \ndynamics are affected by different market regimes. Somewhat surprisingly, we find no effects of \n \n7A standard metric in random forest algorithms to measure the performance of classification models is ROC, or the \nreceiver operating classification curve. The AUC stands for “area under the ROC curve” and essentially captures \nthe predictive ability of the machine learning algorithm. For more discussion of evaluation techniques for financial \nmachine learning see Lopez de Prado (2018). \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p6_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p6:1", "text": "6 \n \ncrypto wint er on our results, suggesting a remarkable stability to the price dynamic effects \nuncovered here. \nOverall, we show that the price dynamics of the crypto market respond to its market \nmicrostructure in ways similar to that of other financial assets. That th e market exhibits \ninefficiency is not too surprising – crypto markets are relatively young, exhibit large volatility \nrelative to other asset classes and are much less regulated, all features that undermine achieving \nprice efficiency. 8 But even in mature m arkets, there can be predictability from microstructure \nvariables. As ELOZ [2021] showed, futures markets also exhibit inefficiency, although not to the \ndegree found here in the crypto markets. What is true in both market settings is that over short \nhorizons liquidity is predictable, and thus potentially exploitable for optimizing trading strategies. \nMoreover, our results on the strong cross effects of the Roll measure, a metric capturing \nautocorrelation properties, and VPIN, a measure of market toxicity, convincingly support that there \nare common factors driving price dynamics across cryptocurrencies.\n9 \nOur research relates to several streams of research directed to understanding the market \ndrivers of cryptocurrencies. A large literature in computer science uses machine learning to predict", "tags": []}
{"fragment_id": "F_R12_p6_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p6:2", "text": "future crypto prices and returns (see, for example, Koker and Koutmos (2020); Jaquart, Kopke, \nand Weinhardt (2022); Cortese, Kolm, and Lindstrom (2023); Filippou, Rapach, and Thimsen \n(2024)). Our analysis also relies on m achine learning but differs from the black box prediction \nmethods typically used in that we are testing for the effects of particular model -based metrics on \nmarket dynamics. In common with these papers, we find strong predictability, and thus \ninefficiency, in crypto markets. \n \n8 See also Nimalendram et al [2021] who use variance tests to show cryptocurrency inefficiency and examine its \nrelationship to market regulation. \n9 For a discussion of factor pricing in crypto see Cong, Karolyi, Tang and Zhao (2022). \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p7_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p7:1", "text": "7 \n \nA large finance literature focuses on the valuation of cryptocurrencies (see Pagnotta and \nBuraschi (2018), Cong, et al (2020), Bias et al (2022) for theoretical analyses). Much of the \nempirical literature draws on applying valuation methodologies to estimate cryptocurrency values. \nErb (2020) investigates linkages between gold valuation and bitcoin valuation but concludes that \n“neither gold nor bitcoin are obvious inflation hedges, stores of value, or safe havens” and so are \nhard to value.\n10 Liu and Tsyvinski (2018) argue that crypto has no exposure to common equity \nmarket or macro factors or to currency or commodity markets. 11 Liu, Tsyvinski and Wu [2021] \ndevelop a three-factor model based solely on crypto market, size and momentum to explain returns. \nCong, Karolyi, Tang and Zhao (2021) augment these factors with value and network factors, \nyielding a five-factor model of crypto returns. Our analysis suggests that inter-dependencies across \ncrypto currencies arising from information and liquidity dynamics may be potential factors to \nconsider in future research. \nAnother substantial body of research focuses on the trading of crypto assets (for a general \nsurvey see Hallaburda et al (2022)). Early papers here include Easley, O’Hara, and Basu (2018), \nHuberman and Lasko (2021), and Cong He, and Li (2021), who analyzed the equilibrium role of", "tags": []}
{"fragment_id": "F_R12_p7_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p7:2", "text": "transaction fees and miners in Bitcoin. Recent work analyzes trading across crypto exchanges, \nwith Makorov and Schoar (2019; 2020) finding extensive arbitra ge opportunities and Crepelliere \net al (2022) documenting a decreasing trend in such opportunities. Our results are consistent with \nthe market inefficiency found there, with our results on its persistence during crypto winter \nunderscoring the importance of understanding the cryptocurrency microstructure. Also relevant is \nMakorov and Schoar (2022) who present intriguing evidence on the trading and network structure \n \n10 See Erb (2020) pg. 15. \n11 Related research investigates the linkages of crypto markets to other markets, with Iyer (2022) establishing \nincreased interdependence between crypto and equity markets. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p8_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p8:1", "text": "8 \n \nof the bitcoin blockchain. These authors find that the bitcoin ecosystem is still dominated by large \nand concentrated players, raising anew the issue of whether crypto currencies can appeal to a wider \ninvestment audience. Research directly addressing this issue includes Hardle, Harvey and Ruelle \n(2020), Harvey et al (2022) and Ang, Morris, and Savi (2022). \nThe paper is organized as follows. Section II describes the microstructure variables we \nconsider, the market statistics we predict using these microstructure variables and the random \nforest procedure we use to generate predictions. Section III describes the data set: Binance data on \nprices and volumes of trade for the leading five crypto currencies for the period January 2021 to \nJuly 2023. Section IV provides results about predictability and the relative importance of each \nmicrostructure measure in generating predictions. Section V offers two robustness tests: the effect \nof crypto winter; and using logistic regressions rather than random forests to generate predictions. \nSection VI concludes. \nII. Research Design \nWe ask whether five standard microstructure variables are useful in predicting various \nmeasures of market dynamics. The specific microstructure measures considered are: Roll \nmeasure, Roll impact measure, Kyle’s lambda, Amihud measure and VPIN (Volume synchronized", "tags": []}
{"fragment_id": "F_R12_p8_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p8:2", "text": "measure of information -based trade). These microstructure measures are intended to measure \nilliquidity or the presence of information -based trade. Illiquidity and information should lead to \nprice volatility, and ELOZ (2021) demonstrate that these microstructure measures are successful \nin predicting price dynamics in futures markets. \nThe price dynamics we focus on are measures of changes in the distribution of realized returns. \nThe specific price dynamics measures we predict are: the sign of the change in the sequential \ncorrelation, the sign of the change in the Jarque -Bera statistic, the sign of the change in realized \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p9_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p9:1", "text": "9 \n \nvolatility, the sign of the change in kurtosis, and the sign of the change in the skewness. These \nmeasures have distinct implications for trading strategies. For example, if realized volatility is \nexpected to increase, then increasing the speed of algo execution would be expected to reduce fill \nprice uncertainty. An increase in predicted serial correlation can result in greater or less price \nimpact depending upon the trade side being executed. As ELO [2015] show, this should change \nthe optimal speed of trading.\n12 The Jarque -Bera statistic captures normality of returns, so its \nincrease signals non-normal returns, suggesting that estimates of implementation shortfall may be \ntoo small. If skewness is expected to increase, then the distribution of returns is shifted to one \nside, perhaps consistent with toxicity in order flow. An increase in kurtosis means greater weight \nin the tails, an outcome that may signal a withdraw of liquidity support by market makers. \nDelaying the speed of execution would then be optimal. \nThese price variables are defined using Binance data on prices and volume. For each crypto \nwe first create one-minute time bars; that is, we split the data into segments of length one-minute \nand we record the price at the beginning of each time bar, at the end of each time bar, and the \ndollar volume of trade that occurs within each time bar.", "tags": []}
{"fragment_id": "F_R12_p9_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p9:2", "text": "13 Let t=1,2,… index time measured in \nminutes. The basic variables we consider are the ending price tp in time bar t, the return \n11( )/t tt tr pp p −−= − in time bar t and tV the dollar volume of trade in time bar t. \nWe next compute a realization of each of our microstructure measures for each time bar t. Each \nof these microstructure measures are based on some amount of past data. For any time bar t, a \nmicrostructure measure at t is computed using data in periods t, t- 1, …, t -W where W is the \n \n12 These authors also demonstrate how incorporating microstructure variables into trading strategies can improve \nupon the outcomes provided by standard trade algorithms. In particular, an algorithm based on predicted VPIN \nchanges and volume participation dominates a VWAP trading strategy. See Lopez de Prado et al (2020) for more \ndiscussion. \n13 We use one-minute time bars because that is the highest granularity bar data the public exchanges offer. This time \nperiod also seems appropriate for capturing market dynamics in a high frequency setting. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p10_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p10:1", "text": "10 \n \nlookback window. In our analysis we consider lookback windows of 50 bars and 100 bars. For \nexample, the Amihud measure at time bar t is the average ratio of absolute returns to dollar volume \nwhere the average is computed over the past W time bars. Note that a microstructure measure at \ntime bar t and one at time bar t+1 are computed using W-1 overlapping bars of market data. \nOur market microstructure variables are defined from these basic variables as follows: \n1. The Roll measure is \n 2 �|𝑐𝑐𝑐𝑐𝑐𝑐(𝚫𝚫𝑷𝑷𝒕𝒕 , 𝚫𝚫𝑷𝑷𝒕𝒕−𝟏𝟏)| , \n𝚫𝚫𝑷𝑷𝒕𝒕 = [∆𝑝𝑝𝑡𝑡−𝑊𝑊, ∆𝑝𝑝𝑡𝑡−𝑊𝑊+1, … ,∆𝑝𝑝𝑡𝑡] , \n 𝚫𝚫𝑷𝑷𝒕𝒕−𝟏𝟏 = [∆𝑝𝑝𝑡𝑡−𝑊𝑊−1, ∆𝑝𝑝𝑡𝑡−𝑊𝑊, … ,∆𝑝𝑝𝑡𝑡−1] , \nwhere ∆𝑝𝑝𝑡𝑡 = 𝑝𝑝𝑡𝑡 −𝑝𝑝𝑡𝑡−1 . \n2. The Roll impact measure ---the Roll measure divided by dollar volume over a certain \nperiod, is \n2 \n�|𝑐𝑐𝑐𝑐𝑐𝑐(𝚫𝚫𝑷𝑷𝒕𝒕 ,𝚫𝚫𝑷𝑷𝒕𝒕−𝟏𝟏)|\n𝑝𝑝𝑡𝑡𝑉𝑉𝑡𝑡\n . \n3. The Amihud measure is \n1\n𝑊𝑊 � |𝑟𝑟𝑖𝑖|\n𝑝𝑝𝑖𝑖𝑉𝑉𝑖𝑖\n𝑡𝑡\n𝑖𝑖=𝑡𝑡−𝑊𝑊+1\n , \n4. Kyle’s 𝜆𝜆 is \n𝑝𝑝𝑡𝑡−𝑝𝑝𝑡𝑡−𝑊𝑊\n∑ 𝑏𝑏𝑖𝑖𝑉𝑉𝑖𝑖𝑡𝑡\n𝑖𝑖=𝑡𝑡−𝑊𝑊\n , \nwhere 𝑏𝑏𝑖𝑖 = 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠(𝑝𝑝𝑖𝑖 −𝑝𝑝𝑖𝑖−1) . \n5. VPIN is \n1\n𝑊𝑊 � |𝑉𝑉𝑖𝑖\n𝑆𝑆 −𝑉𝑉𝑖𝑖\n𝐵𝐵|\n𝑉𝑉𝑖𝑖\n𝑡𝑡\n𝑖𝑖=𝑡𝑡−𝑊𝑊+1\n, \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p11_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p11:1", "text": "11 \n \nwhere 𝑉𝑉𝑖𝑖\n𝐵𝐵 = 𝑉𝑉𝑖𝑖𝑍𝑍�\n∆𝑝𝑝𝑖𝑖\n𝜎𝜎∆𝑝𝑝𝑖𝑖\n�, 𝑉𝑉𝑖𝑖\n𝑆𝑆 = 𝑉𝑉𝑖𝑖 −𝑉𝑉𝑖𝑖\n𝐵𝐵. \n \nWe use these microstructure measures to predict the signs of changes in various market \nstatistics. These “signs of change” are represented by -1 for a negative change and +1 for a positive \nchange. Thus, our market statistics data is a sequence of -1 and +1, one for each time bar. Similar \nto the way we compute m icrostructure measures, these market statistics are also computed using \nsome number of past observations. For example, the distribution of realized returns as of time bar \nt is the empirical distribution of returns over some number of past time bars. The number of past \ntime bars used in computing each market statistic is also W (the lookback window). Note that this \nprocedure implies that the sign of change in a market statistic at time bar t and at time bar t+1 also \nuses W-1 overlapping observations of data. So, when we want to predict the sign of the change in \na market statistic we predict it over enough future time bars to avoid using overlapping data. That \nis, we use a substantial ``look ahead’’ window. We set this look ahead window to be h=1,500 bars; \nroughly one day of trading. \nFormally the signs of change in market statistics are: \n1. The sign of the change in realized volatility \n𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠(𝜎𝜎\n𝑡𝑡+ℎ −𝜎𝜎𝑡𝑡) ,", "tags": []}
{"fragment_id": "F_R12_p11_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p11:2", "text": "where 𝜎𝜎𝑡𝑡 is the realized volatility of one bar returns over a look back window of size W. \n2. The sign of change in Jarque-Bera statistics of realized returns \n𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠�𝐽𝐽𝐽𝐽(𝑟𝑟𝑡𝑡+ℎ) −𝐽𝐽𝐽𝐽(𝑟𝑟𝑡𝑡)� , \n𝐽𝐽𝐽𝐽(𝑟𝑟) = 𝑠𝑠\n6 �𝑆𝑆2 + 1\n4 (𝐶𝐶 −3)2� , \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p12_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p12:1", "text": "12 \n \nwhere 𝑆𝑆 is the skewness and 𝐶𝐶 is the kurtosis of realized returns 𝑟𝑟 over the previous W \nbars. \n3. The sign of change in sequential correlation of realized returns \n𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠(𝑠𝑠𝑐𝑐𝑡𝑡+ℎ −𝑠𝑠𝑐𝑐𝑡𝑡), \n𝑠𝑠𝑐𝑐𝑡𝑡 = 𝑐𝑐𝑐𝑐𝑟𝑟 𝑟𝑟 (𝑟𝑟𝑡𝑡, 𝑟𝑟𝑡𝑡−1). \n4. Th\ne sign of change in absolute skewness of realized returns: \nsign[ thSkew + - tSkew ]. \n5. The sign of the change in kurtosis of realized returns: \nsign[ th tKurt Kurt+ − ]. \nThe microstructure literature suggests that a financial asset’s microstructure measures \nshould provide information about its price dynamics. But the literature provides little guidance \nabout exactly what form this relationship should take. From prior research it also seems reasonable \nto expect cross-asset relationships, for example BTC or ETH microstructure measures may help \npredict market statistics of other cryptos, but again the literature provides no guidance about what \nform this relationship should take.\n14 To avoid restricting ourselves to an arbitrary structure we use \nmachine learning to discover these relationships. \nA wide variety of machine learning procedures could be used to ask about the predictive \ncontent of our market microstructure measures (see Lopez de Prado (2018) and Kelly and Xiu \n(2023) for discussion of these approaches). We chose a random forest procedure primarily to make", "tags": []}
{"fragment_id": "F_R12_p12_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p12:2", "text": "it possible to compare our results to previous results about the predictive content of microstructure \nmeasures. In ELOZ (2021) we used a random forest applied to futures data to ask if these same \n \n14 See, for example, research on crypto currency interdependence by Kukacka and Kristofex (2023) and Qureshi et \nal (2020). \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p13_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p13:1", "text": "13 \n \nmicrostructure measures could predict changes in market statistics. In this standard financial \nmarket, the answer was yes; some own market measures m atter and so do some cross -market \nmeasures. Here we ask if these same microstructure measures provide insight into crypto price \ndynamics. \nIn the language of random forests, our “features” (the microstructure measures) are used \nto predict “labels” (the market statistics). Our random forest procedure begins by building, for each \ncrypto currency, decision trees based on repeated cuts of the time series of labels and features into \ntwo pieces. The division of the data is determined by a two-step process. First, for each feature we \ncompute the information gain (gain in homogeneity of the data) that would be obtained by splitting \nthe data using that feature. Second, we cut the data into two pieces (two branches of the decision \ntree) by randomly selecting two featu res and using the feature with the largest information gain. \nWe repeat this cutting process until no additional cut will yield an information gain. For each \ncrypto currency this process yields a decision tree that predicts labels for any assignment of \nfeatures. To avoid tying our predictions to a single tree that may be too heavily influenced by \nrandomness in the data, we create multiple decision trees for each crypto currency. This is done", "tags": []}
{"fragment_id": "F_R12_p13_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p13:2", "text": "by creating 100 trees using bootstrapped samples of the data from t he actual data set. The \nprediction given any list of features is then the majority prediction using the collection of trees.\n15 \nUltimately, we are interested in the accuracy of our predictions and in which features \ncontribute to any accuracy.16 In a regression analysis it is standard to compute p-values to measure \nthe importance of any predictor. This is, however, an in- sample measure and its computation is \ntied to the regression analysis. In contrast, the feature importance measure Mean Decreas ed \n \n15 For more discussion of this procedure see ELOZ (2021). \n16 For a discussion of how best to measure accuracy in financial machine learning applications see Lopez de Prado \n(2018), Chapter 8. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p14_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p14:1", "text": "14 \n \nAccuracy (MDA) is based on out -of-sample prediction, and it can be used to compute the \nimportance of any predictive variable. Mean Decreased Accuracy for a feature represents how \nmuch prediction accuracy we lose if we compute accuracy first using that f eature and then using \nshuffled values of the feature. To compute MDA, we first split the data into disjoint training and \ntest sets. We run our random forest procedure on the training set and compute accuracy of \npredictions on the test set (the definition of accuracy is given below). We then rerun the random \nforest procedure on the training set after randomizing the value of a feature and compute accuracy \nagain. The relative loss in accuracy is defined as the MDA measure for this feature. \nIII. Data \nThe data used in this study are obtained from the Binance public database for the period \nJanuary 2021 to July 2023. Historically, there have been a large number of crypto exchanges, some \nof which were relatively short -lived. As of December 2023, active centralized crypto exchanges \nwith daily volume greater than one -hundred million dollars numbers in the dozens, according to \nthe crypto analytics site https://www.coingecko.com/. Among these exchanges, Binance is the \nlargest as measured by daily dollar volume so we chose it for the focus our study. Because each", "tags": []}
{"fragment_id": "F_R12_p14_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p14:2", "text": "exchange runs their own limit order book, we cannot directly apply our current analysis to data \ncombined from multiple exchanges. \nThe format of the data is the standard time -bar candle -stick with 1 -minute intervals, \nincluding open, close, high, low price and volume aggregated from tick data within each interval. \nAll features and labels in this study are derived from this data. Since the number of \ncryptocurrencies has grown exponentially, and is quite volatile, it is infeasible to examine all of \nthe tokens traded on the exchange. We focus on the top 5 cryptocurrencies as measured by their \nmarket-capitalization in January 2021: BTC, ETH, ADA, SOL, and XRP. A deep dive into how \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p15_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p15:1", "text": "15 \n \nand if trade in these domina nt tokens differs from the smaller market size tokens would be an \nintriguing topic for future research. \nIV. Results \nTable 1 provides estimates of the mean values of the market microstructure variables over \nour sample period. As is standard in crypto se ttings, we express each currencies’ microstructure \nmeasures with respect to the U.S. dollar as given by Tethers (USDT). As expected, all variables \nhave positive signs. Averaging over longer time bars tends to reduce mean values for the Roll and \nKyle metrics, but has little impact on the Amihud and VPIN measures. The Roll measure is \nmarkedly different across the currencies, with Bitcoin having substantially higher autocorrelation. \nThe presence of “whales” trading large quantities algorithmically could be o ne reason for such a \nfinding. The VPIN measure is remarkably stable across currencies, but its level is surprising: ELO \n(2012) found (using a slightly different methodology) average VPINs for the E-mini S&P500 and \ncrude oil futures of 0.22 to 0.23 whereas these are range from .45 to .47.\n17 Such high toxicity is \nconsistent with greater information-based trading in crypto markets. \nWe now ask how much predictive accuracy we can achieve from our microstructure \nmeasures and which microstructure measures contribute to this predictive accuracy. We consider", "tags": []}
{"fragment_id": "F_R12_p15_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p15:2", "text": "two ways to measure predictive accuracy. One measure (Accuracy) is the number of correct \npredictions of the sign of change divided by the total number of predictions. The second measure \n(Area Under the Curve or AUC) can be interpreted as the probability that our fitted random forest \nwill rank a randomly drawn +1 higher than it ranks a randomly drawn - 1. We confine our \ndiscussion in the text to the simple Accuracy measure as the two measures yield nearly identical \n \n17 They are not as high as the highest VPIN levels of approximately 0.8 that ELO (2012) found for the E-mini during \nthe time period of the flash crash. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p16_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p16:1", "text": "16 \n \nresults.18 We focus on the entire sample period (January 2021 through July 2023); in a later section \nwe divide the sample into two periods and ask about the effects of Crypto Winter. \nFor each crypto currency we use the random forest procedure described in Section II applied \nto 25 features: the five microstructure measures for each of the five crypto currencies. So, we \ninclude both the effect of own microstructure measures and cross -currency microstructure \nmeasures. Random guessing should lead to predict ive accuracy of approximately 0.5 as we are \npredicting whether labels are positive (+1) or negative (-1). Anything above 0.5 suggests that our \nfeatures have some power in predicting our labels. We do not attempt to maximize prediction \naccuracy. For our pur poses, it is enough to demonstrate that our microstructure measures \ncontribute to substantial prediction accuracy here as they do in more established financial markets. \nPrevious research (ELOZ 2020) on futures markets using similar features to predict similar labels \nfound average predictive accuracy in the range 0.54 to 0.61. \nWe first examine average (averaged over the five crypto currencies) predictive accuracy. \nThe most important question is whether we can make useful predictions at all. A secondary \nquestion is how the level of predictive accuracy compares to that found in other financial markets.", "tags": []}
{"fragment_id": "F_R12_p16_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p16:2", "text": "Note that we are predicting changes in market statistics one day ahead and we make predictions \nevery minute. So even small amounts of accuracy (above 0.5) can be valuable. Average (across all \ncryptos in our sample and all labels) predictive accuracy is provided in Table 2. Depending on the \nnumber of bars used as a lookback window in constructing our market microstructure measures, \nwe find that predictive accuracy is between 0.53 and 0.54.\n19 \nIf we exclude skewness, predictive accuracy for our entire sample period ranges (across \naccuracy measures, the number of bars used and labels) from 0.52 to 0.58. These results are \n \n18 Results for both accuracy measures are provided in the tables. \n19 The results for AUC are also given in Table 1 and they range from 0.53 to 0.54. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p17_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p17:1", "text": "17 \n \nprovided in the last two rows of the two final columns of Tables 3 (Auto- correlation), 4 (JB \nstatistic), 5(Kurtosis), and 6 (Realized Volatility). However, as the last two columns of Table 7 \nshow, skewness is not predictable; for skewness we find predictive accuracy of 0.5 meaning that \nfor the crypto currencies we examine our microstructure measures are not at all useful in predicting \nmarket statistics for skewness. This is similar to results for futures where skewness is the least \npredictable label. \nPredictive accuracy is greatest for the sign of the change in realized volatility. For both of \naccuracy measures, the average accuracy of prediction ranges from 0.56 to 0.58 depending on the \nnumber of bars used. This is a remarkably high accuracy level for random forest predictions in \nfinancial applications, see Lopez de Prado (2018). \nThe predictive accuracy results indicate that the crypto market exhibits inefficiencies. When \norder flow is imbalanced, causing positive correlation in price changes and an increased Roll \nmeasure, volatility increases and it remains high through our look-ahead window of 1,500 bars or \nroughly a day. The ability to predict changes in realized volatility from microstructure measures \nintended to measure imbalances or correlations in order flow suggests that there is trend following", "tags": []}
{"fragment_id": "F_R12_p17_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p17:2", "text": "in these markets which could be exploited by sophisticated trading algorithms. \nWe find it interesting that the level of predictive accuracy is not very different from that \nfound in futures markets using a similar approach to prediction. Both market settings are electronic \nand trade almost continuously over a 24- hour day\n20, but they do differ in that futures are well -\nestablished markets widely used by institutional traders whereas crypto trading is more nascent \nand dominated by crypto “natives”. Our results suggest that the market dynamics of crypto \n \n20 CME, one of the leading futures markets offer products that trade up to 23 hours a day Monday through Friday. \nMany crypto exchanges operate 24 hours, 7 days a week without a break. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p18_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p18:1", "text": "18 \n \nmarkets may already be similar enough to more mature markets to facilitate analogous \nsophisticated trading techniques. \nEqually important for our purposes is determining which microstructure measures \ncontribute to prediction accuracy. Figures 1 through 4 provide average MDA scores (averaged \nover our five crypto currencies) for prediction of the change in realized volatility (Figure 1), the \nchange in Kurtosis (Figure 2), the change of the JB statistic (Figure 3) and the change in auto -\ncorrelation (Figure 4).\n21 These results are derived from prediction of each market statistic for each \ncrypto currency using only that currencies own microstructure measures. These results are \nremarkably consistent across the various market statistics. For prediction of each market statistic, \nthe Roll measure is the most important feature as measured by MDA. VPIN is the second most \nimportant measure and the Roll impact measure is third most important. It is reasonable to expect \nthat microstructure measures for other crypto currencies could be useful in predicting market \nstatistics for a specific crypto. In particular, as BTC and ETH are the leading cryptos it could be \nthat trade in these cryptos leads trade, and thus changes in market s tatistics, for other cryptos. \nFigures 5 through 9 provide the results for this analysis. Each figure provides MDA scores for one", "tags": []}
{"fragment_id": "F_R12_p18_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p18:2", "text": "of our price dynamics labels for each of our five crypto currencies. For example, Panel 1 of Figure \n5 provides MDA scores for t he 25 microstructure measures for the sign of the change in realized \nvolatility for ADA. Examining this panel of Figure 5 shows that ADA’s own Roll measure is the \nmost important feature. The next three most important features for ADA are, in order, ADA’s own \nVPIN measure ,the Roll measure for BTC, and the Roll measure for ETH. and \nFigures 5 through 9 tell a compelling story. In almost every case, each crypto’s own Roll \nmeasure is the most important feature for predicting price dynamics. It is reassuring th at some \n \n21 We do not provide results for Skewness as there is no predictive accuracy for it. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p19_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p19:1", "text": "19 \n \nmicrostructure measure matters, but the details of prediction in crypto markets are different from \nthose in futures markets. In futures markets, the Amihud measure and VPIN were the most \nimportant own measures. In our crypto sample, own VPIN shows up frequently as an important \nfeature; and only occasionally do the own Roll impact measure or the Amihud measure have \nimportance. The own Kyle measure typically has a low, nearly-zero MDA score. \nThe cross-market result for Bitcoin and Ethereum reveal particularly interesting dynamics. \nThe BTC Roll measure and BTC VPIN are the most important features in driving Bitcoin price \ndynamics, with the Ethereum Roll measure also playing an important but lesser role. However, the \nother crypto currency cross -measures have virtually no influence. Similarly, price dynamics for \nEthereum are driven by the ETH Roll measure and the ETH VPIN with the Bitcoin Roll measure \nplaying again playing an important but lesser role. As is the case for Bitcoin, the other crypto \ncurrency cross-measures have virtually no influence. Across all other cryptos and labels we study, \nRoll measures for BTC and ETH have strong predictability signified by high MDA scores. All \nother cross-crypto features typically have very low MDA scores. This sug gests that trade in BTC", "tags": []}
{"fragment_id": "F_R12_p19_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p19:2", "text": "and ETH leads price changes and volatility in other cryptos; not a surprising result, but one that is \nconsistent with the notion that trade in other cryptos follows trade in these two large cryptos. \nAlthough the level of predictabil ity we find suggests that there are inefficiencies in the \ncrypto markets, the fact that market microstructure measures are important for price dynamics \ndemonstrates that these markets have much in common with more standard financial asset markets. \nOur random forest analysis does not imply causation flowing from our features to our labels. But \nit does suggest that trading tools based on own Roll and VPIN as well as cross BTC and ETH Roll \nmeasures can be valuable. \nV. Crypto Winter and Other Robustness Tests \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p20_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p20:1", "text": "20 \n \nIn this section we consider two robustness tests. First, we ask if our results are stable over time \nusing the beginning of crypto winter as a date to split our sample. Second, we ask if a logistic \nregression would produce similar results. \nV.1 . Crypto Winter \nPrices and trading volumes of crypto currencies changed dramatically over our sample \nperiod. Approximately the first half of our sample falls in a “boom” period for crypto currencies \nwith rising prices and increasing high trading volumes. During this period, overall daily trading \nvolume reached a peak of $158.64B on April 10, 2021, and remained high over much of 2021. \nEmblematic of crypto prices, during this period the price of Bitcoin reached a high of $67,617 on \nNovember 2021, where after prices began a steady decline, reaching a low of $15,742 in October \n2022.\n22 This latter period is generally referred to as “crypto winter”. A natural question is whether \nour predictability results differ in the period before crypto winter and after it began. We selected \nNovember 10, 2021 as the date to use in breaking our sample into two pieces as the total crypto \nmarket capitalization reached its high on November 9, 2021. \nWe reran our analysis separately on these before and after periods. Ex ante it is not obvious", "tags": []}
{"fragment_id": "F_R12_p20_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p20:2", "text": "whether predictability should increase or decrease as a result of crypto winter or if there should be \nchanges which microstructures contribute most to them. Crypto winter was a time of great \nuncertainty in the value of crypto and this should make predicting more difficult. But the market \nwas also likely less efficient and this might make predicting easier. \nTable 8 provides our results. Although valuations changed dramatically between these two \nsub-periods, predictability is nearly unchanged and the importance of our various microstructure \nmeasures is also unchanged. These results suggest that although valuations changed, the structure \n \n22 Along with depressed prices, crypto winter also featured several spectacular failures of crypto currencies and \nexchanges. For discussion of these aspects of crypto winter see Arner, Zetsche, Buckley, and Kirkwood (2023). \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p21_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p21:1", "text": "21 \n \nof trading did not change, and the level of inefficiency in the market was also nearly unchanged. \nWe regard this as good news for Crypto markets as it implies that trading tools based on these \nmicrostructure measures should be robust to the extreme volatility of Crypto currency markets. \n Furthermore, in Tables 9 and 10 we present the values calculated for before and after \nNovember 2021 for the microstructure features used in our analysis. It is worth noting that out of \nthe five market microstructure variables that we consider, Roll measure, Amihud measure and \nKyle’s λ are proportional to the scale of price while Roll Impact and VPIN are not. As such, \nbecause the price of most crypto tokens dropped significantly after the peak in November 2021, \nthere is a more pronounced change in the Roll measure, the Amihud measure and Kyle’s λ. On the \nother hand, the values of Roll Impact and VPIN are comparably more stable before and after \nNovember 2021. \nV.2. Logistic Regression \nAn alternative to using a random forest to predict our binary labels is to use a logistic \nregression. The logistic regression uses a liner regression to model the log odds of the two labels.\n23 \nThe aggregated accuracy and MDA results for the logistic regression are provided in Figure 10. \nOverall, the results are similar to those obtained via a random forest. There is predictive accuracy", "tags": []}
{"fragment_id": "F_R12_p21_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p21:2", "text": "for all labels other than skewness. The ranking of features by MDA is unchanged, although the \nactual MDA scores for the most important features, the Roll measure and VPIN, are increased \nrelative to the scores for the less useful features. This is reassuring as these results suggest that our \nability to predict and our ranking of the importance of various microstructure measures is not a \nresult of the specific method used to classify observations of market statistics. \n \n \n23 The specific approach we use is discussed in ELOZ (2022), Section 4.6. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p22_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p22:1", "text": "22 \n \nVI. Conclusion \nThe degree of predictability for market dynamics in leading crypto currencies indicates that \nthere are some inefficiencies in the crypto markets, and it’s reasonable to suspect that these \ninefficiencies would be even greater for less well established cryptos and might differ across \nexchanges. However, at least for trades of these five leading cryptos in Binance, the amount of \npredictability is not particularly different from what we found in futures markets. And this \npredictability remained almost unchanged during the crypto winter period, suggesting that market \ndynamics on the largest crypto exchange exhibit substantial stationarity. Specifically, using \nstandard microstructure metrics, we find non- trivial prediction accuracy and AUC scores for \nmultiple return statistics measures such as volatility and auto-correlation. We also highlight the \nprominent predictive role of auto- correlation (captured by the Roll Measure), providing a \nmicrostructure foundation for the momentum observed in crypto prices. \nPerhaps the most intriguing result is that the market microstructure measures we find to be \nimportant for price dynamics in cryptos are similar to those that matter for prediction in futures \nmarkets. This similarity suggests that these crypto markets have much in common with liquidity", "tags": []}
{"fragment_id": "F_R12_p22_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p22:2", "text": "and price dynamics in more standard financial asset markets. Our random forest analysis does not \nimply causation flowing from our features to our labels. But it does suggest that trading tools based \non own Roll and VPIN as well as cross BTC and ETH Roll measures can be valuable. For \ninstitutional and high frequency traders, whose trading relies on sophisticated algorithmic and \noptimized trading strategies, this commonality removes an important obstacle to their \nparticipation. Whether that tips the balance to crypto becoming an as set class in its own right \nremains to be seen. \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p23_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p23:1", "text": "23 \n \n \n \n \n \n \n \n \n \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p24_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p24:1", "text": "24 \n \nReferences \nAng, Andrew and Morris, Tom and Savi, Raffaele, 2022, Asset Allocation with Crypto: \nApplication of Preferences for Positive Skewness, Available at \nSSRN: https://ssrn.com/abstract=4042239 or http://dx.doi.org/10.2139/ssrn.4042239 \nArner, D. W. , Zetzsche, D. Buckley, R. and J. Kirkwood, 2023, The Financialization of Crypto: \nLessons from FTX and the Crypto Winter of 2022-2023, ,Available at \nSSRN: https://ssrn.com/abstract=4372516 or http://dx.doi.org/10.2139/ssrn.4372516\n \nBiais, Bruno and Bisiere, Christophe and Bouvard, Matthieu and Casamatta, Catherine and \nMenkveld, Albert J., 2023, Equilibrium Bitcoin Pricing, Journal of Finance, 78(2) 967-\n1014 \nCong, Lin and He, Zhiguo and Li, Jiasun, 2021, Decentralized Mining in Centralized Pools, \nReview of Financial Studies, 34(3)1191-1235. \nCong, Lin W. and Karolyi, George A. and Tang, Ke and Zhao, Weiyi, 2022, Value Premium, \nNetwork Adoption, and Factor Pricing of Crypto Assets, Available at \nSSRN: https://ssrn.com/abstract=3985631 or http://dx.doi.org/10.2139/ssrn.3985631\n \nCong, Lin and Li, Ye and Wang, Neng, 2020, Tokenomics: Dynamic Adoption and Valuation \nReview of Financial Studies, 34 (3), 1105-1155. \nCortese, Federico and Kolm, Petter N. and Lindstrom, Erik, 2023, What Drives Cryptocurrency \nReturns? A Sparse Statistical Jump Model Approach. Available at", "tags": []}
{"fragment_id": "F_R12_p24_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p24:2", "text": "SSRN: https://ssrn.com/abstract=4330421 or http://dx.doi.org/10.2139/ssrn.4330421\n \nCrépellière, Tommy and Pelster, Matthias and Zeisberger, Stefan, 2022, Arbitrage in the Market \nfor Cryptocurrencies Journal of Financial Markets, forthcoming \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p25_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p25:1", "text": "25 \n \nEasley, D., M. Lopez de Prado, and M. O’Hara, ELO, 2012, Flow Toxicity and Volatility in a \n High Frequency World, Review of Financial Studies, 25, 5, 1457-93. \nEasley, D., M. Lopez de Prado, and M. O’Hara, ELO, 2015, “Optimal Execution Horizon,” \n Mathematical Finance, , 25(3), 640-672. \nEasley, D., M. Lopez de Prado, M. O’Hara, and Z. Zhang ELOZ, 2021. Microstructure in the \nmachine age. Review of Financial Studies 34:3316–63. \nEasley, D., M. O’Hara, and S. Basu, 2019, From Mining to Markets: The Evolution of Bitcoin \nTransaction Fees, Journal of Financial Economics, 134(1), 91-109. \nErb, Claude B., 2020, Bitcoin is Exactly Like Gold Except When it Isn't, Available at \nSSRN: https://ssrn.com/abstract=3746997 or http://dx.doi.org/10.2139/ssrn.3746997 \nFilippou, Ilias and Rapach, David and Thimsen, Christoffer, 2024, Cryptocurrency Return \n Predictability: A Machine-Learning Analysis, Available at SSRN: \n https://ssrn.com/abstract=3914414 or http://dx.doi.org/10.2139/ssrn.3914414 \n \nHalaburda, Hanna and Haeringer, Guillaume and Gans, Joshua and Gandal, Neil, 2022 The \nMicroeconomics of Cryptocurrencies, Journal of Economic Literature, 60(3) 971-1013. \nHardle, W., C. Harvey and R. Reule, 2020, Understanding Cryptocurrencies, Journal of Financial \nEconometrics, 12:2, 181-208. \nHarvey, C., T.A. Zeid, T. Draaisma, M. Luk, H., Neville, A. Ryzm, and O. Van Hemert, An", "tags": []}
{"fragment_id": "F_R12_p25_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p25:2", "text": "investor’s guide to Crypto, 2022, Available at \nSSRN: https://ssrn.com/abstract=4124576 or http://dx.doi.org/10.2139/ssrn.4124576 \nHuberman, G., J. Leshno, and C. Moallemi, 2021, “Monopoly without a Monopolist: An \nEconomic Analysis of the Bitcoin Payment System,” Review of Economic Studies, 88 (6), \n3011-3040; \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p26_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p26:1", "text": "26 \n \nIver, T., 2022, Crypto Connections; Spillover between crypto and equity markets, IMF Global \nStability Note No. 2022/1. \nJaquart, P., S. Kopke, and C. Weinhardt, 2022, Machine learning for cryptocurrency market \nprediction and trading, Journal of Finance and Data Science, 8, (2022) 331-352. \nKelly, Bryan T. and Xiu, Dacheng, 2023, Financial Machine Learning, Available at \nSSRN: https://ssrn.com/abstract=4501707 \nKogan, S., I. Makarov, M. Niessner, and A. Schoar, 2024Are Cryptos Different? Evidence \nfrom Retail Trading, Journal of Financial Economics, forthcoming. Journal \nKoker, T. and D. Koutmos, Cryptocurrency trading using machine learning, 2020, J. Risk \nFinancial Manag. , 13(8), 178 \nKukacka, Jiri and Kristoufek, Ladislav, 2023, Fundamental and Speculative Components of \nthe Cryptocurrency Pricing Dynamics, Financial Innovation (9). \n Liu, Yukun, Tsyvinski, Aleh, and Wu, Xi, 2022, Common Risk Factors in Cryptocurrency, \nJournal of Finance, 77(2) 1133-1177. \nLopez de Prado, M., 2018, Advances in Financial Machine Learning, (Wiley; New York). \nMakarov, Igor, and Antoinette Schoar. 2019. \"Price Discovery in Cryptocurrency \nMarkets.\" AEA Papers and Proceedings, 109: 97-99. \nMakarov, Igor and Schoar, Antoinette, 2020, Trading and Arbitrage in Cryptocurrency Markets, \nJournal of Financial Economics, 135(2) 293-319.", "tags": []}
{"fragment_id": "F_R12_p26_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p26:2", "text": "Makarov, Igor and Schoar, Antoinette, 2020, Blockchain Analysis of the Bitcoin Market \nWorking paper, Available at \nSSRN: https://ssrn.com/abstract=3942181 or http://dx.doi.org/10.2139/ssrn.3942181 \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p27_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p27:1", "text": "27 \n \nNimalendran, Mahendrarajah and Pathak, Praveen and Petryk, Mariia and Qiu, Liangfei, \nInformational Efficiency of Cryptocurrency Markets (February 11, 2021). Available at \nSSRN: https://ssrn.com/abstract=3818818 \nPagnotta, E. and Buraschi, A. (2018). An equilibrium valuation of bitcoin and decentralized \nnetwork assets. Working paper, Imperial College. \nQureshi, S., M. Aftab, E. Bouri, and T. Saeed, 2020, Dynamic interdependence of crypto \nmarkets: An analysis across time and frequency, PhysicsA: Statistical Mechanics and its \nApplications, 559(1). \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p28_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p28:1", "text": "28 \n \n \nTables \n \nTable 1: Mean values of market microstructure variables \nType Window Roll Measure Roll \nImpact \nKyle’s \nLambda \nAmihud VPIN \nSOLUSDT 50 bars 0.073091217 1.1765E-\n06 \n8.60909E-05 3.17524E-\n08 \n0.469121339 \n100 bars 0.06355549 1.06911E-\n06 \n4.54613E-05 3.0913E-\n08 \n0.462114902 \nXRPUSDT 50 bars 0.000640782 5.43329E-\n09 \n2.73E-09 8.80316E-\n09 \n0.465487876 \n100 bars 0.000547614 5.05243E-\n09 \n6.91752E-09 9.06127E-\n09 \n0.456565606 \nADAUSDT 50 bars 0.001026723 2.5131E-\n08 \n6.55367E-09 3.04002E-\n08 \n0.464679 \n100 bars 0.000923532 2.33606E-\n08 \n1.77393E-09 2.95753E-\n08 \n0.458025021 \nBTCUSDT 50 bars 20.36453232 1.78623E-\n05 \n15.9869968 4.28419E-\n10 \n0.469240396 \n100 bars 18.16115194 1.62215E-\n05 \n1.255397095 4.28417E-\n10 \n0.459877142 \nETHUSDT 50 bars 1.643139356 2.83363E-\n06 \n0.002306957 1.19623E-\n09 \n0.470527845 \n100 bars 1.45823823 2.58103E-\n06 \n0.00490705 1.19626E-\n09 \n0.461889666 \n \n \nTable 2: Aggregated accuracy and AUC \nWindow Aggregated Accuracy Aggregated AUC \n50 bars 0.538089 0.538134 \n100 bars 0.530436 0.530428 \n \nBoth accuracy measures are aggregated across all 5 cryptocurrencies, across the entire test \nperiod, and across all 5 labels. The results provided in the table are the average values. \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p29_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p29:1", "text": "29 \n \nTable 3: Feature importance and prediction performance for auto-correlation \nPeriod Window Roll \nMeasure \nRoll \nImpact \nKyle’s \nLambda \nAmihud VPIN Accuracy AUC \n2021.1 – \n2021.11 \n50 bars 0.04324 0.01002 0.00444 0.00364 0.01496 0.533329 0.533324 \n100 bars 0.03972 0.00594 0.00452 0.00244 0.01042 0.527921 0.527826 \n2021.11 – \n2023.7 \n50 bars 0.04506 0.01044 0.00168 0.00298 0.01674 0.536524 0.536573 \n100 bars 0.03606 0.00722 0.00112 0.00164 0.01396 0.526781 0.526674 \n2021.1-\n2023.7 \n50 bars 0.04426 0.00986 0.00396 0.00464 0.01552 0.531708 0.531698 \n100 bars 0.03892 0.00630 0.00424 0.00318 0.01016 0.523608 0.523532 \n \nTable 4: Feature importance and prediction performance for JB Statistics \nPeriod Window Roll \nMeasure \nRoll \nImpact \nKyle’s \nLambda \nAmihud VPIN Accuracy AUC \n2021.1 – \n2021.11 \n50 bars 0.04662 0.00838 0.00502 0.00006 0.0154 0.532057 0.532077 \n100 bars 0.04256 0.00544 0.00516 -0.0002 0.00884 0.530776 0.530836 \n2021.11 – \n2023.7 \n50 bars 0.04918 0.00772 0.00036 0.00142 0.01678 0.527283 0.527297 \n100 bars 0.03978 0.00518 0.00106 0.00042 0.01404 0.523098 0.523367 \n2021.1-\n2023.7 \n50 bars 0.04822 0.00814 0.00476 0.00124 0.01636 0.532994 0.533029 \n100 bars 0.04330 0.00616 0.00506 0.00190 0.00946 0.531124 0.531142 \n \nTable 5: Feature importance and prediction performance for Kurtosis \nPeriod Window Roll \nMeasure \nRoll \nImpact \nKyle’s \nLambda", "tags": []}
{"fragment_id": "F_R12_p29_2", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p29:2", "text": "Amihud VPIN Accuracy AUC \n2021.1 – \n2021.11 \n50 bars 0.03308 0.00814 0.0036 0.00256 0.01882 0.544144 0.544135 \n100 bars 0.03196 0.0048 0.00438 0.00198 0.01082 0.531292 0.531236 \n2021.11 – \n2023.7 \n50 bars 0.0335 0.00852 0.00214 0.00344 0.0221 0.545655 0.545724 \n100 bars 0.02728 0.00586 0.00184 0.00192 0.01498 0.529588 0.529195 \n2021.1-\n2023.7 \n50 bars 0.03396 0.00792 0.00332 0.00328 0.01888 0.544184 0.544134 \n100 bars 0.03124 0.00510 0.00452 0.00286 0.01066 0.530874 0.530842 \n \nTable 6: Feature importance and prediction performance for Realized Volatility \nPeriod Window Roll \nMeasure \nRoll \nImpact \nKyle’s \nLambda \nAmihud VPIN Accuracy AUC \n2021.1 – \n2021.11 \n50 bars 0.05662 0.00922 0.00562 -0.00092 0.00808 0.579853 0.57897 \n100 bars 0.04578 0.0048 0.0036 -0.00226 0.0057 0.562562 0.561153 \n2021.11 \n– 2023.7 \n50 bars 0.06442 0.00822 0.00092 0.00132 0.00922 0.584063 0.582128 \n100 bars 0.05098 0.00542 0.00182 0.00014 0.00962 0.567168 0.564179 \n2021.1– \n2023.7 \n50 bars 0.058120 0.00878 0.00534 0.001060 0.00922 0.581566 0.581797 \n100 bars 0.046740 0.00562 0.00376 0.000660 0.00664 0.564701 0.564723 \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p30_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p30:1", "text": "30 \n \nTable 7: Feature importance and prediction performance for Skewness \nPeriod Window Roll \nMeasure \nRoll \nImpact \nKyle’s \nLambda \nAmihud VPIN Accuracy AUC \n2021.1 – \n2021.11 \n50 bars 0.03448 0.00804 0.00348 0.00266 0.01202 0.500303 0.50031 \n100 bars 0.03202 0.00452 0.00366 0.00214 0.00858 0.501256 0.501286 \n2021.11 \n– 2023.7 \n50 bars 0.03704 0.00858 0.00162 0.00318 0.01402 0.504024 0.50409 \n100 bars 0.02936 0.006 0.00138 0.00124 0.01174 0.503889 0.503985 \n2021.1– \n2023.7 \n50 bars 0.035320 0.00782 0.00302 0.00342 0.01238 0.499993 0.500012 \n100 bars 0.031440 0.0051 0.00392 0.00282 0.00862 0.501874 0.501902 \n \n \n \nTable 8: Aggregated accuracy and AUC before and after Nov 2021. \nPeriod Window Aggregated Accuracy Aggregated AUC \n2021.1 – 2021.11 50 bars 0.537937 0.537763 \n100 bars 0.530761 0.530467 \n2021.11 – 2023.7 50 bars 0.53951 0.539162 \n100 bars 0.530105 0.52948 \n2021.1 – 2023.7 50 bars 0.538089 0.538134 \n100 bars 0.530436 0.530428 \n \nBoth accuracy measures are aggregated across all 5 cryptocurrencies, and across all 5 labels. The \nresults in the table are the averages. \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p31_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p31:1", "text": "31 \n \n \nTable 9: Mean values of market microstructure variables before Nov 2021 \nType Window Roll Measure Roll Impact Kyle’s \nLambda \nAmihud VPIN \nSOLUSDT 50 bars 0.095517402 1.17672E-\n06 \n7.20968E-05 3.46395E-\n08 \n0.472742998 \n100 bars 0.095411928 1.05633E-\n06 \n2.02224E-05 3.12291E-\n08 \n0.467878084 \nXRPUSDT 50 bars 0.001119395 3.97082E-\n09 \n1.98885E-09 4.44686E-\n09 \n0.473858007 \n100 bars 0.000972959 3.71646E-\n09 \n1.36295E-09 4.44819E-\n09 \n0.465288554 \nADAUSDT 50 bars 0.001736012 8.62496E-\n09 \n1.27048E-08 4.90547E-\n09 \n0.475664534 \n100 bars 0.001543807 7.83887E-\n09 \n4.24928E-09 4.90575E-\n09 \n0.468764089 \nBTCUSDT 50 bars 30.59170154 2.22919E-\n05 \n42.21893443 4.85441E-\n10 \n0.477695275 \n100 bars 27.11981393 2.00616E-\n05 \n1.100938512 4.85445E-\n10 \n0.469784298 \nETHUSDT 50 bars 2.275569375 2.63985E-\n06 \n0.005914603 9.99954E-\n10 \n0.483894069 \n100 bars 2.006449812 2.37222E-\n06 \n0.00496139 9.99933E-\n10 \n0.476930492 \n \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p32_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p32:1", "text": "32 \n \n \nTable 10: Mean values of market microstructure variables after Nov 2021 \nType Window Roll Measure Roll \nImpact \nKyle’s \nLambda \nAmihud VPIN \nSOLUSDT 50 bars 0.059758002 1.17637E-\n06 \n9.44109E-05 3.00358E-\n08 \n0.466968126 \n100 bars 0.044615633 1.07671E-\n06 \n6.04667E-05 3.07251E-\n08 \n0.458688473 \nXRPUSDT 50 bars 0.000356229 6.30279E-\n09 \n3.17064E-09 1.13931E-\n08 \n0.460511516 \n100 bars 0.00029473 5.84671E-\n09 \n1.02199E-08 1.18039E-\n08 \n0.451379484 \nADAUSDT 50 bars 0.000605024 3.49444E-\n08 \n2.89657E-09 4.55577E-\n08 \n0.458147685 \n100 bars 0.000554756 3.25889E-\n08 \n3.02232E-10 4.42423E-\n08 \n0.45164024 \nBTCUSDT 50 bars 14.28409348 1.52287E-\n05 \n0.391117916 3.94517E-\n10 \n0.46421365 \n100 bars 12.83488862 1.39385E-\n05 \n1.347228563 3.94512E-\n10 \n0.453986963 \nETHUSDT 50 bars 1.267135794 2.94883E-\n06 \n0.000162075 1.31292E-\n09 \n0.462581119 \n100 bars 1.132305714 2.70517E-\n06 \n0.004874742 1.31298E-\n09 \n0.452947326 \n \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p33_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p33:1", "text": "33 \n \n \nFigures \n \nFigure 1: Average MDA own feature importance for change of realized volatility. \n \nMDA feature importance using only own features for change of realized volatility. The \nprediction window is 50 bars. Results are aggregated across all five cryptocurrencies. \n \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p34_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p34:1", "text": "34 \n \n \nFigure 2: Average MDA feature importance for change of Kurtosis. \n \nMDA feature importance using only own features for the change of Kurtosis. The \nprediction window is 50 bars. Results are aggregated across all five cryptocurrencies \n \nFigure 3: MDA feature importance for change of JB Statistics \n \nMDA feature importance using only own features for the change in the JB statistic. The \nprediction window is 50 bars. Results are aggregated across all five cryptocurrencies. \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p35_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p35:1", "text": "35 \n \n \nFigure 4: MDA feature importance for change of return auto-correlation. \n \nMDA feature importance using only own features for the change in return \nautocorrelation. The prediction window is 50 bars. Results are aggregated across all five \ncryptocurrencies. \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p36_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p36:1", "text": "36 \n \nFigure 5: Aggregated MDA scores for ADA using all 25 features \n \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p37_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p37:1", "text": "37 \n \n \nEach panel of this figure provides MDA scores (aggregated over 50 and 100 bars) for \nprediction of market statistics for ADA using all 25 features (five features for each of the \nfive crypto currencies). \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p38_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p38:1", "text": "38 \n \nFigure 6: Aggregated MDA scores for BTC using all 25 features \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p39_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p39:1", "text": "39 \n \n \nEach panel of this figure provides MDA scores (aggregated over 50 and 100 bars) for \nprediction of market statistics for BTC using all 25 features (five features for each of the \nfive crypto currencies). \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p40_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p40:1", "text": "40 \n \nFigure 7: Aggregated MDA scores for ETH using all 25 features \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p41_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p41:1", "text": "41 \n \n \nEach panel of this figure provides MDA scores (aggregated over 50 and 100 bars) for \nprediction of market statistics for ETH using all 25 features (five features for each of the \nfive crypto currencies). \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p42_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p42:1", "text": "42 \n \nFigure 8: Aggregated MDA scores for SOL using all 25 features \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p43_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p43:1", "text": "43 \n \n \nEach panel of this figure provides MDA scores (aggregated over 50 and 100 bars) for \nprediction of market statistics for SOL using all 25 features (five features for each of the \nfive crypto currencies). \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p44_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p44:1", "text": "44 \n \nFigure 9: Aggregated MDA scores for XRP using all 25 features \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p45_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p45:1", "text": "45 \n \n \nEach panel of this figure provides MDA scores (aggregated over 50 and 100 bars) for \nprediction of market statistics for XRP using all 25 features (five features for each of the \nfive crypto currencies). \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p46_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p46:1", "text": "46 \n \n \n \nFigure 10: Aggregated Accuracy and MDS Results from a Logistic Regression\nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R12_p47_1", "source_id": "R12", "locator": "Easley_ssrn-4814346.pdf:p47:1", "text": "47 \n \n \n \n \nElectronic copy available at: https://ssrn.com/abstract=4814346", "tags": []}
{"fragment_id": "F_R13_p1_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p1:1", "text": "Cryptocurrency Market Efficiency Revisited:\nA Bibliometric Analysis\nIslam Abdeljawad\n *,§, Ahmad Tina\n *,¶ , M. Kabir Hassan\n †,||\nand Mamunur Rashid\n ‡,**\n*An-Najah National University, Nablus, Palestine\n†University of New Orleans, New Orleans, USA\n‡Canterbury Christ Church University\nCanterbury, Kent, UK\nReceived: 18 July 2025; Accepted: 13 September 2025\nPublished: 1 November 2025\nAbstract\nThe aim of this comprehensive review of the papers published on the Scopus database is to gain\ninsights into the various indicators that determine the level of market efficiency within the global\ncryptocurrency markets. We have employed a series of bibliometric and content analyses on 3,224\npapers published during 2014–2024. Findings indicate that the scholarly literature on cryptocurrency\nexhibits a varied range of perspectives, frequently encompassing multiple academic disciplines such\nas economics, finance, accounting, technology, and engineering. We present three significant\nfindings. First, despite a growing list of recent studies supporting some efficiency, cryptocurrency\nassets frequently deviate from conventional norms of market efficiency. Herding, co-movement,\nsentiment, and overconfidence are the major contributors behind the inefficient cryptocurrency\nmarket. Second, the intricate nature of these assets and their lack of connection to fundamental", "tags": []}
{"fragment_id": "F_R13_p1_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p1:2", "text": "economic value contribute to inconsistencies, instability, and ambiguity. Third, regulators are\nexpected to intervene with prudent and globally collaborative regulations to optimize the potential of\nthis market. In this discourse, we analyze the potential consequences derived from various\nframeworks and methods, with the aim of informing forthcoming scholarly investigations.\nThis is an Open Access article published by World Scientific Publishing Company. It is distributed under\nthe terms of the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 (CC BY-NC-ND)\nLicense, which permits use, distribution and reproduction, provided that the original work is properly cited,\nthe use is non-commercial and no modifications or adaptations are made.\nEmail addresses: §islamjawad@najah.edu, ¶ah.teneh@gmail.com, kmhassan@uno.edu, **Mamunur.\nrashid@canterbury.ac.uk\n§Corresponding author.\nOPEN ACCESS\nInternational Journal of Financial Engineering\n(2025) 2550021 (37 pages)\n© The Author(s)\nDOI: 10.1142/S2424786325500215\n2550021-1\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p2_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p2:1", "text": "Keywords: Cryptocurrency; market efficiency; financial markets; regulation; technology; behavioural\nbiases.\n1. Introduction\nThe cryptocurrency market has been characterized by significant volatility since its\ninception in 2008 (Nakamoto, 2008; Urquhart, 2016). Initially perceived as a\nsymbol of decentralized, unregulated, and economically efficient currency, scho-\nlars have identified the presence of investment prospects and hedging capabilities\nwithin the realm of cryptocurrency investment (Selgin, 2015; Dyhrberg, 2016).\nHowever, in contrast to other commodities and currencies traded in financial\nmarkets, cryptocurrencies lack a fundamental value, a characteristic widely be-\nlieved to be the primary driver of their high volatility (Cheah and Fry, 2015).\nFurthermore, cryptocurrencies exhibit a high level of volatility, surpassing that of\nother assets such as precious metals and currencies (Dwyer, 2015).\nHowever, the crypto-market efficiency is found to be time-varying and regu-\nlation sensitive (Nimalendran et al., 2025); as such, some assets are found to be\nefficient at the initial floatation period, say the first six months (Polyzos et al.,\n2024). Past information, such as the trading volume, also fails to predict future\nreturn (Sahoo and Sethi, 2022). These mixed bags of findings not only indicate", "tags": []}
{"fragment_id": "F_R13_p2_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p2:2", "text": "inconsistencies and a gross deviation from the expected patterns of market effi-\nciency but also suggest that cryptocurrencies have the potential to instigate sub-\nstantial market disruptions, particularly around crisis time, such as the COVID-19\n(Mokni et al., 2024).\nThe main objective of this study is to conduct a comprehensive review of\npublished papers to address two specific research questions.First, how is the\nliterature on cryptocurrency efficiency growing? Based on a bibliometric analysis,\non a variety of keywords, it is possible to create a thematic map that captures the\nfundamental concepts surrounding crypto-market efficiency.Second, what rational\nand irrational indicators of market efficiency are most common in cryptocurrency\nliterature? Using a thorough content analysis of crypto-market efficiency literature\nfound in the Scopus database, the study aims to delve deeper into the realms of\nmarket regulation, the engagement of pivotal market actors, and the resolution of\nrelevant concerns to augment the overall efficiency of the market.\nCryptocurrencies, which emerged in 2008 (Klarin, 2020), are digital, encrypted\nforms of currency that function as a means of exchange in lieu of traditional\nphysical currencies. These are results of binary codes that are stored within\ndatabases. In contrast to local currencies, cryptocurrencies are decentralized in\nI. Abdeljawad et al.", "tags": []}
{"fragment_id": "F_R13_p2_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p2:3", "text": "2550021-2\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p3_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p3:1", "text": "nature, thus not issued and monitored by any central authority. Cryptocurrencies\nalso possess characteristics that make them attractive as investment instruments\n(Corbet et al., 2019).\nAs of January 2025, the cryptocurrency market comprises more than 9,000\nactive digital currencies traded across approximately 249 spot exchanges, collec-\ntively possessing a market capitalization of approximately $3.35 trillion.\n1 The\nmarket exhibited significant fluctuations, accompanied by substantial returns for\ncertain cryptocurrencies. As an illustration, the price of a specific cryptocurrency,\nnamely Bitcoin, experienced a notable fluctuation over a span of time. Specifically,\nin October 2014, the closing price of Bitcoin stood at $338.32, which subsequently\nescalated to $96,029 by January 2025.\n2 The substantial observed rate of return has\nraised numerous concerns regarding the efficiency of the cryptocurrency market.\nThis study aims to study that on a global scale.\nIn the subsequent sections, we provide a concise overview of the review\nmethodology. Subsequently, we present findings pertaining to bibliographic con-\ncentrations and the literature pertaining to various facets of the cryptocurrency\nmarket that exert an influence on market efficiency. Considering the extensive\nnumber of references examined, we have included in the reference section only", "tags": []}
{"fragment_id": "F_R13_p3_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p3:2", "text": "those materials that are most pertinent to our study. However, additional references\ncan be obtained upon request in the form of a spreadsheet.\n2. Review Approach\nThe review is conducted in two stages. The initial phase entails conducting a\nbibliometric analysis utilizing the Biblioshiny and VOSviewer packages. The\nprocess of visually representing the depth of a literary work involves several\nsequential stages, commencing with the selection of appropriate keywords for\nconducting comprehensive and targeted literature searches. The search query was\nconducted using the Scopus database, by employing relevant keywords. Keywords\nused in the search process are“cryptocurrency and efficiency”, “bitcoin and effi-\nciency”, “cryptocurrency and momentum ,”“ cryptocurrency and reversal ,”\n“behavioural or behavioural and biases,”“ cryptocurrency and bubbles,” and\n“cryptocurrency and pricing”. This corpus encompasses scholarly works such as\njournal papers, book chapters, and review papers. The papers in question have\nbeen gathered between 2014 and 2024.\n1See the details in https://coinmarketcap.com\n2Price changes can be downloaded from Yahoo Finance at https://finance.yahoo.com/quote/BTC-\nUSD/history?p¼BTC-USD\nCryptocurrency Market Efficiency Revisited\n2550021-3\nJ. Finan. Eng. Downloaded from www.worldscientific.com", "tags": []}
{"fragment_id": "F_R13_p3_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p3:3", "text": "by Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p4_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p4:1", "text": "Following the bibliometric analysis, an integrative approach was conducted, as\ndescribed byTorraco (2005) andSnyder (2019), focusing on a concise selection of\nliterature pertaining to finance, economics, and business. The analysis and eval-\nuation conducted are qualitative in nature. The examination of numerous market\nanomalies presents a complex challenge when conducting a systematic review. To\nresolve this complexity, the reviewed papers were specifically chosen based on\ntheir relevance to the sub-topics and sub-themes under investigation. Therefore, a host\nof research papers on purely technology and engineering-related aspects of FinTech\nwere excluded from review. This integrative review holds significance in initiating an\ninformed discourse regarding the efficiency surrounding cryptocurrency markets.\n3. Market Efficiency and other Relevant Theories\nThe theory of market efficiency, as proposed byFama (1970), has had a significant\nimpact on the field of finance. The financial system’s overall integrity may be\nsubject to scrutiny if one of its components, namely the financial markets\nencompassing cryptocurrencies, exhibits deficiencies. The efficient market hy-\npothesis (EMH) posits that the valuation of an efficient financial asset ought to\nincorporate all pertinent information pertaining to said asset. The EMH faces", "tags": []}
{"fragment_id": "F_R13_p4_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p4:2", "text": "criticisms as it fails to provide a comprehensive explanation for numerous financial\nanomalies that have the potential to impact financial markets (De Bondt and\nThaler, 1985). An increasing quantity of these irregularities is being examined\nwithin the framework of the cryptocurrency markets.\nAdaptive market efficiency (AME) has gained momentum in recent years.\nIt challenges some assumptions of the normative market efficiency theories,\nassuming participants to be rational and qualified to make a quick and accurate\ninvestment decision. AME assumes that investors adapt to changing market\nscenarios, where assets perform non-randomly differently in different scenarios, due\nto competition, imitation and dynamic learning mechanism, challenging the norms\nof traditional market efficiency (Lo, 2005). Due to time-varying and extremely\ncontext-dependent adaptation, assets experience waxing and waning, while the\nperformance of investment strategies differs in different contexts (Lo, 2004; Xiong\net al., 2019). Due to the behavior existing an early-stage investment vehicle, the\nvolatility and differences in performance based on time, type, and environment make\ncryptocurrency market resemble the AME. This review contributes to our limited\nunderstanding of the cogency of the cryptocurrency market, specially of specific", "tags": []}
{"fragment_id": "F_R13_p4_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p4:3", "text": "events, dates, locations, portfolio, and asset types that can be strategically manipu-\nlated to achieve superior market performance.\nCheah and Fry (2015) suggested that Bitcoin intrinsic price is zero, which\ninvalidated discussion of Bitcoin bubbles, as abrupt price swings without a\nI. Abdeljawad et al.\n2550021-4\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p5_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p5:1", "text": "theoretical underpinning cannot justify or address bubbles. To overcome the theory,\nTirole's (1985) anticipation of the consumption loan model, originally suggested by\nSamuelson (1958), explains that fiat money is intrinsically worthless but positively\nvalued for what it fetches in exchange.García-Monle/C19on et al. (2021) forward that\nsince cryptocurrencies are essentially abstract information, Metcalfe’s Law states\nthat the usefulness of cryptocurrencies is the economic worth of the communicated\ninformation via networks.Hayes (2017) used cryptocurrency production network\ncompetitiveness, unit production rate, and mining complexity to calculate Bitcoin’s\nfair value. Bolt and Van Oordt (2019) also examined consumer acceptability and\nadoption of virtual money in merchandising, the current value of virtual currency\ntransactions, and investor forward-looking decisions and expectations of buying\nsuch a currency.\n4. Results of the Bibliometric Review\n4.1. Sources, collaboration, and influential author\nBased on AppendixA, the average annual growth rate of publications is 15.08%,\nwhile the average number of citations per document is 17.75. About a third of the\npublications (29.62%) are collaborated internationally. Most of the documents\nanalyzed are journal papers (82%; 2,646 out of 3,224).\nTable 1 presents that the USA (1448) exhibited the highest level of scholarly", "tags": []}
{"fragment_id": "F_R13_p5_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p5:2", "text": "output on the selected keywords, with China (630) and India (559) following suit.\nNotably, despite China’s banning of the cryptocurrencies, South China Morning\nPost\n3 reported that China had a total transaction of over US$220 billion from June\n2021 until July 2022 and that Binance4 completed an estimated US$90 billion\nworth of transactions in May 2023.\nFigure 1 illustrates the top two literature exhibiting exceptional citation counts.\nFirst, a study conducted byUrquhart (2016) to examine the presence of infor-\nmational inefficiency within the Bitcoin market.Second, extended fromUrquhart\n(2016), Nadarajah and Chu(2017) reported an opposite finding that the Bitcoin\nmarket held the market efficiency hypothesis after changing the power transfor-\nmation of the Bitcoin returns. The time- and context-varying nature of efficiency\nsupports the AME hypothesis.\n3See the report on the South China Morning Post at https://www.scmp.com/tech/policy/article/\n3196781/chinas-cryptocurrency-market-still-among -worlds-strongest-despite-beijings-crackdown-\ntrading-mining#\n4Report on Binance by the Wall Street Journal at https://www.wsj.com/articles/crypto-is-illegal-in-\nchina-binance-does-90-billion-of-business-there-anyway-2a0af975?mod¼business_minor_pos4#\nCryptocurrency Market Efficiency Revisited\n2550021-5\nJ. Finan. Eng. Downloaded from www.worldscientific.com", "tags": []}
{"fragment_id": "F_R13_p5_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p5:3", "text": "by Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p6_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p6:1", "text": "4.2. Growth and sources of scientific productions\nBased on Fig.2, the publication growth exhibits a massive increase since 2018,\nwith the number of papers increasing from 219 in 2018 to 554 in 2024. Figure3\nshows that“Finance Research Letters” has emerged as a highly influential journal,\nhaving published a total of 87 papers, followed by“Management Science” with 65\ncounts of papers.\n4.3. Major themes and keywords\nFigure 4 depicts the interconnectedness of keywords within several distinct con-\nstructs. Table 2 provides additional information that supports the findings on\nkeywords. The keyword that garners the highest search volume is Cryptocurrency\n(395), followed by the terms“behavioural finance” (302), “behavioural biases”\nTable 1. Top 10 countries\n(timespan 2014–2024).\nCountry Freq.\nUSA 1448\nCHINA 630\nINDIA 559\nUK 546\nGERMANY 361\nAUSTRALIA 212\nFRANCE 212\nITALY 190\nCANADA 154\nSPAIN 142\nFig. 1. Most cited global authors.\nI. Abdeljawad et al.\n2550021-6\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p7_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p7:1", "text": "(272), and “Bitcoin” (240). While these keywords are promising, the broader\ncoverage in the literature warrants in-depth studies on efficiency.\nFigure 5 presents a conceptual mapping of the keywords utilizing multiple\ncorrespondence analysis (MCA). The larger dimension that is toward the center of\nthe map at the intersection of or near the dotted lines includes topics that are major\ncontributors to cryptocurrency market. These topics include behavioural biases,\ndecision-making, cryptocurrency, Bitcoin, heuristics, loss aversion, and so on.\nFigure 6 facilitates our comprehension of three prominent themes that are\ncategorized into four distinct quadrants (Cobo et al., 2011). Motor themes play a\nvaluable role in the formulation of novel arguments. These themes encompass a\ncompilation of robust and well-established ideas.“Behavioural finance, biases,\nand overconfidence” are among the most significant motor themes that also\nsubstantiate the aptness of this review. These themes also share quadrants with\nbasic themes. The basic themes, although crucial, often lack in-depth development.\nFig. 3. Most significant publication sources (at least 20 publications).\nFig. 2. Annual scientific production.\nCryptocurrency Market Efficiency Revisited\n2550021-7\nJ. Finan. Eng. Downloaded from www.worldscientific.com", "tags": []}
{"fragment_id": "F_R13_p7_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p7:2", "text": "by Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p8_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p8:1", "text": "Such sharing between two themes/C0 /C0 /C0 motor and basic/C0 /C0 /C0 indicates the strength as\nwell as newness of these ideas that require more research. Additionally, emerging\nand declining themes encompass ideas that are either gaining prominence or losing\nsignificance. A host of these themes also share place with basic themes.\nFig. 4. Network of keywords.\nTable 2. Occurrences of the keywords (minimum of 50 occurrences).\nWords Occurrences Words Occurrences\nCryptocurrency 395 loss aversion 63\nBehavioural finance 302 disposition effect 60\nBehavioural biases 272 heuristics 59\nBitcoin 240 cognitive bias 55\nBehavioural economics 191 prospect theory 55\nMarket efficiency 118 asset pricing 52\nOverconfidence 114 decision-making 52\nBlockchain 109 financial literacy 50\nCOVID-19 69\nI. Abdeljawad et al.\n2550021-8\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p9_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p9:1", "text": "These include contents like“behavioural economics, cognitive biases, and pros-\npect theory.”\nThemes with a high level of density can be classified into two categories: motor\nthemes and niche themes. Among these, the contents of“cryptocurrency, Bitcoin,\nmarket efficiency, and blockchain” stand out as relatively more prominent niches.\nFig. 5. Conceptual structure map using MCA.\nFig. 6. Major themes.\nCryptocurrency Market Efficiency Revisited\n2550021-9\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p10_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p10:1", "text": "5. Cryptocurrency Market (in)Efficiency: General Overview\n5.1. Evidence of inefficiency\nA growing list of studies reported gross violation of the assumptions of market\nefficiency surrounding cryptocurrency markets and assets ( Kristoufek, 2018).\nZhang et al. (2018) conducted a comprehensive examination of nine crypto-\ncurrencies, employing several efficiency tests on both individual cryptocurrencies\nand a value-weighted index of cryptocurrencies. The findings indicated that all\nthe examined cryptocurrency marketplaces exhibited inefficiencies. To a greater\nextent, most cryptocurrencies violate the assumptions of therandom walk hy-\npothesis (Aggarwal, 2019), and as such, the daily returns of crypto assets, such as\nBitcoin, exhibit predictability. Grobys et al. (2020) employed technical trading\nmethods on a sample of 11 cryptocurrencies. Their findings indicated that the\nimplementation of a simple moving average rule resulted in the generation of\nexcess returns.\nSapkota and Grobys (2021) found that cryptocurrencies, such as Bitcoins,\ndemonstrated a discernible state of market equilibrium. This implies that there is\nan absence of any inherent inclination for the market to undergo changes, since\neach buyer can locate a seller at the prevailing price, and conversely, each seller is\nable to find a buyer. The herding behavior observed in bitcoin pricing may be", "tags": []}
{"fragment_id": "F_R13_p10_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p10:2", "text": "attributed to this factor. Vidal-Tom/C19as et al. (2019) conducted an analysis of\nherding behaviorwithin cryptocurrency markets. Their findings revealed instances\nof herding behavior during periods of market decline. Additionally, they observed\nthat smaller cryptocurrencies exhibited herding tendencies toward larger ones.\nHowever, Goczek and Skliarov(2019) reported that the price of Bitcoin exhibited\nheterogeneity in supply and demand dynamics, which might affect market\nliquidity.\n5.2. Evidence of efficiency\nAfter analyzing over 1,000 cryptocurrencies utilizing the low volatility-factor\nmodel, Burggraf and Rudolf(2021) revealed that the efficiency of cryptocurrency\nmarkets surpassed the prevailing assertions in the literature. Apopo and Phiri\n(2021) offered opposing views fromAggarwal (2019), explaining the prevalence\nof random walk behavior in the daily series of the five leading cryptocurrencies.\nLiew et al. (2019) examined the top 100 cryptocurrencies from 2015 to 2018\nand indicated that the cryptocurrency markets exhibit a degree of semi-strong form\nof efficiency in the short-term. On similar notes,Bartos (2015) found that prices of\nBitcoin promptly reacted to the publicly available information, exhibiting signs of\nI. Abdeljawad et al.\n2550021-10\nJ. Finan. Eng. Downloaded from www.worldscientific.com", "tags": []}
{"fragment_id": "F_R13_p10_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p10:3", "text": "by Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p11_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p11:1", "text": "market efficiency. Nevertheless, the outcomes of these studies indicate support\ntoward adaptive market hypothesis more than the EMH.\nZhang et al. (2020) studied Bitcoin, Ethereum, and Litecoin, and reported\nefficiency of the three currencies during bullish periods using hourly returns. Using\nthe price-volume framework,Sahoo and Sethi(2022) examined the performance\nof the leading eight cryptocurrencies between the years 2015 and 2022. They\nconfirmed the inability to predict Bitcoin returns using trading volume, which\nindicated at least weak-form efficiency.Hattori and Ishida (2020) examined the\narbitrage tactics utilized by investors in the spot and futures markets of Bitcoin.\nTheir findings suggest that the spot and futures market for Bitcoin exhibits a\npredominantly efficient nature.\nKyriazis (2019) offered two primary conclusions.First, most of the data suggest\nthat cryptocurrencies exhibit inefficiency.Second, there has been a notable trend\ntoward enhanced efficiency within the realm of cryptocurrencies in recent years.\nThis finding aligns with the research conducted byTran and Leirvik(2020), which\nalso indicates that prior to 2017, the efficiency of Bitcoin marketplaces was pre-\ndominantly deficient. Kang et al. (2021) found a subset (54) of 893 crypto-\ncurrencies to adhere to the weak-form efficiency, and only 24 cryptocurrencies", "tags": []}
{"fragment_id": "F_R13_p11_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p11:2", "text": "were observed to comply with the semi-strong market hypothesis. The time-\nvarying and context-dependent nature of the efficiency in cryptocurrency market\noffers strong support toward the AME hypothesis.\n6. Biases in Cryptocurrency Market\nIt may be observed that a significant proportion of participants in the crypto-\ncurrency markets lack experience, which consequently gives rise to certain psy-\nchological biases (Fonseca et al., 2019). In this section, we discuss the behavioural\nbiases of the cryptocurrency markets.\n6.1. Herding behavior\nVidal-Tom/C19as et al. (2019) noted the potential occurrence of herding behavior in\nperiods of market decline.Ballis and Drakos(2020) conducted an analysis of six\nprominent cryptocurrencies between the years 2015 and 2018, and confirmed a\ntendency to rise in tandem.Akyildirim et al. (2020) found that a contagion effect in\ncryptocurrency markets, where fear and uncertainty increased herding.Shrotryia\nand Kalra (2021) also claimed that herding occurs in the cryptocurrency market\nduring bullish and high volatility.Kumar (2020), however, argued that anti-herding\nbehavior replaced herding in stable or bullish markets.\nCryptocurrency Market Efficiency Revisited\n2550021-11\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p12_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p12:1", "text": "6.2. Co-movement and interdependence\nSeveral studies, includingKatsiampa (2019), Katsiampa et al. (2019), and Qiao\net al. (2020), have found a positive co-movement or interdependency between\ncryptocurrencies. Bouri et al. (2021) found that severe positive or negative events\nhave higher connectivity than average or median conditions.Bouri et al. (2019b)\nfound that when one digital currency’s value fluctuates, another can too. However,\nManahov (2020) found cryptocurrency market co-movement to be dynamic.\nAntonakakis et al. (2019) found that cryptocurrencies’ interdependence or corre-\nlated behavior can vary by 25–75%. Like other markets, the cryptocurrency market\nis highly contagious too (Ferreira and Pereira, 2019).\n6.3. Investor sentiment\nGurdgiev and O’Loughlin (2020) explained how investor sentiment affects cryp-\ntocurrency market herding and anchoring heuristics. Emotions affected Bitcoin\ntrading volume and volatility, causing price fluctuations (Ahn and Kim, 2021).\nInvestors’ optimism about Bitcoin can predict price fluctuations (Anamika et al.,\n2021). Akyildirim et al. (2021) found large cryptocurrencies dominating the\nconnection between attitudes and cryptocurrency returns, which lasted only for\nabout 15 min, making it difficult to exploit these for a meaningful investment\nstrategy. Investors paid more attention to media-covered cryptocurrencies like", "tags": []}
{"fragment_id": "F_R13_p12_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p12:2", "text": "Bitcoin and Ethereum (Gu/C19egan and Renault, 2020; Subramaniam and Chakra-\nborty, 2019).\n6.4. Announcement effect\nJoo et al. (2020) revealed that there is a delay (up to 6 days) in price adjustment\nfollowing the release of significant news events in prominent cryptocurrency\nmarkets, indicating gradual diffusion of information and overreaction to negative\nnews. News sentiment carried relatively higher influence on cryptocurrencies that\nwere younger, smaller in market capitalization, and characterized by higher levels\nof volatility (Anamika and Subramaniam,\n2022).\nUnlike traditional currencies,Rognone et al. (2020) observed that Bitcoin’s value\ndemonstrates an increase in both negative and positive market announcements, in-\ndicating an early adopter enthusiasm among individuals for Bitcoin. Narratives on\nsocial media and search engines also offered measurable influence on the crypto-\ncurrency prices and volatility in the form of social attention (Zhang et al., 2021;\nSabalionis et al., 2020; Azqueta-Gavald/C19on, 2020). Industry-specific negative news,\nsuch as the cyber-attacks or fraudulent activities, negatively impacted the value of\nBitcoin and contributed to its price volatility (Corbet et al., 2020).\nI. Abdeljawad et al.\n2550021-12\nJ. Finan. Eng. Downloaded from www.worldscientific.com", "tags": []}
{"fragment_id": "F_R13_p12_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p12:3", "text": "by Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p13_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p13:1", "text": "6.5. Disposition effect, ambiguity aversion, and gambler’s fallacy\nIn the presence of a disposition effect, investors exhibit a reluctance to divest their\nunderperforming assets, while displaying a greater propensity to sell their profit-\nable assets (Shefrin and Statman, 1985). Bitcoin demonstrated a tendency toward a\npositive disposition impact when experiencing negative market conditions, and\nconversely, a reverse disposition effect during bullish market periods (Haryanto\net al., 2019).\nTo streamline the decision-making process when faced with two bets, investors\nare faced with and should rationally avoid ambiguities (Ellsberg, 1961). Luo et al.\n(2021) suggested that, on average, market participants displayed a tendency to\nambiguity aversion, and when the ambiguity aversion is kept minimum, investors\nhave the potential to achieve anomalous gains.\nDespite the inherent ambiguity of cryptocurrency markets, investors often fall\nvictim to the gambler’s fallacy, as described byRogers (1998). Gemayel and Preda\n(2021) provided empirical evidence that investors who have encountered poor\nrates of trading success or low returns in past transactions tend to augment their\nfuture trading volume.Senarathne (2019) reported a consistent positive connection\nbetween risk and investors’ motivation to invest in Bitcoin while examining\ngambling behavior.\n6.6. Overconfidence and overreaction", "tags": []}
{"fragment_id": "F_R13_p13_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p13:2", "text": "Heuristics can be described as a cognitive process that involves substituting complex\nproblems with simpler ones (Kahneman, 2003). An example of a commonly seen\nphenomenon in bitcoin trading is the presence of a widespread illusion of control\n(Delfabbro et al., 2021), which manifests as an overconfidence heuristic relating to\noverstating someone’s capacity to have an impact.Borgards and Czudaj(2020)\nfound the presence of price overreaction among 12 cryptocurrencies. The instances\nof negative overreactions were more prevalent than those of positive overreactions.\nNonetheless, there is a growing list of heuristics to be explored further, which include\ntake-the-best, recognition, availability, and representativeness.\n7. Other Indicators of Market Inefficiency\n7.1. Cryptocurrency momentum reversals and anomalies\nCommon anomalies like Momentum and Reversal, extensively studied by\nDe Bondt and Thaler(1985), Chan et al. (1996), Jegadeesh (1990), Jegadeesh and\nTitman (1993), andLee and Swaminathan(2000), have challenged the explanatory\npower of the efficient market theory. The concept of momentum suggests that\nCryptocurrency Market Efficiency Revisited\n2550021-13\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p14_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p14:1", "text": "financial assets that have exhibited strong performance in the past are likely to\ncontinue outperforming assets that have performed poorly in subsequent time\nperiods. On the other hand, the concept of reversal proposes the opposite, sug-\ngesting that assets with poor past performance may generate positive abnormal\nreturns compared to assets with strong past performance.\nLee et al. (2020a) argued that speculative motives of cryptocurrency investors\nwho employed a momentum trading strategy in the Bitcoin market during higher\nvolatility and a contrarian strategy during low volatility.\nSeveral studies reported a strong presence of a momentum effect in cryptocurrency\nmarket (Borgards, 2021; Liu and Tsyvinski, 2020). Caporale et al. (2018) examined\nthe concept of price persistence as a measure of momentum of Bitcoin, Litecoin,\nRipple, and Dash cryptocurrencies from 2013 to 2017. The research findings indicate a\ngood association between the historical and future prices of various digital currencies.\nOmane-Adjepong et al. (2019) extended this analysis, including Ethereum, Stellar,\nMonero, and Nem coins and found similar results.Cheng et al. (2019) also confirmed\nthe above results using both Bitcoin and Ethereum.\nLi et al. (2021) argued that maintaining a long position in a cryptocurrency is\nmore probable to yield greater future gains.Nguyen et al. (2020), however, their", "tags": []}
{"fragment_id": "F_R13_p14_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p14:2", "text": "findings did not offer conclusive proof that integrating a momentum-based\ninvestment strategy into cryptocurrency portfolio management can yield positive\nabnormal returns. Caporale and Plastun (2020) indicated otherwise: abnormal\nprofit is possible in cryptocurrency markets using the momenturm strategy.\nKozlowski et al. (2020) studied the reverse effect on a sample of 200 crypto-\ncurrencies, examining their performance across various time durations. The find-\nings verified that the impact endured for 50% of the duration of the sample period.\nZaremba et al. (2021) conducted an analysis on the daily returns of a substantial\nsample size of 36,000 cryptocurrencies that revealed the presence of a robust\nreversal signal.\nShen et al. (2020) employed the three-factor model proposed byFama and\nFrench (1993) on a sample of 1,700 cryptocurrencies and found a higher proba-\nbility of reversal returns for larger cryptocurrencies. Using the size effect (see\nBanz, 1981), Li et al. (2019) found that cryptocurrencies with smaller market\nvalues exhibit a tendency to outperform those with larger market values in sub-\nsequent periods.\nOzdamar et al. (2021) observed that cryptocurrencies exhibiting the most\nsubstantial daily returns throughout the preceding month generally exhibit more\nfavorable anticipated returns. The disparity in average weekly returns amongst", "tags": []}
{"fragment_id": "F_R13_p14_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p14:3", "text": "cryptocurrencies is approximately 3.03%. Even after controlling for other\nvariables such as size, price, and momentum, the observed disparity remains ap-\nproximately 1.99%.\nI. Abdeljawad et al.\n2550021-14\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p15_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p15:1", "text": "Kosc et al. (2019) investigated the top 100 cryptocurrencies based on market\ncapitalization and found that the impact of a contrarian strategy, characterized by the\nact of opposing prevailing market trends, exhibits greater prominence in the short-term\nas compared to the momentum effect and other conventional investment benchmarks.\nGrobys and Sapkota(2019) presented some early challenges against momen-\ntum and reversal-related anomalies. Although the research findings did not provide\nany empirical proof of substantial momentum returns, the prevailing sentiment\namong the majority still favors the inefficiencies of Bitcoin markets.\n7.2. Cryptocurrency seasonality\nSeasonality is a term used to describe the recurrent patterns/C0 /C0 /C0 positive and\nnegative /C0 /C0 /C0 observed in the returns of financial assets. The months of July and\nAugust are deemed unfavorable for cryptocurrency acquisition due to the ob-\nserved data set, which indicates significantly lower returns during these two\nmonths compared to the remaining duration of the year (Plastun et al., 2019).\nLong et al. (2020) indicated that the average returns of cryptocurrencies on the\nsame weekday in the past had a positive predictive effect on future returns. The\nrepatriation of Bitcoin on Mondays is higher compared to other days of the week\n(Aharon and Qadan, 2019; Caporale and Plastun, 2019b; Fraz et al., 2019). The", "tags": []}
{"fragment_id": "F_R13_p15_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p15:2", "text": "values of Bitcoins tend to cluster more frequently around whole numbers on\nFridays compared to the other days of the week (Mbanga, 2018).\nQadan et al. (2022) examined four anomalies, namely calendar patterns, natural\nconditions, holidays coinciding with the closure of the New York Stock Exchange,\nand holidays coinciding with the simultaneous operation of the New York Stock\nExchange and the Chicago Exchanges. They found a favorable correlation\nbetween the occurrence of New Year’s Day and the performance of Ethereum and\nMonero cryptocurrencies, but not for the Chinese New Year’s Day.\nSeveral recent findings align with the concept of AME, indicating that effi-\nciency is not fixed but rather dynamic and subject to change. While the weak form\nof market efficiency cannot be rejected outright (Kaiser, 2019), seasonality patterns\nwere shown to be significant but lacked consistency. Other studies (Khuntia and\nPattanayak, 2021; Shahzad et al., 2022) blamed dynamism and erratic fluctuations\nin cryptocurrency market for seasonality, indicating signs of adaptations.\n7.3. Arbitrage in cryptocurrency market\nArbitrage opportunities arise when cryptocurrency exchanges have large price\ndisparities. A trend in trade volumes across exchanges explains 80% of Bitcoin’s\nreturn, say, in the USA. However, exchange-specific changes affect arbitrage prices\n(Makarov and Schoar, 2020). Lee et al. (", "tags": []}
{"fragment_id": "F_R13_p15_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p15:3", "text": "2020b) found that deviations, or price\nCryptocurrency Market Efficiency Revisited\n2550021-15\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p16_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p16:1", "text": "discrepancies between spot and future values, do occur and become more evident\nduring disturbances like Bitcoin thefts or the introduction of new cryptocurrencies.\nThis exhibit locational arbitrage/C0 /C0 /C0 traders may theoretically acquire Bitcoin at a\ncheaper price in one market and sell it at a higher price in another/C0 /C0 /C0 reaping a\nprofit without risk/C0 /C0 /C0 essentially arbitrage.\n7.4. Information asymmetry\nAnte (2020) used blockchain transparency to study trade volume and knowledge\nasymmetry in large Bitcoin transactions. The study found favorable trading vol-\nume changes linked to 2,132 large Bitcoin transactions between September 2018\nand November 2019. Public dissemination of high-information-asymmetry trans-\nactions reduced abnormal trading activity. Less information asymmetry has\nopposite impacts. The findings show that knowledgeable traders use blockchain\ntransaction data to shape Bitcoin’s market dynamics. This observation suggests\nthat market information is not always consistently absorbed or acted upon. On a\nsimilar note, Alexander et al. (2020) found that institutional investors are better\ninformed, suggesting lower information asymmetry.\n7.5. Market bubbles\nShort-term speculators often cause reasonable bubbles (Wei and Dukes, 2021).\nSpeculative investors and cryptocurrency users may engage more, increasing the", "tags": []}
{"fragment_id": "F_R13_p16_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p16:2", "text": "risk of a bubble. According to Eom (2021), fundamental uncertainty divides\ninvestors. Trading based on significant differences in belief can cause speculative\nbubbles. Dogecoin’s price boom and crash were strongly connected to social\nmedia comments by Elon Musk (Cary, 2021; Shahzad et al., 2022). Researchers\nsuggested using the bubbles carefully to explain the explosive character of the\ncryptocurrency market (Gronwald, 2021). Similar to other assets, crypto valuation\nshould go beyond its financial fundamentals (Abraham, 2019).\nCryptocurrency bubbles can occur when the market value of a currency differs\nfrom its intrinsic worth. Bitcoin and Ethereum prices have shown bubble behavior for\nmultiple periods (Corbet et al., 2018). Bitcoin has been in a bubble since its price rose\nbeyond $1000 (Phillips and Yu, 2011). However, several studies concluded that\nBitcoin bubbles are widespread, but their testing methods did not prove their\npersistence (Geuder et al., 2019; Filimonov and Sornette, 2013; Phillips et al., 2015).\n8. Adaptive Market Efficiency Hypothesis\nThe AME hypothesis combines the EMH and behavioural finance.Ferreira et al.\n(2020) emphasized that investors cannot consistently profit from cryptocurrency\nI. Abdeljawad et al.\n2550021-16\nJ. Finan. Eng. Downloaded from www.worldscientific.com", "tags": []}
{"fragment_id": "F_R13_p16_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p16:3", "text": "by Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p17_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p17:1", "text": "price movements as they are close to the random walk. Due to the dynamic random\nwalk, bitcoin markets are not efficient. Market efficiency changes over time in\ncryptocurrency markets (Noda, 2020; Khuntia and Pattanayak, 2018, 2020). AME\ntheory, a novel version of the EMH (Lo, 2004), explains this dynamic state.\nDynamic adaptation is supported by several studies: byL/C19opez-Martín et al.\n(2021) in Bitcoin, Ripple, Stellar, and Monero cryptocurrencies; and byChu et al.\n(2019) in Ethereum.Keshari Jenaet al. (2020) andKhursheed et al. (2020) ranked\nseveral cryptocurrencies. Ethereum, Ripple, Bitcoin, Dash, and Nem are ranked\nfrom most to least efficient ( Keshari Jena et al., 2020). Bitcoin, Monaro,\nand Litecoin have the longest efficiency periods (Khursheed et al., 2020).\nSentimental investing during extreme market conditions (e.g., the COVID-19\npandemic, cryptocurrency heist) contradicts static market efficiency and is more\nconsistent with the AME (Skwarek, 2025; Aslam et al., 2023; Maghyereh and\nAl-Shboul, 2024; Li et al., 2025). These incidents align with AME, as investors\nstruggle to process information, triggering emotional reactions like panic selling.\nThis dynamic state of inefficiency is influenced not only by external conditions but\nalso by internal ones, such as liquidity and volatility, further supporting the AME\nframework (Polyzos et al., 2024).", "tags": []}
{"fragment_id": "F_R13_p17_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p17:2", "text": "However, due to a nonlinear structure, the AME in cryptocurrency markets is\nmore complex than in traditional financial markets. The absence of a central clearing\nmechanism makes cryptocurrency markets more dependent on their network\nstructure, leading to complex, nonlinear dynamic adaptive dynamics (Zhu et al.,\n2025). While linear and nonlinear behaviors may appear similar, they diverge during\ncertain periods, such as the early 2018 Bitcoin breakdown (Bundi and Wildi, 2019).\nFurthermore, adaptive efficiency behavior may vary across cryptocurrencies. While\nthe market efficiency of Bitcoin and Ripple evolves over time, Ethereum tends to\nmove toward a more inefficient state (Ghazani and Jafari, 2021).\nWhile the AME remains more applicable than the EMH due to the frequent\noccurrences of price bubbles across different time scales (Alvarez-Ramirez et al.,\n2018), critiques of AME reject this notion (Jiang et al., 2018) as they did not find\nevidence of improvements in Bitcoin’s efficiency over time.\n9. What Contributes to (in)Efficiency?\nConvergence occurs when an efficient market prevents simultaneous trading of a\nfinancial instrument at two prices, reducing arbitrage opportunities.Apergis et al.\n(2021) found that market microstructure drives the convergence of eight crypto-\ncurrencies. Among other factors, range volatility, market capitalization, and min-", "tags": []}
{"fragment_id": "F_R13_p17_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p17:3", "text": "ing fees increase convergence due to price uncertainty, but information and news\ntrade volume, and network addresses failed to contribute to convergence.\nCryptocurrency Market Efficiency Revisited\n2550021-17\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p18_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p18:1", "text": "Contrary to trade volume, Güleç and Aktaş (2019) found that market volume\nincreases efficiency. Trading volume also positively correlated with speculative\ncryptocurrency booms (Eom, 2021).\nMarket capitalization plays an efficiency function by lowering momentum impact\nin cryptocurrency marketplaces (Jia et al., 2022; Liu et al., 2020). K€ochling et al.\n(2019) found high correlations between bid-ask spreads, market cap, and weak-form\ncryptocurrency market efficiency. A highly liquid, low-volatility cryptocurrency\nmarket is efficient (Al-Yahyaeeet al., 2020). Dong et al. (2022) explained that a fall in\ncryptocurrency liquidity increases anomalous returns, inversely affecting market ef-\nficiency.Wei(2018) confirmed that liquidity is a key factor in market efficiency, while\nFakhfekh and Jeribi(2020) justified the volatility result by arguing that increased\nvolatility may cause uninformed investors to herd. Also, volatility (trading volume)\nincreases (decreases) herding behavior in the cryptocurrency market (Youssef, 2020).\nLow-priced cryptocurrencies are also more volatile than high-priced ones (Aloosh and\nOuzan, 2020), making efficient cryptocurrencies more costly.\nAlthough low volatility improves market efficiency, high volatility can improve\nit, especially during pre- and post-crash periods (Yaya et al., 2020). Diaconaşu", "tags": []}
{"fragment_id": "F_R13_p18_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p18:2", "text": "et al. (2022) also found that pandemics boost cryptocurrency efficiency.Wang and\nWang (2021) analyzed the COVID-19 pandemic using Entropy-based analysis and\nfound that Bitcoin markets are more robust and less inefficient than stock markets.\nThese traits made it a safe haven.Naeem et al. (2021) used asymmetric multi-\nfractal detrended fluctuation (MF-DFA) and time-varying deficit on 1-h Bitcoin,\nEthereum, Litecoin, and Ripple data. Bitcoin and Ethereum were the least efficient\nof the four cryptocurrencies due to the COVID-19 epidemic. The COVID-19\npandemic had a greater impact on bitcoin market efficiency than bubble episodes\nin late 2017, early 2018, and July 2020 (Montasser et al., 2022).\nCaporale and Plastun(2019a) revealed that transaction costs (mining fees) may\nreduce profit potential that drives investors to overbuy or oversell a coin due to fear or\ngreed. Leveraged trading affects cryptocurrency market efficiency (Strych, 2022). Also,\nmargin trading and short selling diminish market pricing efficiency. Financial deriva-\ntives may improve bitcoin market informationefficiency. The Bitcoin futures increased\nthe spot price’s information efficiency (Shynkevich, 2020). Cryptocurrencies have no\nfundamental worth, except for their utility as a tool for exchange. Speculative bubbles\noccur when investors overvalue or undervalue cryptocurrencies (Latif et al., 2017).", "tags": []}
{"fragment_id": "F_R13_p18_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p18:3", "text": "10. Decentralized Finance and Non-Fungible Tokens\nCryptocurrencies are often designed around decentralized principles, aiming to\neliminate the need for central intermediaries. They are traded in decentralized\nexchanges (DEXs) like Uniswap-V2, but they can also be traded in centralized\nI. Abdeljawad et al.\n2550021-18\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p19_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p19:1", "text": "exchanges (CEXs) like Binance. Almeida et al. (2025) found that Ethereum-\nBitcoin pair exhibits a superior level of weak form efficiency in DEXs over CEXs.\nTherefore, DeFi enhances cryptocurrency market efficiency. This is driven by\nDEX’s inherent transparency, facilitation of arbitrage, global accessibility,\nalgorithmic trading, fewer regulatory hurdles, and rapid adaptation to market\nconditions.\nIn contrast, Momtaz (2024) agreed that decentralized finance (DeFi) aims to\nenhance cryptocurrency market efficiency by reducing intermediation and trans-\naction costs. However, in practice, DeFi exhibits significant inefficiencies due to\nsearch frictions driven by market granularity, asymmetric information, moral\nhazard in signaling, and cyber risks. While transactions are technically efficient,\ndistributed ledger technology (DLT) and asset tokenization lead to highly granular\nmarkets, increasing search intensity and effort required to find optimal matches.\nHigh information asymmetry can cause under- or overinvestment, while moral\nhazard signaling can facilitate fraudulent signaling. Additionally, most users lack\nthe expertise to detect cyber-attacks, further undermining efficiency. Consequently,\na certain degree of centralization, such as intermediation by crypto funds, is often\nnecessary for efficient functioning within the cryptocurrency ecosystem, indicating", "tags": []}
{"fragment_id": "F_R13_p19_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p19:2", "text": "that perfectly DeFi may not be optimal for market efficiency.\nThe relationship between DeFi and cryptocurrency market efficiency is multi-\nfaceted. Makridis et al. (2023) observed that DEXs experience the front-running\nphenomenon, where some transactions are executed ahead of others to secure risk-\nfree profits. Their design also exposes users to front-running cyber-attacks.\nAdditionally, mechanisms like “airdrops”, while driving growth, can increase\nvolatility when “yield farmers” cash out tokens rapidly after price surges. These\nfactors hinder efficiency. On the other hand, DEXs reduce information asymmetry\nthrough integral smart contracts. Notably, growth does not appear to stem from\nspeculation or regulatory arbitrage (e.g., hiding illicit gains). These features sug-\ngest that while certain aspects in DEXs hinder efficiency, others promote it.\nCryptocurrencies like Bitcoin and Ethereum are fungible tokens (FTs), while non-\nfungible tokens (NFTs) represent unique, non-fungible assets that allow for the\ncreation, registration, and transfer of ownership.Okorie et al. (2024) find that both\nFTs and NFTs exhibit AME, with increased inefficiency during the COVID-19\npandemic. However, NFTs demonstrate greater resilience and improved efficiency\nduring Russia–Ukraine conflict. Similarly,Chowdhury et al. (2023) observe that", "tags": []}
{"fragment_id": "F_R13_p19_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p19:3", "text": "DeFi-based FTs are more efficient than centralized FTs, and NFTs are generally more\nefficient than FTs. Despite this, inefficiencies persist.Tekin (2025) notes that NFT\nmarkets remain sentiment-driven, influenced by behavioural biases such as the\nbandwagon effect, herding, and speculative bubbles and hype.Benedek and Nagy\n(2025) highlight the role of information asymmetries influencing NFTs.\nCryptocurrency Market Efficiency Revisited\n2550021-19\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p20_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p20:1", "text": "Although NFT markets are more mature than FT markets, NFT prices are unidirec-\ntionally caused by FTs prices (Apostuet al., 2022).Christopher Westland(2024)s h o w e d\nthat Bitcoin and Ether pricesare strongly correlated with NFT prices and volume due to\nhedging strategies, sentiment, and holdings transfer and transaction dynamics.\n11. Regulatory Frameworks\nCryptocurrencies, due to inherently weak regulatory frameworks and limited\nnformation disclosures, are highly influenced by psychological and sociological\nfactors (Naeem et al., 2021). Regulations, often perceived as uncertain events and\nbad news, tend to increase market volatility, affecting prices, liquidity, and returns,\nthereby reducing market efficiency. However, during periods of heightened\ninvestor greed, regulation can enhance efficiency by curbing excessive sentiment.\nNotably, the impact of regulatory policies on volatility diminishes during crises or\nepidemics, as investors view cryptocurrencies as safe-haven assets regardless of\nregulatory actions. These actions often include trading bans, risk warnings, and\nrestrictions on virtual currency mining (Zhang et al., 2023). Tight and strict\nregulations may reduce cryptocurrency market efficiency by hindering efficient\ninformation dissemination and increasing price distortion and uncertainty (Yi", "tags": []}
{"fragment_id": "F_R13_p20_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p20:2", "text": "et al., 2023). In contrast, Bouteska et al. (2025) demonstrated that introducing\nregulatory reforms in cryptocurrency markets can enhance efficiency by reducing\nregulatory uncertainty, increasing market trust, improving liquidity, and providing\nclearer guidelines for market participants. Examples of such reforms include tax\nclarity, anti-money laundering (AML) and know-your-customer (KYC) compli-\nance, transparent auditing, governance standards, liquidity pooling, smart contract\nregulation, improved scalability and interoperability, information disclosure, and\nsupport for institutional investment tools and central bank digital currencies.\nMeanwhile, some studies report neutral effects. For instance, Feinstein and\nWerbach (2021) found almost entirely null results regarding the impact of regu-\nlatory actions, such as securities classifications AML and anti-fraud enforcement,\nand bespoke licensing, on trading dynamics or overall market participation.\n12. Implications for Cryptocurrency Investors and Risk Managers\nEssentially, the most important question regarding crypto asset investment surrounds\nits extreme volatility and lack of regulation to protect the interest of the investors.\nNimalendranet al. (2025)found that regulation and compliance, at this initial stage of\nmarket development, can significantly boost public confidence, leading to an efficient", "tags": []}
{"fragment_id": "F_R13_p20_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p20:3", "text": "cryptocurrency market. Additionally, assets voluntarily following regulations can\nachieve efficiency almost similar to the government-regulated assets.\nI. Abdeljawad et al.\n2550021-20\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p21_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p21:1", "text": "Technical indicators related to Blockchain offer traders valuable insights\n(Lahmiri et al., 2021). Technical trading tactics were more profitable than buy-\nand-hold, protecting against large losses (Hudson and Urquhart, 2019). However,\nsimplistic technical trading may not be as profitable as a buy-and-hold strategy\n(Ahmed et al., 2020). The 20-day moving average trading method (Grobys et al.,\n2020) is successful. The use of candlestick patterns is found to be ineffective and\nmisleading for cryptocurrency trading (Ho et al., 2021). Every minute counts in\ncrypto trading. Traders who trade every 1 or 60 min can generate an abnormal\nreturn (Aslan and Sensoy, 2020).\nCryptocurrency investment is significant for risk managers for its volatility,\ncapacity to mitigate idiosyncratic risk and effect price volatility (Al Guindy, 2021;\nSubramaniam and Chakraborty, 2019; Yao et al., 2021; Zhu et al., 2021; Kim,\n2022). Specialized technical indicators can be developed to capitalize on extreme\nmarket conditions (Chan et al., 2022). Traders’ misinterpretation could lead to\nilliquid cryptocurrency markets and dramatic price volatility. Combining trading\nindicators, like moving averages, with social factors like user sentiment, improves\ncryptocurrency price prediction by over 50% (Ortu et al., 2022).\nLucey et al. (2022) found that professional investors react to policy changes,", "tags": []}
{"fragment_id": "F_R13_p21_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p21:2", "text": "while amateur investors react to general media hype. To better understand investor\nconduct in turbulent cryptocurrency markets, they created Cryptocurrency\nUncertainty Indexes that capture these two sorts of uncertainties. Addressing these\nuncertainties will generate higher returns. Investors should also be aware of the\nherding behavior as it may reduce crypto investment return and may expose\ncrypto-only investors to risk (Bouri et al., 2019a). For correct risk assessment,\nforecasting, and strategic portfolio building, investors and portfolio managers\nshould include the long-memory behavior of cryptocurrencies under the Fractal\nMarkets Theory (Assaf et al., 2022).\nThe Three-Factor Model comprises the cryptocurrency market, size, and\nmomentum. The model can predict returns on crypto assets (Liu et al., 2022). They\nidentified ten 10 cryptocurrency traits that were explained well by the three-factor\nmodel. An extension to the four-factor model by adding a contagion risk factor to\nFama–French’s three-factor model was found to outperform both the crypto-\ncurrency-CAPM and the original three-factor models (Shahzad et al., 2021).\n13. Conclusion\nThis review investigates published literature on efficiency in and drivers of the\ncryptocurrency market. Based on the bibliometric review, China dominates the\nbitcoin market and efficiency publications. Bitcoin is the most studied crypto-", "tags": []}
{"fragment_id": "F_R13_p21_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p21:3", "text": "currency. The systematic review suggests that cryptocurrency markets are inefficient\nCryptocurrency Market Efficiency Revisited\n2550021-21\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p22_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p22:1", "text": "and easier to spot. Despite claims of market efficiency, they were rare, and outcomes\nvaried by sample. Opponents of cryptocurrency market efficiency found emotions,\nherding, co-movements, explosivity, disposition effect, size effect, heuristics,\nambiguity, gambler’s fallacy, momentum, reversal, seasonality, and market bubbles\nto support the presence of market inefficiency. Controlling for Bitcoin market effi-\nciency drivers, such the range volatility, market capitalization, mining fees, trading\nvolume, and network addresses, helps minimize market inefficiency.\nMajor implications are as follows. First, stock exchange regulators should\nregulate cryptocurrency markets to safeguard investors from manipulation and\nfraud. They should strengthen market cybersecurity to mitigate security breaches\nand heists. Regulators should require clear and accessible disclosures from token\nissuers to reduce information asymmetry, moral hazard signals and behavioural\nbiases like herding. Perfectly DeFi-based cryptocurrency is not optimal for market\nefficiency, and some centralizing is beneficial such as establishing centralized\nclearing houses for cryptocurrencies or further support establishing and investing\nin crypto funds. Furthermore, regulatory interventions should be timed based on\nmarket sentiment /C0 /C0 /C0 tightening oversight when investor greed is high to reduce", "tags": []}
{"fragment_id": "F_R13_p22_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p22:2", "text": "speculation, and avoiding hasty intervention during panic to prevent further vol-\natility. Policymakers can implement sentiment index-based warning systems, such\nas the Crypto Fear and Greed Index (FGI), this can support timely and effective\nregulatory responses.\nSecond, despite the lack of robustness, technical investment tactics might give\nspeculators abnormal returns. Third, to avoid arbitrageurs, speculators, and\nfinancial “criminals,” investors should choose cryptocurrencies with efficiency\nboosters. Fourth, diversification may reduce unsystematic risk, but bitcoin port-\nfolio managers should be wary of strong cryptocurrency co-movement. Further-\nmore, investors should adopt adaptive strategies to crypto market inefficiencies and\napply active trading approaches. They should also assume that, despite differences\nin market inefficiency strength between DeFi-based cryptocurrencies and\ncentralized ones, or between FTs and NFTs assets. All remain inefficient and\nrequire similar technical investment tactics. Finally, academics should not assume\ncryptocurrency markets behave like stock markets or other financial markets.\nFurthermore, it seems that the literature regarding comparisons between DeFi-\nbased and centralized-based cryptocurrencies, as well as FTs and NFTs in the\ncontext of market efficiency, are relatively few. Therefore, more fundamental", "tags": []}
{"fragment_id": "F_R13_p22_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p22:3", "text": "investigations should be conducted on relevant topics.\nThis paper provides an overview and establishes a foundation for examining the\nefficiency of cryptocurrency marketplaces. Nevertheless, it is important to\nacknowledge that the realm of cryptocurrency is vast and dynamic, which has im-\nposed certain constraints on the scope of our study. The current landscape of\nI. Abdeljawad et al.\n2550021-22\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p23_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p23:1", "text": "cryptocurrencies encompasses a vast array of thousands of digital currencies.\nHowever, the body of published academic literature pertaining to market efficiency in\nthis domain is limited in scope, since it addresses just a fraction of the whole cryp-\ntocurrency population. Furthermore, this study employs a representativeness\nframework, hence necessitating the acknowledgment that the findings pertaining to a\nspecific cryptocurrency should not be extrapolated to other cryptocurrencies.\nFurthermore, while there exists a sufficient body of research examining the promi-\nnent abnormalities within cryptocurrency markets, numerous behavioural phe-\nnomena remain unexplored. Hence, it is recommended that forthcoming researchers\nbroaden the scope of their investigations by encompassing a wider range of cryp-\ntocurrencies and incorporating other behavioural phenomena. Furthermore, this\nstudy is an initial attempt to examine the concept of market efficiency in the realm of\nbitcoin. Subsequent researchers may consider broadening the scope of this investi-\ngation to encompass a more comprehensive analysis of this subject matter.\nAppendix A: Key Statistics of the Literature Considered for\nBibliometric review\nDescription Information/Freq.\nCore information\nTimespan 2014:2024\nSources (Journals, Books, etc.) 1177\nDocuments 3224\nAnnual Growth Rate % 15.08\nDocument Average Age 4.53", "tags": []}
{"fragment_id": "F_R13_p23_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p23:2", "text": "Average citations per doc 17.75\nContents\nKeywords Plus (ID) 4440\nAuthor’s Keywords (DE) 7974\nAuthors\nAuthors 6831\nAuthors of single-authored docs 565\nCollaboration\nSingle-authored docs 621\nCoauthors per Doc 2.66\nInternational co-authorships % 29.62\nDocument types\nPaper 2646\nBook chapter 238\nConference paper 161\nCryptocurrency Market Efficiency Revisited\n2550021-23\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p24_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p24:1", "text": "ORCID\nIslam Abdeljawad\n https://orcid.org/0000-0003-2625-698X\nAhmad Tina\n https://orcid.org/0009-0004-7668-1524\nM. Kabir Hassan\n https://orcid.org/0000-0001-6274-3545\nMamunur Rashid\n https://orcid.org/0000-0002-6688-5740\nReferences\nAbraham, M (2019). Studying the patterns and long-run dynamics in cryptocurrency\nprices, Journal of Corporate Accounting & Finance, 31(3), 98–113, https://doi.org/\n10.1002/jcaf.22427.\nAggarwal, D (2019). Do bitcoins follow a random walk model?Research in Economics,\n73(1), 15–22, https://doi.org/10.1016/j.rie.2019.01.002.\nAharon, DY and M Qadan (2019). Bitcoin and the day-of-the-week effect, Finance\nResearch Letters, 31, https://doi.org/10.1016/j.frl.2018.12.004.\nAhmed, S, K Grobys and N Sapkota (2020). Profitability of technical trading rules among\ncryptocurrencies with privacy function, Finance Research Letters, 35, 101495,\nhttps://doi.org/10.1016/j.frl.2020.101495.\nAhn, Y and D Kim (2021). Emotional trading in the cryptocurrency market,Finance\nResearch Letters, 42, 101912, https://doi.org/10.1016/j.frl.2020.101912.\n(Continued )\nDescription Information/Freq.\nBook 48\nReview 131\nNotes: Scopus search specifications: (TITLE-ABS-KEY\n(cryptocurrency AND efficiency) OR TITLE-ABS-KEY\n(bitcoin AND efficiency) OR TITLE-ABS-KEY (crypto-\ncurrency AND momentum) OR TITLE-ABS-KEY (cryp-\ntocurrency AND reversal) OR TITLE-ABS-KEY", "tags": []}
{"fragment_id": "F_R13_p24_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p24:2", "text": "(behavioral OR behavioural AND biases) OR TITLE-ABS-\nKEY (cryptocurrency AND bubbles) OR TITLE-ABS-KEY\n(cryptocurrency AND pricing)) AND PUBYEAR> 2013\nAND PUBYEAR < 2025 AND (LIMIT-TO (SUBJAREA,\n“ECON”) OR LIMIT-TO (SUBJAREA, “BUSI”)) AND\n(LIMIT-TO (DOCTYPE,“ar”) OR LIMIT-TO (DOCTYPE,\n“cp”) OR LIMIT-TO (DOCTYPE, “ch”) OR LIMIT-TO\n(DOCTYPE, “re”) OR LIMIT-TO (DOCTYPE, “bk”))\nAND (LIMIT-TO (LANGUAGE,“English”)).\nI. Abdeljawad et al.\n2550021-24\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p25_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p25:1", "text": "Akyildirim, E, AF Aysan, O Cepni and SPC Darendeli (2021). Do investor sentiments\ndrive cryptocurrency prices? Economics Letters , 206, 109980, https://doi.org/\n10.1016/j.econlet.2021.109980.\nAkyildirim, E, S Corbet, B Lucey, A Sensoy and L Yarovaya (2020). The relationship\nbetween implied volatility and cryptocurrency returns,Finance Research Letters, 33,\n101212, https://doi.org/10.1016/j.frl.2019.06.010.\nAlexander, C, J Choi, HR Massie and S Sohn (2020). Price discovery and microstructure\nin ether spot and derivative markets,International Review of Financial Analysis, 71,\n101506, https://doi.org/10.1016/j.irfa.2020.101506.\nAl Guindy, M (2021). Cryptocurrency price volatility and investor attention,International\nReview of Economics & Finance, 76, 556–570, https://doi.org/10.1016/j.iref.2021.\n06.007.\nAlmeida, LM, MS Perlin and FM Müller (2025). Pricing efficiency in cryptocurrencies:\nThe case of centralized and decentralized markets,Journal of Economics and Busi-\nness, 133, 106224, https://doi.org/10.1016/j.jeconbus.2024.106224.\nAloosh, A and S Ouzan (2020). The psychology of cryptocurrency prices,Finance Re-\nsearch Letters, 33, 101192, https://doi.org/10.1016/j.frl.2019.05.010.\nAlvarez-Ramirez, J, E Rodriguez and C Ibarra-Valdez (2018). Long-range correlations and\nasymmetry in the Bitcoin market,Physica A: Statistical Mechanics and its Appli-", "tags": []}
{"fragment_id": "F_R13_p25_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p25:2", "text": "cations, 492, 948–955, https://doi.org/10.1016/j.physa.2017.11.025.\nAl-Yahyaee, KH, W Mensi, HU Ko, SM Yoon and SH Kang (2020). Why cryptocurrency\nmarkets are inefficient: The impact of liquidity and volatility,The North American\nJournal of Economics and Finance, 52, 101168, https://doi.org/10.1016/j.najef.2020.\n101168.\nAnamika, M Chakraborty and S Subramaniam (2021). Does sentiment impact crypto-\ncurrency? Journal of Behavioral Finance, 24(2), 202–218, https://doi.org/10.1080/\n15427560.2021.1950723.\nAnamika, A and S Subramaniam (2022). Do news headlines matter in the cryptocurrency\nmarket? Applied Economics, 54(54), 6322–6338, https://doi.org/10.1080/00036846.\n2022.2061904.\nAnte, L (2020). Bitcoin transactions, information asymmetry and trading volume,Quan-\ntitative Finance and Economics, 4(3), 365–381, https://doi.org/10.3934/qfe.2020017.\nAntonakakis, N, I Chatziantoniou and D Gabauer (2019). Cryptocurrency market conta-\ngion: Market uncertainty, market complexity, and dynamic portfolios,Journal of\nInternational Financial Markets, Institutions and Money, 61, 37–51, https://doi.org/\n10.1016/j.intfin.2019.02.003.\nApergis, N, D Koutmos and JE Payne (2021). Convergence in cryptocurrency prices? The\nrole of market microstructure,Finance Research Letters, 40, 101685, https://doi.org/\n10.1016/j.frl.2020.101685.", "tags": []}
{"fragment_id": "F_R13_p25_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p25:3", "text": "Apopo, N and A Phiri (2021). On the (in)efficiency of cryptocurrencies: Have they taken\ndaily or weekly random walks? Heliyon, 7(4), e06685, https://doi.org/10.1016/j.\nheliyon.2021.e06685.\nCryptocurrency Market Efficiency Revisited\n2550021-25\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p26_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p26:1", "text": "Apostu, SA, M Panait, L Vasa, C Mihaescu and Z Dobrowolski (2022). NFTs and crypto-\ncurrencies /C0 /C0 /C0 the metamorphosis of the economy under the sign of blockchain: A time\nseries approach,Mathematics, 10(17), 3218, https://doi.org/10.3390/math10173218.\nAslam, F, BA Memon, AI Hunjra and E Bouri (2023). The dynamics of market efficiency\nof major cryptocurrencies, Global Finance Journal, 58, 100899, https://doi.org/\n10.1016/j.gfj.2023.100899.\nAslan, A and A Sensoy (2020). Intraday efficiency-frequency nexus in the cryptocurrency\nmarkets, Finance Research Letters, 35, 101298, https://doi.org/10.1016/j.frl.2019.\n09.013.\nAssaf, A, A Bhandari, H Charif and E Demir (2022). Multivariate long memory structure\nin the cryptocurrency market: The impact of COVID-19,International Review of\nFinancial Analysis, 82, 102132, https://doi.org/10.1016/j.irfa.2022.102132.\nAzqueta-Gavald/C19on, A (2020). Causal inference between cryptocurrency narratives and\nprices: Evidence from a complex dynamic ecosystem, Physica A: Statistical\nMechanics and Its Applications, 537, 122574, https://doi.org/10.1016/j.physa.2019.\n122574.\nBallis, A and K Drakos (2020). Testing for herding in the cryptocurrency market,Finance\nResearch Letters, 33, 101210, https://doi.org/10.1016/j.frl.2019.06.008.\nBanz, RW (1981). The relationship between return and market value of common stocks,", "tags": []}
{"fragment_id": "F_R13_p26_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p26:2", "text": "Journal of Financial Economics, 9(1), 3–18, https://doi.org/10.1016/0304-405x(81)\n90018-0.\nBartos, J (2015). Does Bitcoin follow the hypothesis of efficient market?International\nJournal of Economic Sciences , IV(2), 10–23, https://doi.org/10.20472/es.2015.4.\n2.002.\nBenedek, B and BZ Nagy (2025). Asymmetries in factors influencing non-fungible tokens’\n(NFTs) returns,Financial Innovation, 11(1), 28, https://doi.org/10.1186/s40854-024-\n00672-w.\nBolt, W and MR Van Oordt (2019). On the value of virtual currencies,Journal of Money,\nCredit and Banking, 52(4), 835–862, https://doi.org/10.1111/jmcb.12619.\nBorgards, O (2021). Dynamic time series momentum of cryptocurrencies, The North\nAmerican Journal of Economics and Finance, 57, 101428, https://doi.org/10.1016/j.\nnajef.2021.101428.\nBorgards, O and RL Czudaj (2020). The prevalence of price overreactions in the cryp-\ntocurrency market, Journal of International Financial Markets, Institutions and\nMoney, 65, 101194, https://doi.org/10.1016/j.intfin.2020.101194.\nBouri, E, R Gupta and D Roubaud (2019a). Herding behaviour in cryptocurrencies,\nFinance Research Letters, 29, 216–221, https://doi.org/10.1016/j.frl.2018.07.008.\nBouri, E, T Saeed, XV Vo and D Roubaud (2021). Quantile connectedness in the cryp-\ntocurrency market, Journal of International Financial Markets, Institutions and", "tags": []}
{"fragment_id": "F_R13_p26_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p26:3", "text": "Money, 71, 101302, https://doi.org/10.1016/j.intfin.2021.101302.\nBouri, E, SJH Shahzad and D Roubaud (2019b). Co-explosivity in the cryptocurrency\nmarket, Finance Research Letters, 29, 178–183, https://doi.org/10.1016/j.frl.2018\n.07.005.\nI. Abdeljawad et al.\n2550021-26\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p27_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p27:1", "text": "Bouteska, A, T Sharif, L Isskandarani and MZ Abedin (2025). Market efficiency and its\ndeterminants: Macro-level dynamics and micro-level characteristics of crypto-\ncurrencies, International Review of Economics & Finance, 98, 103938, https://doi.\norg/10.1016/j.iref.2025.103938.\nBundi, N and M Wildi (2019). Bitcoin and market-(in)efficiency: A systematic time series\napproach, Digital Finance, 1(1), 47–65, https://doi.org/10.1007/s42521-019-00004-z.\nBurggraf, T and M Rudolf (2021). Cryptocurrencies and the low volatility anomaly,Fi-\nnance Research Letters, 40, 101683, https://doi.org/10.1016/j.frl.2020.101683.\nCaporale, GM, L Gil-Alana and A Plastun (2018). Persistence in the cryptocurrency\nmarket, Research in International Business and Finance, 46, 141–148, https://doi.\norg/10.1016/j.ribaf.2018.01.002.\nCaporale, GM and A Plastun (2019a). Price overreactions in the cryptocurrency market,\nJournal of Economic Studies, 46(5), 1137–1155, https://doi.org/10.1108/jes-09-2018-\n0310.\nCaporale, GM and A Plastun (2019b). The day of the week effect in the cryptocurrency\nmarket, Finance Research Letters, 31, https://doi.org/10.1016/j.frl.2018.11.012.\nCaporale, GM and A Plastun (2020). Momentum effects in the cryptocurrency market after\none-day abnormal returns,Financial Markets and Portfolio Management, 34(3), 251–\n266, https://doi.org/10.1007/s11408-020-00357-1.", "tags": []}
{"fragment_id": "F_R13_p27_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p27:2", "text": "Cary, M (2021). Down with the #Dogefather: Evidence of a cryptocurrency responding in\nreal time to a crypto-tastemaker, Journal of Theoretical and Applied Electronic\nCommerce Research, 16(6), 2230–2240, https://doi.org/10.3390/jtaer16060123.\nChan, LKC, N Jegadeesh and J Lakonishok (1996). Momentum strategies,The Journal of\nFinance, 51(5), 1681–1713, https://doi.org/10.1111/j.1540-6261.1996.tb05222.x.\nChan, S, J Chu, Y Zhang and S Nadarajah (2022). An extreme value analysis of the tail\nrelationships between returns and volumes for high frequency cryptocurrencies,\nResearch in International Business and Finance, 59, 101541, https://doi.org/10.1016/\nj.ribaf.2021.101541.\nCheah, ET and J Fry (2015). Speculative bubbles in Bitcoin markets? An empirical\ninvestigation into the fundamental value of Bitcoin,Economics Letters, 130, 32–36,\nhttps://doi.org/10.1016/j.econlet.2015.02.029.\nCheng, Q, X Liu and X Zhu (2019). Cryptocurrency momentum effect: DFA and MF-DFA\nanalysis, Physica A: Statistical Mechanics and Its Applications, 526, 120847, https://\ndoi.org/10.1016/j.physa.2019.04.083.\nChowdhury, MAF, M Abdullah, M Alam, MZ Abedin and B Shi (2023). NFTs, DeFi, and\nother assets efficiency and volatility dynamics: An asymmetric multifractality anal-\nysis, International Review of Financial Analysis, 87, 102642, https://doi.org/10.1016/\nj.irfa.2023.102642.", "tags": []}
{"fragment_id": "F_R13_p27_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p27:3", "text": "Chu, J, Y Zhang and S Chan (2019). The adaptive market hypothesis in the high frequency\ncryptocurrency market, International Review of Financial Analysis, 64, 221–231,\nhttps://doi.org/10.1016/j.irfa.2019.05.008.\nCobo, M, A L/C19opez-Herrera, E Herrera-Viedma and F Herrera (2011). An approach for\ndetecting, quantifying, and visualizing the evolution of a research field: A practical\nCryptocurrency Market Efficiency Revisited\n2550021-27\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p28_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p28:1", "text": "application to the Fuzzy Sets Theory field,Journal of Informetrics, 5(1), 146–166,\nhttps://doi.org/10.1016/j.joi.2010.10.002.\nCorbet, S, DJ Cumming, BM Lucey, M Peat and SA Vigne (2020). The destabilising\neffects of cryptocurrency cybercriminality,Economics Letters, 191, 108741, https://\ndoi.org/10.1016/j.econlet.2019.108741.\nCorbet, S, B Lucey, A Urquhart and L Yarovaya (2019). Cryptocurrencies as a financial\nasset: A systematic analysis,International Review of Financial Analysis, 62, 182–\n199, https://doi.org/10.1016/j.irfa.2018.09.003.\nCorbet, S, B Lucey and L Yarovaya (2018). Datestamping the Bitcoin and Ethereum\nbubbles, Finance Research Letters, 26, 81–88, https://doi.org/10.1016/j.frl.2017.12.\n006.\nChristopher Westland, J (2024). Periodicity, Elliott waves, and fractals in the NFT market,\nScientific Reports, 14(1), 4480, https://doi.org/10.1038/s41598-024-55011-x.\nDe Bondt, WFM and R Thaler (1985). Does the stock market overreact?The Journal of\nFinance, 40(3), 793–805, https://doi.org/10.1111/j.1540-6261.1985.tb05004.x.\nDelfabbro, P, DL King and J Williams (2021). The psychology of cryptocurrency trading:\nRisk and protective factors, Journal of Behavioral Addictions, 10(2), 201–207,\nhttps://doi.org/10.1556/2006.2021.00037.\nDiaconaşu, DE, S Mehdian and O Stoica (2022). An analysis of investors’ behavior in", "tags": []}
{"fragment_id": "F_R13_p28_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p28:2", "text": "Bitcoin market, PLOS ONE , 17(3), e0264522, https://doi.org/10.1371/journal.\npone.0264522.\nDong, B, L Jiang, J Liu and Y Zhu (2022). Liquidity in the cryptocurrency market and\ncommonalities across anomalies, International Review of Financial Analysis, 81,\n102097, https://doi.org/10.1016/j.irfa.2022.102097.\nDwyer, GP (2015). The economics of Bitcoin and similar private digital currencies,\nJournal of Financial Stability, 17, 81–91, https://doi.org/10.1016/j.jfs.2014.11.006.\nDyhrberg, AH (2016). Bitcoin, gold and the dollar/C0 /C0 /C0 A GARCH volatility analysis,\nFinance Research Letters, 16, 85–92, https://doi.org/10.1016/j.frl.2015.10.008.\nEllsberg, D (1961). Risk, ambiguity, and the Savage axioms,The Quarterly Journal of\nEconomics, 75(4), 643, https://doi.org/10.2307/1884324.\nEom, Y (2021). Kimchi premium and speculative trading in bitcoin,Finance Research\nLetters, 38, 101505, https://doi.org/10.1016/j.frl.2020.101505.\nFakhfekh, M and A Jeribi (2020). Volatility dynamics of crypto-currencies’returns: Evidence\nfrom asymmetric and long memory GARCH models,Research in International Busi-\nness and Finance, 51, 101075, https://doi.org/10.1016/j.ribaf.2019.101075.\nFama, EF (1970). Efficient capital markets: A review of theory and empirical work,The\nJournal of Finance, 25(2), 383, https://doi.org/10.2307/2325486.", "tags": []}
{"fragment_id": "F_R13_p28_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p28:3", "text": "Fama, EF and KR French (1993). Common risk factors in the returns on stocks and bonds,\nJournal of Financial Economics, 33(1), 3–56, https://doi.org/10.1016/0304-405x(93)\n90023-5.\nFeinstein, BD and K Werbach (2021). The impact of cryptocurrency regulation on trading\nmarkets, Journal of Financial Regulation, 7(1), 48–99, https://doi.org/10.1093/jfr/\nfjab003.\nI. Abdeljawad et al.\n2550021-28\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p29_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p29:1", "text": "Ferreira, P, L Kristoufek and EJDAL Pereira (2020). DCCA and DMCA correlations of\ncryptocurrency markets,Physica A: Statistical Mechanics and Its Applications, 545,\n123803, https://doi.org/10.1016/j.physa.2019.123803.\nFerreira, P and D Pereira (2019). Contagion effect in cryptocurrency market,Journal of\nRisk and Financial Management, 12(3), 115, https://doi.org/10.3390/jrfm12030115.\nFilimonov, V and D Sornette (2013). A stable and robust calibration scheme of the log-\nperiodic power law model,Physica A: Statistical Mechanics and Its Applications,\n392(17), 3698–3707, https://doi.org/10.1016/j.physa.2013.04.012.\nFonseca, V , L Pacheco and J Lobão (2019). Psychological barriers in the cryptocurrency\nmarket, Review of Behavioral Finance, 12(2), 151–169, https://doi.org/10.1108/rbf-\n03-2019-0041.\nFraz, A, A Hassan and S Chughtai (2019). Seasonality in Bitcoin market,NICE Research\nJournal,1 –11, https://doi.org/10.51239/nrjss.v0i0.78.\nGarcía-Monle/C19on, F, I Danvila-del-Valle and FJ Lara (2021). Intrinsic value in crypto\ncurrencies, Technological Forecasting and Social Change, 162, 120393, https://doi.\norg/10.1016/j.techfore.2020.120393.\nGemayel, R and A Preda (2021). Performance and learning in an ambiguous environment:\nA study of cryptocurrency traders,International Review of Financial Analysis, 77,\n101847, https://doi.org/10.1016/j.irfa.2021.101847.", "tags": []}
{"fragment_id": "F_R13_p29_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p29:2", "text": "Geuder, J, H Kinateder and NF Wagner (2019). Cryptocurrencies as financial bubbles: The\ncase of Bitcoin,Finance Research Letters, 31, https://doi.org/10.1016/j.frl.2018.11.\n011.\nGhazani, MM and MA Jafari (2021). Cryptocurrencies, gold, and WTI crude oil market\nefficiency: A dynamic analysis based on the adaptive market hypothesis,Financial\nInnovation, 7(1), 29, https://doi.org/10.1186/s40854-021-00246-0.\nGoczek, U and I Skliarov (2019). What drives the Bitcoin price? A factor augmented error\ncorrection mechanism investigation,Applied Economics, 51(59), 6393–6410, https://\ndoi.org/10.1080/00036846.2019.1619021.\nGrobys, K, S Ahmed and N Sapkota (2020). Technical trading rules in the cryptocurrency\nmarket, Finance Research Letters, 32, 101396, https://doi.org/10.1016/j.frl.2019.\n101396.\nGrobys, K and N Sapkota (2019). Cryptocurrencies and momentum,Economics Letters,\n180, 6–10, https://doi.org/10.1016/j.econlet.2019.03.028.\nGronwald, M (2021). How explosive are cryptocurrency prices?Finance Research Letters,\n38, 101603, https://doi.org/10.1016/j.frl.2020.101603.\nGu/C19egan, D and T Renault (2021). Does investor sentiment on social media provide robust\ninformation for Bitcoin returns predictability?Finance Research Letters, 38, 101494,\nhttps://doi.org/10.1016/j.frl.2020.101494.\nGüleç, TC and H Aktaş (2019). Kripto para birimi piyasalar/C16 nda etkinliğin uzun haf/C16 za ve değişen", "tags": []}
{"fragment_id": "F_R13_p29_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p29:3", "text": "varyans €ozelliklerinin testi yoluyla analizi,Eskişehir Osmangazi Üniversitesi_Iktisadi ve\n_Idari Bilimler Dergisi, 14(2), 491–510, https://doi.org/10.17153/oguiibf. 520679.\nCryptocurrency Market Efficiency Revisited\n2550021-29\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p30_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p30:1", "text": "Gurdgiev, C and D O’Loughlin (2020). Herding and anchoring in cryptocurrency markets:\nInvestor reaction to fear and uncertainty,Journal of Behavioral and Experimental\nFinance, 25, 100271, https://doi.org/10.1016/j.jbef.2020.100271.\nHaryanto, S, A Subroto and M Ulpah (2019). Disposition effect and herding behavior in\nthe cryptocurrency market, Journal of Industrial and Business Economics, 47(1),\n115–132, https://doi.org/10.1007/s40812-019-00130-0.\nHattori, T and R Ishida (2020). The relationship between arbitrage in futures and spot\nmarkets and Bitcoin price movements: Evidence from the Bitcoin markets,Journal of\nFutures Markets, 41(1), 105–114, https://doi.org/10.1002/fut.22171.\nHayes, AS (2017). Cryptocurrency value formation: An empirical study leading to a cost\nof production model for valuing bitcoin,Telematics and Informatics, 34(7), 1308–\n1321, https://doi.org/10.1016/j.tele.2016.05.005.\nHo, KH, TT Chan, H Pan and C Li (2021). Do candlestick patterns work in cryptocurrency\ntrading? 2021 IEEE International Conference on Big Data (Big Data), https://doi.org/\n10.1109/BigData52589.2021.9671826.\nHudson, R and A Urquhart (2019). Technical trading and cryptocurrencies,Annals of\nOperations Research , 297(1 –2), 191 –220, https://doi.org/10.1007/s10479-019-\n03357-1.\nJegadeesh, N (1990). Evidence of predictable behavior of security returns,The Journal of", "tags": []}
{"fragment_id": "F_R13_p30_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p30:2", "text": "Finance, 45(3), 881–898, https://doi.org/10.1111/j.1540-6261.1990.tb05110.x.\nJegadeesh, N and S Titman (1993). Returns to buying winners and selling losers: Impli-\ncations for stock market efficiency,The Journal of Finance, 48(1), 65–91, https://doi.\norg/10.1111/j.1540-6261.1993.tb04702.x.\nJia, B, JW Goodell and D Shen (2022). Momentum or reversal: Which is the appropriate\nthird factor for cryptocurrencies?Finance Research Letters, 45, 102139, https://doi.\norg/10.1016/j.frl.2021.102139.\nJiang, Y, H Nie and W Ruan (2018). Time-varying long-term memory in Bitcoin market,\nFinance Research Letters, 25, 280–284, https://doi.org/10.1016/j.frl.2017.12.009.\nJoo, MH, Y Nishikawa and K Dandapani (2020). Announcement effects in the crypto-\ncurrency market, Applied Economics, 52(44), 4794–4808, https://doi.org/10.1080/\n00036846.2020.1745747.\nKahneman, D (2003). Maps of bounded rationality: Psychology for behavioral economics,\nAmerican Economic Review, 93(5), 1449–1475, https://doi.org/10.1257/000282803\n322655392.\nKaiser, L (2019). Seasonality in cryptocurrencies,Finance Research Letters, 31, https://\ndoi.org/10.1016/j.frl.2018.11.007.\nKamps, J and B Kleinberg (2018). To the moon: Defining and detecting cryptocurrency\npump-and-dumps, Crime Science, 7(1), https://doi.org/10.1186/s40163-018-0093-5.\nKang, HJ, SG Lee and SY Park (2021). Information efficiency in the cryptocurrency", "tags": []}
{"fragment_id": "F_R13_p30_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p30:3", "text": "market: The efficient-market hypothesis,Journal of Computer Information Systems,\n62(3), 622–631, https://doi.org/10.1080/08874417.2021.1872046.\nKatsiampa, P (2019). V olatility co-movement between Bitcoin and Ether,Finance Re-\nsearch Letters, 30, 221–227, https://doi.org/10.1016/j.frl.2018.10.005.\nI. Abdeljawad et al.\n2550021-30\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p31_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p31:1", "text": "Katsiampa, P, S Corbet and B Lucey (2019). High frequency volatility co-movements in\ncryptocurrency markets,Journal of International Financial Markets, Institutions and\nMoney, 62, 35–52, https://doi.org/10.1016/j.intfin.2019.05.003.\nKeshari Jena, S, AK Tiwari, B Doğan and S Hammoudeh (2020). Are the top six cryp-\ntocurrencies efficient? Evidence from time-varying long memory, International\nJournal of Finance & Economics, 27(3), 3730–3740, https://doi.org/10.1002/ijfe.\n2347.\nKhuntia, S and J Pattanayak (2018). Adaptive market hypothesis and evolving predict-\nability of bitcoin, Economics Letters, 167, 26–28, https://doi.org/10.1016/j.econ-\nlet.2018.03.005.\nKhuntia, S and J Pattanayak (2020). Adaptive long memory in volatility of intra-day\nbitcoin returns and the impact of trading volume, Finance Research Letters, 32,\n101077, https://doi.org/10.1016/j.frl.2018.12.025.\nKhuntia, S and J Pattanayak (2021). Adaptive calendar effects and volume of extra returns\nin the cryptocurrency market, International Journal of Emerging Markets, 17(9),\n2137–2165, https://doi.org/10.1108/ijoem-06-2020-0682.\nKhursheed, A, M Naeem, S Ahmed and F Mustafa (2020). Adaptive market hypothesis:\nAn empirical analysis of time-varying market efficiency of cryptocurrencies,Cogent\nEconomics & Finance, 8(1), 1719574, https://doi.org/10.1080/23322039.2020.17\n19574.", "tags": []}
{"fragment_id": "F_R13_p31_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p31:2", "text": "Kim, JH (2022). Analyzing diversification benefits of cryptocurrencies through backfill\nsimulation, Finance Research Letters, 50, 103238, https://doi.org/10.1016/j.frl.2022.\n103238.\nKlarin, A (2020). The decade-long cryptocurrencies and the blockchain rollercoaster:\nMapping the intellectual structure and charting future directions,Research in Inter-\nnational Business and Finance , 51, 101067, https://doi.org/10.1016/j.ribaf.2019.\n101067.\nK€ochling, G, J Müller and PN Posch (2019). Price delay and market frictions in crypto-\ncurrency markets, Economics Letters, 174, 39–41, https://doi.org/10.1016/j.econ-\nlet.2018.10.025.\nKosc, K, P Sakowski and RŚlepaczuk (2019). Momentum and contrarian effects on the\ncryptocurrency market, Physica A: Statistical Mechanics and Its Applications, 523,\n691–701, https://doi.org/10.1016/j.physa.2019.02.057.\nKozlowski, SE, MR Puleo and J Zhou (2020). Cryptocurrency return reversals,Applied\nEconomics Letters , 28(11), 887 –893, https://doi.org/10.1080/13504851.2020.178\n4831.\nKristoufek, L (2018). On Bitcoin markets (in)efficiency and its evolution,Physica A:\nStatistical Mechanics and Its Applications, 503, 257–262, https://doi.org/10.1016/j.\nphysa.2018.02.161.\nKumar, A (2020). Empirical investigation of herding in cryptocurrency market under\ndifferent market regimes,Review of Behavioral Finance, 13(3), 297–308, https://doi.", "tags": []}
{"fragment_id": "F_R13_p31_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p31:3", "text": "org/10.1108/rbf-01-2020-0014.\nCryptocurrency Market Efficiency Revisited\n2550021-31\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p32_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p32:1", "text": "Kyriazis (2019). A survey on efficiency and profitable trading opportunities in crypto-\ncurrency markets,Journal of Risk and Financial Management, 12(2), 67, https://doi.\norg/10.3390/jrfm12020067.\nLahmiri, S, S Bekiros and A Giakoumelou (2021). Multi-scale analysis reveals different\npatterns in technical indicators of blockchain, Fractals, 29(07), https://doi.org/\n10.1142/s0218348x21501851.\nLatif, SR, MA Mohd, MNM Amin and AI Mohamad (2017). Testing the weak form of\nefficient market in cryptocurrency, Journal of Engineering and Applied Sciences,\n12(9), 2285–2288, Article ISSN: 1816-949x.\nLee, AD, M Li and H Zheng (2020a). Bitcoin: Speculative asset or innovative technology?\nJournal of International Financial Markets, Institutions and Money, 67, 101209,\nhttps://doi.org/10.1016/j.intfin.2020.101209.\nLee, CM and B Swaminathan (2000). Price momentum and trading volume,The Journal\nof Finance, 55(5), 2017–2069, https://doi.org/10.1111/0022-1082.00280.\nLee, S, NE Meslmani and LN Switzer (2020b). Pricing efficiency and arbitrage in the\nBitcoin spot and futures markets,Research in International Business and Finance,\n53, 101200, https://doi.org/10.1016/j.ribaf.2020.101200.\nLi, M, V Manahov and J Ashton (2025). The impact of cryptocurrency heists on Bitcoin’s\nmarket efficiency,International Journal of Finance & Economics, 30(3), 2912–2929,\nhttps://doi.org/10.1002/ijfe.3049.", "tags": []}
{"fragment_id": "F_R13_p32_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p32:2", "text": "Li, Y , A Urquhart, P Wang and W Zhang (2021). MAX momentum in cryptocurrency\nmarkets, International Review of Financial Analysis, 77, 101829, https://doi.org/\n10.1016/j.irfa.2021.101829.\nLi, Y, W Zhang, X Xiong and P Wang (2019). Does size matter in the cryptocurrency\nmarket? Applied Economics Letters, 27(14), 1141–1149, https://doi.org/10.1080/\n13504851.2019.1673298.\nLiew, J, R Li, T Budav/C19ari and A Sharma (2019). Cryptocurrency investing examined,The\nJournal of the British Blockchain Association, 2(2), 1–12, https://doi.org/10.31585/\njbba-2-2-(2)2019.\nLiu, W, X Liang and G Cui (2020). Common risk factors in the returns on cryptocurrencies,\nEconomic Modelling, 86, 299–305, https://doi.org/10.1016/j.econmod.2019.09.035.\nLiu, Y and A Tsyvinski (2020). Risks and returns of cryptocurrency,The Review of\nFinancial Studies, 34(6), 2689–2727, https://doi.org/10.1093/rfs/hhaa113.\nLiu, Y , A Tsyvinski and X Wu (2022). Common risk factors in cryptocurrency,The\nJournal of Finance, 77(2), 1133–1177, https://doi.org/10.1111/jofi.13119.\nLo, AW (2004). The adaptive markets hypothesis,The Journal of Portfolio Management,\n30(5), 15–29, https://doi.org/10.3905/jpm.2004.442611.\nLo, AW (2005). Reconciling efficient markets with behavioral finance: The adaptive\nmarkets hypothesis, Journal of Investment Consulting, 7(2), 21–44.", "tags": []}
{"fragment_id": "F_R13_p32_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p32:3", "text": "Long, H, A Zaremba, E Demir, JJ Szczygielski and M Vasenin (2020). Seasonality in the\ncross-section of cryptocurrency returns, Finance Research Letters, 35, 101566,\nhttps://doi.org/10.1016/j.frl.2020.101566.\nI. Abdeljawad et al.\n2550021-32\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p33_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p33:1", "text": "L/C19opez-Martín, C, S Benito Muela and R Arguedas (2021). Efficiency in cryptocurrency\nmarkets: New evidence,Eurasian Economic Review, 11(3), 403–431, https://doi.org/\n10.1007/s40822-021-00182-5.\nLucey, BM, SA Vigne, L Yarovaya and Y Wang (2022). The cryptocurrency uncertainty\nindex, Finance Research Letters , 45, 102147, https://doi.org/10.1016/j.frl.2021.\n102147.\nLuo, D, T Mishra, L Yarovaya and Z Zhang (2021). Investing during a fintech revolution:\nAmbiguity and return risk in cryptocurrencies,Journal of International Financial\nMarkets, Institutions and Money, 73, 101362, https://doi.org/10.1016/j.intfin.2021.\n101362.\nMaghyereh, A and M Al-Shboul (2024). Have the extraordinary circumstances of the\nCOVID-19 outbreak and the Russian–Ukrainian conflict impacted the efficiency of\ncryptocurrencies? Financial Innovation, 10(1), 8, https://doi.org/10.1186/s40854-\n023-00550-x.\nMakarov, I and A Schoar (2020). Trading and arbitrage in cryptocurrency markets,Journal\nof Financial Economics, 135(2), 293–319, https://doi.org/10.1016/j.jfineco.2019.07.\n001.\nMakridis, CA, M Fr€owis, K Sridhar and R B€ohme (2023). The rise of decentralized\ncryptocurrency exchanges: Evaluating the role of airdrops and governance tokens,\nJournal of Corporate Finance, 79, 102358, https://doi.org/10.1016/j.jcorpfin.2023.\n102358.", "tags": []}
{"fragment_id": "F_R13_p33_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p33:2", "text": "Manahov, V (2020). Cryptocurrency liquidity during extreme price movements: Is there a\nproblem with virtual money?Quantitative Finance, 21(2), 341–360, https://doi.org/\n10.1080/14697688.2020.1788718.\nMbanga, CL (2018). The day-of-the-week pattern of price clustering in Bitcoin,Applied\nEconomics Letters, 26(10), 807–811, https://doi.org/10.1080/13504851.2018.1497\n844.\nMokni, K, G El Montasser, AN Ajmi and E Bouri (2024). On the efficiency and its drivers\nin the cryptocurrency market: The case of Bitcoin and Ethereum,Financial Inno-\nvation, 10(1), 39.\nMomtaz, PP (2024). Decentralized finance (DeFi) markets for startups: Search frictions,\nintermediation, and the efficiency of the ICO market,Small Business Economics,\n63(4), 1415–1447, https://doi.org/10.1007/s11187-024-00886-3.\nMontasser, GE, L Charfeddine and A Benhamed (2022). COVID-19, cryptocurrencies\nbubbles and digital market efficiency: Sensitivity and similarity analysis,Finance\nResearch Letters, 46, 102362, https://doi.org/10.1016/j.frl.2021.102362.\nNadarajah, S and J Chu (2017). On the inefficiency of Bitcoin,Economics Letters, 150, 6–9.\nNaeem, MA, E Bouri, Z Peng, SJH Shahzad and XV Vo (2021). Asymmetric efficiency of\ncryptocurrencies during COVID19,Physica A: Statistical Mechanics and Its Appli-\ncations, 565, 125562, https://doi.org/10.1016/j.physa.2020.125562.", "tags": []}
{"fragment_id": "F_R13_p33_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p33:3", "text": "Nakamoto, S (2008). Bitcoin: A Peer-to-Peer Electronic Cash System, Decentralized\nBusiness Review, https://assets.pubpub.org/d8wct41f/31611263538139.pdf.\nCryptocurrency Market Efficiency Revisited\n2550021-33\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p34_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p34:1", "text": "Nguyen, H, B Liu and NY Parikh (2020). Exploring the short-term momentum effect in the\ncryptocurrency market, Evolutionary and Institutional Economics Review, 17(2),\n425–443, https://doi.org/10.1007/s40844-020-00176-z.\nNimalendran, M, P Pathak, M Petryk and L Qiu (2025). Informational efficiency of\ncryptocurrency markets, Journal of Financial and Quantitative Analysis , 60(3),\n1427–1456, https://doi.org/10.1017/S0022109024000310.\nNoda, A (2020). On the evolution of cryptocurrency market efficiency,Applied Economics\nLetters, 28(6), 433–439, https://doi.org/10.1080/13504851.2020.1758617.\nOkorie, DI, E Bouri and M Mazur (2024). NFTs versus conventional cryptocurrencies: A\ncomparative analysis of market efficiency around COVID-19 and the Russia-Ukraine\nconflict, The Quarterly Review of Economics and Finance, 95, 126–151, https://doi.\norg/10.1016/j.qref.2024.03.001.\nOmane-Adjepong, M, P Alagidede and NK Akosah (2019). Wavelet time-scale persistence\nanalysis of cryptocurrency market returns and volatility,Physica A: Statistical Mechanics\nand Its Applications, 514, 105–120, https://doi.org/10.1016/j.physa.2018.09.013.\nOrtu, M, N Uras, C Conversano, S Bartolucci and G Destefanis (2022). On technical\ntrading and social media indicators for cryptocurrency price classification through\ndeep learning, Expert Systems With Applications , 198, 116804, https://doi.org/", "tags": []}
{"fragment_id": "F_R13_p34_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p34:2", "text": "10.1016/j.eswa.2022.116804.\nOzdamar, M, L Akdeniz and A Sensoy (2021). Lottery-like preferences and the MAX\neffect in the cryptocurrency market, Financial Innovation, 7(1), https://doi.org/\n10.1186/s40854-021-00291-9.\nPhillips, PCB, S Shi and J Yu (2015). Testing for multiple bubbles: Historical episodes of\nexuberance and collapse in the S&P 500,International Economic Review, 56(4),\n1043–1078, https://doi.org/10.1111/iere.12132.\nPhillips, PCB and J Yu (2011). Dating the timeline of financial bubbles during the sub-\nprime crisis, Quantitative Economics, 2(3), 455–491, https://doi.org/10.3982/qe82.\nPlastun, A, AO Drofa and TV Klyushnik (2019). Month of the year effect in the crypto-\ncurrency market and portfolio management, European Journal of Management\nIssues, 27(1–2), 29–35, https://doi.org/10.15421/191904.\nPolyzos, E, G Rubbaniy and M Mazur (2024). Efficient market hypothesis on the\nblockchain: A social-media-based index for cryptocurrency efficiency, Financial\nReview, 59(3), 807–829.\nQadan, M, DY Aharon and R Eichel (2022). Seasonal and calendar effects and the price\nefficiency of cryptocurrencies,Finance Research Letters, 46, 102354, https://doi.org/\n10.1016/j.frl.2021.102354.\nQiao, X, H Zhu and L Hau (2020). Time-frequency co-movement of cryptocurrency return\nand volatility: Evidence from wavelet coherence analysis,International Review of", "tags": []}
{"fragment_id": "F_R13_p34_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p34:3", "text": "Financial Analysis, 71, 101541, https://doi.org/10.1016/j.irfa.2020.101541.\nRogers, P (1998). The cognitive psychology of lottery gambling: A theoretical review,\nJournal of Gambling Studies, 14, 111–134, https://doi.org/10.1023/A:1023042708\n217.\nI. Abdeljawad et al.\n2550021-34\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p35_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p35:1", "text": "Rognone, L, S Hyde and SS Zhang (2020). News sentiment in the cryptocurrency market:\nAn empirical comparison with Forex,International Review of Financial Analysis, 69,\n101462, https://doi.org/10.1016/j.irfa.2020.101462.\nSabalionis, A, W Wang and H Park (2020). What affects the price movements in Bitcoin\nand Ethereum? The Manchester School, 89(1), 102–127, https://doi.org/10.1111/\nmanc.12352.\nSahoo, PK and D Sethi (2022). Market efficiency of the cryptocurrencies: Some new\nevidence based on price–volume relationship, International Journal of Finance &\nEconomics, https://doi.org/10.1002/ijfe.2744.\nSamuelson, PA (1958). An exact consumption-loan model of interest with or without the\nsocial contrivance of money,Journal of Political Economy, 66(6), 467–482, https://\ndoi.org/10.1086/258100.\nSapkota, N and K Grobys (2021). Asset market equilibria in cryptocurrency markets:\nEvidence from a study of privacy and non-privacy coins,Journal of International\nFinancial Markets, Institutions and Money, 74, 101402, https://doi.org/10.1016/j.\nintfin.2021.101402.\nSelgin, G (2015). Synthetic commodity money,Journal of Financial Stability, 17, 92–99,\nhttps://doi.org/10.1016/j.jfs.2014.07.002.\nSenarathne, CW (2019). Gambling behaviour in the cryptocurrency market,International\nJournal of Applied Behavioral Economics , 8(4), 1 –16, https://doi.org/10.4018/\nijabe.2019100101.", "tags": []}
{"fragment_id": "F_R13_p35_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p35:2", "text": "Shahzad, SJH, M Anas and E Bouri (2022). Price explosiveness in cryptocurrencies and\nElon Musk’s tweets,Finance Research Letters, 47, 102695, https://doi.org/10.1016/j.\nfrl.2022.102695.\nShahzad, SJH, E Bouri, T Ahmad, MA Naeem and XV V o (2021). The pricing of bad\ncontagion in cryptocurrencies: A four-factor pricing model,Finance Research Letters,\n41, 101797, https://doi.org/10.1016/j.frl.2020.101797.\nShanaev, S and B Ghimire (2022). A generalised seasonality test and applications for\ncryptocurrency and stock market seasonality,The Quarterly Review of Economics\nand Finance, 86, 172–185, https://doi.org/10.1016/j.qref.2022.07.002.\nShefrin, H and M Statman (1985). The disposition to sell winners too early and ride losers\ntoo long: Theory and evidence,The Journal of Finance, 40(3), 777–790, https://doi.\norg/10.1111/j.1540-6261.1985.tb05002.x.\nShen, D, A Urquhart and P Wang (2020). A three-factor pricing model for crypto-\ncurrencies, Finance Research Letters, 34, 101248, https://doi.org/10.1016/j.frl.2019.\n07.021.\nShrotryia, VK and H Kalra (2021). Herding in the crypto market: A diagnosis of heavy\ndistribution tails, Review of Behavioral Finance, 14(5), 566–587, https://doi.org/\n10.1108/rbf-02-2021-0021.\nShynkevich, A (2020). Impact of bitcoin futures on the informational efficiency of bitcoin\nspot market, Journal of Futures Markets, 41(1), 115–134, https://doi.org/10.1002/", "tags": []}
{"fragment_id": "F_R13_p35_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p35:3", "text": "fut.22164.\nCryptocurrency Market Efficiency Revisited\n2550021-35\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p36_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p36:1", "text": "Skwarek, M (2025). Why do investors behave irrationally in the cryptocurrency and\nemerging stock markets? SAGE Open, 15(3), 21582440251361212, https://doi.org/\n10.1177/21582440251361212.\nSnyder, H (2019). Literature review as a research methodology: An overview and\nguidelines, Journal of Business Research, 104, 333–339, https://doi.org/10.1016/j.\njbusres.2019.07.039.\nStrych, JO (2022). The impact of margin trading and short selling by retail investors on\nmarket price efficiency: Empirical evidence from bitcoin exchanges,Finance Re-\nsearch Letters, 47, 102689, https://doi.org/10.1016/j.frl.2022.102689.\nSubramaniam, S and M Chakraborty (2019). Investor attention and cryptocurrency returns:\nEvidence from quantile causality approach,Journal of Behavioral Finance, 21(1),\n103–115, https://doi.org/10.1080/15427560.2019.1629587.\nTekin, B (2025). Bitcoin as a behavioral bellwether: Unveiling the bandwagon effect and\ninvestor sensitivity in the NFT landscape,Journal of the Knowledge Economy,1 –26,\nhttps://doi.org/10.1007/s13132-025-02788-5.\nTirole, J (1985). Asset bubbles and overlapping generations,Econometrica, 53(6), 1497,\nhttps://doi.org/10.2307/1913231.\nTorraco, RJ (2005). Writing integrative literature reviews: Guidelines and examples,\nHuman Resource Development Review , 4(3), 356 –367, https://doi.org/10.1177/\n1534484305278283.", "tags": []}
{"fragment_id": "F_R13_p36_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p36:2", "text": "Tran, VL and T Leirvik (2020). Efficiency in the markets of crypto-currencies,Finance\nResearch Letters, 35, 101382, https://doi.org/10.1016/j.frl.2019.101382.\nTsuchiya, T (2021). Profitability of cryptocurrency pump and dump schemes,Digital\nFinance, 3(2), 149–167, https://doi.org/10.1007/s42521-021-00034-6.\nUrquhart, A (2016). The inefficiency of Bitcoin,Economics Letters, 148, 80–82, https://\ndoi.org/10.1016/j.econlet.2016.09.019.\nVidal-Tom/C19as, D, AM Ib/C19añez and JE Farin/C19os (2019). Herding in the cryptocurrency market:\nCSSD and CSAD approaches,Finance Research Letters, 30, 181–186, https://doi.\norg/10.1016/j.frl.2018.09.008.\nWang, J and X Wang (2021). COVID-19 and financial market efficiency: Evidence from\nan entropy-based analysis, Finance Research Letters, 42, 101888, https://doi.org/\n10.1016/j.frl.2020.101888.\nWei, WC (2018). Liquidity and market efficiency in cryptocurrencies,Economics Letters,\n168, 21–24, https://doi.org/10.1016/j.econlet.2018.04.003.\nWei, YM and A Dukes (2021). Cryptocurrency adoption with speculative price bubbles,\nMarketing Science, 40(2), 241–260, https://doi.org/10.1287/mksc.2020.1247.\nXiong, X, Y Meng, X Li and D Shen (2019). An empirical analysis of the adaptive market\nhypothesis with calendar effects: Evidence from China,Finance Research Letters, 31,\n321–333.", "tags": []}
{"fragment_id": "F_R13_p36_3", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p36:3", "text": "Yao, S, X Kong, A Sensoy, E Akyildirim and F Cheng (2021). Investor attention and\nidiosyncratic risk in cryptocurrency markets,The European Journal of Finance,1 –19,\nhttps://doi.org/10.1080/1351847x.2021.1989008.\nI. Abdeljawad et al.\n2550021-36\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R13_p37_1", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p37:1", "text": "Yaya, OS, AE Ogbonna, R Mudida and N Abu (2020). Market efficiency and volatility\npersistence of cryptocurrency during pre- and post-crash periods of Bitcoin: Evidence\nbased on fractional integration,International Journal of Finance & Economics, 27(3),\n1318–1335, https://doi.org/10.1002/ijfe.1851.\nYi, E, B Yang, M Jeong, S Sohn and K Ahn (2023). Market efficiency of cryptocurrency:\nEvidence from the Bitcoin market,Scientific Reports, 13(1), 4789, https://doi.org/\n10.1038/s41598-023-31618-4.\nYoussef, M (2020). What drives herding behavior in the cryptocurrency market?Journal\nof Behavioral Finance, 23(2), 230–239, https://doi.org/10.1080/15427560.2020.186\n7142.\nZaremba, A, MH Bilgin, H Long, A Mercik and JJ Szczygielski (2021). Up or down?\nShort-term reversal, momentum, and liquidity effects in cryptocurrency markets,\nInternational Review of Financial Analysis, 78, 101908, https://doi.org/10.1016/j.\nirfa.2021.101908.\nZhang, P, K Xu and J Qi (2023). The impact of regulation on cryptocurrency market volatility\nin the context of the COVID-19 pandemic/C0 /C0 /C0 evidence from China,Economic Analysis\nand Policy, 80, 222–246, https://doi.org/10.1016/j.eap.2023.08.015.\nZhang, W, P Wang, X Li and D Shen (2018). The inefficiency of cryptocurrency and its\ncross-correlation with Dow Jones Industrial Average, Physica A: Statistical", "tags": []}
{"fragment_id": "F_R13_p37_2", "source_id": "R13", "locator": "cryptocurrency-market-efficiency-revisited-a-bibliometric-analysis.pdf:p37:2", "text": "Mechanics and Its Applications, 510, 658–670, https://doi.org/10.1016/j.physa.2018.\n07.032.\nZhang, X, F Lu, R Tao and S Wang (2021). The time-varying causal relationship between\nthe Bitcoin market and internet attention,Financial Innovation, 7(1), https://doi.org/\n10.1186/s40854-021-00275-9.\nZhang, Y, S Chan, J Chu and H Sulieman (2020). On the market efficiency and liquidity of\nhigh-frequency cryptocurrencies in a bull and bear market, Journal of Risk and\nFinancial Management, 13(1), 8, https://doi.org/10.3390/jrfm13010008.\nZhu, F, S Fu and X Liu (2025). A quantile spillover-driven Markov switching model for\nvolatility forecasting: Evidence from the cryptocurrency market, Mathematics,\n13(15), 2382, https://doi.org/10.3390/math13152382.\nZhu, P, X Zhang, Y Wu, H Zheng and Y Zhang (2021). Investor attention and crypto-\ncurrency: Evidence from the Bitcoin market,PLOS ONE, 16(2), e0246331, https://\ndoi.org/10.1371/journal.pone.0246331.\nCryptocurrency Market Efficiency Revisited\n2550021-37\nJ. Finan. Eng. Downloaded from www.worldscientific.com\nby Islam Abdeljawad on 11/02/25. Re-use and distribution is strictly not permitted, except for Open Access articles.", "tags": []}
{"fragment_id": "F_R14_p1_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p1:1", "text": "Citation: Cohen, G.; Qadan, M. The\nComplexity of Cryptocurrencies\nAlgorithmic Trading. Mathematics\n2022, 10, 2037. https://doi.org/\n10.3390/math10122037\nAcademic Editor: Tatiana Filatova\nReceived: 8 May 2022\nAccepted: 10 June 2022\nPublished: 12 June 2022\nPublisher’s Note:MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional afﬁl-\niations.\nCopyright: © 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nmathematics\nArticle\nThe Complexity of Cryptocurrencies Algorithmic Trading\nGil Cohen 1, *\n and Mahmoud Qadan 2\n1 Department of Management, Western Galilee Academic College, Acre 2412101, Israel\n2 School of Business Administration, University of Haifa, Haifa 3498838, Israel; mqadan@univ.haifa.ac.il\n* Correspondence: gilc@wgalil.ac.il\nAbstract: In this research, we provided an answer to a very important trading question, what is the\noptimal number of technical tools in order to achieve the best trading results for both swing trade\nthat uses daily bars and intraday trade that uses minutes bars? We designed Machine Learning (ML)\nsystems that can trade four major cryptocurrencies: Bitcoin, Ethereum, BNB, and Solana. We found", "tags": []}
{"fragment_id": "F_R14_p1_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p1:2", "text": "that more indicators do not necessarily mean better trading performance. Swing traders that use daily\nbars should trade Bitcoin and Solana using Ichimoku Cloud (IC) plus Moving Average Convergence\nDivergence (MACD), Ethereum with IC plus Chaikin Money Flow (CMF), and BNB with IC alone.\nWith regard to intraday trading, we documented that different cryptocurrencies should be trading\nusing different time frames. These results emphasize that the optimal number of indicators that are\nused to trade daily bars is one or, at maximum, two. The Multi-Layer (MUL) system that consists of\nall three examined technical indicators failed to improve the trading results for both days (swing)\nand intraday trades. The main implication of this study for traders is that more indicators does not\nnecessarily improve trades performances.\nKeywords: cryptocurrencies; trading; intraday; swing; technical indicators\nMSC: 68T07\n1. Introduction\nForecasting the ﬁnancial market direction has noalways been challenging and complex\nbecause of the multiple factors that simultaneously inﬂuence ﬁnancial assets’ price. In\nrecent years, Machine Learning (ML) based on deep Neural Networks (NN) has contributed\ndramatically to the forecasting ability. Those systems are not only designed for price\nforecasting of ﬁnancial assets, but also for algorithmic trading. The input of such a system is", "tags": []}
{"fragment_id": "F_R14_p1_3", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p1:3", "text": "typically sequential past prices and volumes that are processed by mathematical calculation\nand produce a numeric valuation that enables the system to calculate the probabilities for\ntrend continuity or reversal. The designer of such a system lay layers of conditions that are\nbased on technical analysis, which determines when to enter or exit a trade (long or short).\nAs the conditions are mounted, the system will wait until all required conditions are met to\nengage in trading. This in many cases results in a late entry/exit of a trade to a point where a\nprice reversal is more likely to occur than the continuation of the current trend. Researchers\nhave tried in the past to implement different trading strategies for different ﬁnancial assets.\nMost of them (see for example Ref. [1]) have test trading proﬁtability according to a single\ntrading indicator or methodology. The present study compares trading results of a single\nindicators system to a Multiple Layers (MUL) system that is comprised of several indicators\ncombined. Our challenge is to ﬁnd how many indicators should be incorporated into the\nML system to ensure fast enough entry and exit of trade without neglecting important\nsignals that could result in the wrong prediction. This dilemma has not been fully explored\nin the literature, and the current paper wants to bridge that existing gap. As shown in", "tags": []}
{"fragment_id": "F_R14_p1_4", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p1:4", "text": "Figure 1, adding indicators to the designed trading system may increase proﬁtability until\nthe marginal technical tool does not contribute to the proﬁt. In that case, the optimum\nMathematics 2022, 10, 2037. https://doi.org/10.3390/math10122037 https://www.mdpi.com/journal/mathematics", "tags": []}
{"fragment_id": "F_R14_p2_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p2:1", "text": "Mathematics 2022, 10, 2037 2 of 11\nnumber of indicators that guide the ML system should be X. On the other hand, the optimal\nlevel of indicators may be a single indicator, meaning that adding more indicators will\ncause proﬁt to drop, and therefore should be avoided.\nMathematics 2022, 10, x FOR PEER REVIEW 2 of 12 \n \n \nto bridge that existing gap. As shown in Figure 1, adding indicators to the designed trad-\ning system may increase profitability until the marginal technical tool does not contribute \nto the profit. In that case, the optimum number of indicators that guide the ML system \nshould be X. On the other hand, the optimal level of indicators may be a single indicator, \nmeaning that adding more indicators will cause profit to drop , and therefore should be \navoided. \n \nFigure 1. Profits and the number of technical indicators. \nIn the following research, we test stand -alone three indicator systems and then \nformed a dual indicator system and eventually Multi -Layer (MUL), which combines all \nthree tools into one system. We seek the optimal combinations of technical tools incorpo-\nrated into our NN system that will lead to a maximum profit trading cryptocurrencies \nusing daily data (swing trade) and intraday data. Our data cont ain 5, 15, 30, 45, 60, and \n120 min trading bars (a trading bar includes the opening and closing price of the specified", "tags": []}
{"fragment_id": "F_R14_p2_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p2:2", "text": "time frame along with the highest and lowest prices ) along with daily bars of four major \ncryptocurrencies: Bitcoin, Ethereum BNB (Binance Coins), and Solana from the beginning \nof 2021 till the end of April 2022. \nWe find that more indicators do not necessarily mean better trading performance . \nThe best stand -alone indicator to trade daily bars of all examined cryptocurrencies was \nfound to be Ichimoku Cloud (IC). Adding Moving Average Convergence Divergence \nMACD to IC improved performance, trading Bitcoin and Solana while adding Chaikin \nMoney Flow (CMF) improved the trading results of Ethereum. BNB best performing sys-\ntem relied only on IC. With regard to intraday trading, the best performances were \nachieved by a single indicator system. \n2. Literature Review \nThe complexity of financial asset price forecasting often drives researchers to use \nNN-based systems because of their ability to handle big data that originated from differ-\nent sources of information (see for example: [2 –4]). The evolution of this field has led re-\nsearchers to construct more complex networks that are based on transformed data rather \nthan the standard raw data. Ref [5] used a radical basic function NN that transforms input \nsignals before they are fed to the networks. They concluded their system is superior to", "tags": []}
{"fragment_id": "F_R14_p2_3", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p2:3", "text": "more traditional NN in both trading performance and statistical accuracy. Long Short -\nTerm Memory (LSTM) systems include three stages—input, forget and output—that con-\ntrol the flow of information over time , and therefore suit time series analysis. The chal-\nlenge that this methodology introduces is to set the time frames for the memory and forget \nperiods. The wrong setting may result in a set of data that confuses the algorithmic system \nand can lead to bad trading performances. Ref [6] studies the usage of LSTM networks to \npredict future trends of stock prices based on past prices. Their results showed an average \nof 55.9% accuracy in predicting stock uptrends in the near future. Ref. [7] used Long Short-\nTerm Memory (LSTM) system to predict the S & P500 index and concluded that this type \nFigure 1. Proﬁts and the number of technical indicators.\nIn the following research, we test stand-alone three indicator systems and then formed\na dual indicator system and eventually Multi-Layer (MUL), which combines all three tools\ninto one system. We seek the optimal combinations of technical tools incorporated into our\nNN system that will lead to a maximum proﬁt trading cryptocurrencies using daily data\n(swing trade) and intraday data. Our data contain 5, 15, 30, 45, 60, and 120 min trading\nbars (a trading bar includes the opening and closing price of the speciﬁed time frame along", "tags": []}
{"fragment_id": "F_R14_p2_4", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p2:4", "text": "with the highest and lowest prices) along with daily bars of four major cryptocurrencies:\nBitcoin, Ethereum BNB (Binance Coins), and Solana from the beginning of 2021 till the end\nof April 2022.\nWe ﬁnd that more indicators do not necessarily mean better trading performance. The\nbest stand-alone indicator to trade daily bars of all examined cryptocurrencies was found\nto be Ichimoku Cloud (IC). Adding Moving Average Convergence Divergence MACD to\nIC improved performance, trading Bitcoin and Solana while adding Chaikin Money Flow\n(CMF) improved the trading results of Ethereum. BNB best performing system relied only\non IC. With regard to intraday trading, the best performances were achieved by a single\nindicator system.\n2. Literature Review\nThe complexity of ﬁnancial asset price forecasting often drives researchers to use\nNN-based systems because of their ability to handle big data that originated from dif-\nferent sources of information (see for example: [2–4]). The evolution of this ﬁeld has led\nresearchers to construct more complex networks that are based on transformed data rather\nthan the standard raw data. Ref. [5] used a radical basic function NN that transforms input\nsignals before they are fed to the networks. They concluded their system is superior to\nmore traditional NN in both trading performance and statistical accuracy. Long Short-Term", "tags": []}
{"fragment_id": "F_R14_p2_5", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p2:5", "text": "Memory (LSTM) systems include three stages—input, forget and output—that control the\nﬂow of information over time, and therefore suit time series analysis. The challenge that\nthis methodology introduces is to set the time frames for the memory and forget periods.\nThe wrong setting may result in a set of data that confuses the algorithmic system and\ncan lead to bad trading performances. Ref. [ 6] studies the usage of LSTM networks to\npredict future trends of stock prices based on past prices. Their results showed an average\nof 55.9% accuracy in predicting stock uptrends in the near future. Ref. [ 7] used Long\nShort-Term Memory (LSTM) system to predict the S & P500 index and concluded that this\ntype of forecasting method is superior to other machine learning methods. Ref. [8] used the\nLSTM model to predict the direction of the Chinese stocks and showed a 13% forecasting\nimprovement compared to other methods. Ref. [9] used LSTM for intraday stock prediction\nusing technical analysis indicators as network inputs and found that their model performs\nbetter than the benchmark or equally weighted ensembles. Ref. [10] used transformed NN", "tags": []}
{"fragment_id": "F_R14_p3_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p3:1", "text": "Mathematics 2022, 10, 2037 3 of 11\nusing indicators like relative strength index to predict future prices and concluded that\nincorporating domain knowledge in NN improves price prediction.\nNN systems are also being used to predict the volatility of the ﬁnancial asset. Ref. [ 11]\ncombined the GARCH model with NN and compared their model to the classical GARCH\nmodel and found that the hybrid model is superior to the classical model in prediction\nvolatility. Ref. [12] proved that ARMA-GARCH with artiﬁcial neural networks can effec-\ntively predict market shocks.\nCryptocurrency prices suffer from a highly volatile trading environment that can\nbeneﬁt from algorithmic trading systems (see for example Ref. [13,14]). Ref. [15] examined\nthe cryptocurrency exchanges’ effects on prices and concluded that the shared information\nis dynamic and has a signiﬁcant impact on investors. Ref. [ 16] modeled interaction in dis-\ncontinuous movements of cryptocurrencies and showed that small price jumps observed in\ncryptocurrencies negatively affect the jump component of other cryptocurrencies’ realized\nvolatility, while large jumps have the opposite effect. They also documented a negative\neffect of S & P500 jumps on cryptocurrency price jumps. In the current study, we also\ndocumented discontinuous movements of some of the examined cryptocurrencies that", "tags": []}
{"fragment_id": "F_R14_p3_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p3:2", "text": "made future price forecasting more challenging. However, this phenomenon is mitigated\nas we moved to smaller periods of time frames.\n3. Materials and Methods\nOur intention in this research is to examine the optimal number of indicators that\nwill produce the best trading performances for both minutes and daily bars. Our algo-\nrithmic trading system uses a single standalone indicator and compares its performances\nto integrated systems that use two and three indicators together. Our null hypothesis is\nas follows:\nHypothesis 0 (H0). More indicators will improve trading performances.\nOur data contain minutes of various trading bars of sizes along with daily bars of four\nmajor cryptocurrencies: Bitcoin, Ethereum BNB (Binance Coins), and Solana from January\n2021 till April 2022. These cryptocurrencies were selected because of their market value\nand volumes of trade (The data was retrieved from investing.com).\n3.1. Recurrent Neural Networks\nThe nature of the ﬁnancial markets data is derived from the emergence of Recurrent\nNeural Networks (RNN). Unlike standard NN procedures that only learn from training\ndata, Recurrent Neural Network (RNN) can also use past states to process its inputs and\nimprove the system predictions abilities [ 17]. Since ﬁnancial asset price prediction is\ncomplex and depends on various inputs, a complex-valued weights RNN is needed [18,19].", "tags": []}
{"fragment_id": "F_R14_p3_3", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p3:3", "text": "The RNN goal is to get a predicted value that is as close as possible to the actual value.\nAt time t, h(t) is obtained through a channel estimation that uses a series of d past values:\nhr(t − 1), hi(t − 1), hr(t − 2), hi(t − 2) . . . . . .hr(t − d), hi(t − d) that are fed into the RNN\n(i, r are example values, however, the model can integrate as many values as needed). The\ninput of the system is denoted in Equation (1), while the output and the prediction channel\nare presented in Equations (2) and (3).\nX(t) = [hr(t), hi(t) . . . . . .hr(t − d), hi(t − d)]\nT\n(1)\nY(t) = [hr(t + 1), hi(t + 1)]\nT\n(2)\nh(t + 1) = hr(t + 1) + jhi(t + 1) (3)\nWe utilized the RNN procedure by using inputs calculation derived from three differ-\nent technical tools. These calculations are then used together as ﬁlters of our system that\ndetermines the actual entry and exit of a trade.", "tags": []}
{"fragment_id": "F_R14_p4_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p4:1", "text": "Mathematics 2022, 10, 2037 4 of 11\n3.2. The Trading Strategy\nOur trading strategy is based primarily on Ichimoku Cloud (IC) indicator developed\nby Goich Hosoda. The indicator points to momentum and trend direction along with using\nmultiple averages that create a cloud shape ﬁgure that helps traders to detect the market\ndirection and support and resistance eras. As shown in Figure 2, when the price of the\nﬁnancial asset is above the cloud, the trend is up, and the system enters a long position\nand when the price is below the cloud the trend is down and the system enters a short\nposition. In addition to IC we incorporate into our system two other trading indicators\nCMF and MACD.\nMathematics 2022, 10, x FOR PEER REVIEW 4 of 12 \n \n \nℎ(𝑡 + 1) = ℎ𝑟(𝑡 + 1) + 𝑗ℎ𝑖(𝑡 + 1) (3) \nWe utilized the RNN procedure by using inputs calculation derived from three dif-\nferent technical tools. These calculations are then used together as filters of our system \nthat determines the actual entry and exit of a trade. \n3.2. The Trading Strategy \nOur trading strategy is based primarily on Ichimoku Cloud (IC) indicator developed \nby Goich Hosoda. The indicator points to momentum and trend direction along with us-\ning multiple averages that create a cloud shape figure that helps traders to detect the mar-\nket direction and support and resistance eras. As shown in Figure 2, when the price of the", "tags": []}
{"fragment_id": "F_R14_p4_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p4:2", "text": "financial asset is above the cloud, the trend is up, and the system enters a long position \nand when the price is below the cloud the trend is down and the system en ters a short \nposition. In addition to IC we incorporate into our system two other trading indicators \nCMF and MACD. \n \nFigure 2. Ichimoku cloud long and short signaling. \nChaikin Money Flow (CMF) developed by Marc Chaikin is a volume-weighted aver-\nage of accumulation and distribution over a specified period. The contribution of CMF to \nour trading system is its ability to identify market direction by integrating trade volumes \nwith price movements. Above zero CMF is serves as an uptrend signal while below zero \nscores is a downtrend signal. MACD is a trend momentum indicator that measures the \ndifference between two Exponential Moving Averages (EMA) (the exponential moving \naverage is a moving average that places a greater weight on the most recent data than \nolder data) of a financial asset’s price. When the MACD crosses above its signal line (the \nsignal line is a nine -day EMA of the MACD) , an uptrend is predicted and when the \nMACD crosses below its signal line a downtrend is predicted. The long entry conditions \nare described in Equations (4)–(7) and the short entry conditions are described in Equa-\ntions (8–(11). Conditions 4 and 5 for long tr ades and 8 and 9 for short trades are derived", "tags": []}
{"fragment_id": "F_R14_p4_3", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p4:3", "text": "from IC calculations , while Conditions 6 and 10 (long and short) are derived from the \nCMF indicator calculations , and 7 and 11 (long and short) are derived from the MACD \nformula. \nLong Conditions: \nFigure 2. Ichimoku cloud long and short signaling.\nChaikin Money Flow (CMF) developed by Marc Chaikin is a volume-weighted average\nof accumulation and distribution over a speciﬁed period. The contribution of CMF to our\ntrading system is its ability to identify market direction by integrating trade volumes\nwith price movements. Above zero CMF is serves as an uptrend signal while below zero\nscores is a downtrend signal. MACD is a trend momentum indicator that measures the\ndifference between two Exponential Moving Averages (EMA) (the exponential moving\naverage is a moving average that places a greater weight on the most recent data than\nolder data) of a ﬁnancial asset’s price. When the MACD crosses above its signal line (the\nsignal line is a nine-day EMA of the MACD), an uptrend is predicted and when the MACD\ncrosses below its signal line a downtrend is predicted. The long entry conditions are\ndescribed in Equations (4)–(7) and the short entry conditions are described in Equations\n(8–(11). Conditions 4 and 5 for long trades and 8 and 9 for short trades are derived from\nIC calculations, while Conditions 6 and 10 (long and short) are derived from the CMF", "tags": []}
{"fragment_id": "F_R14_p4_4", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p4:4", "text": "indicator calculations, and 7 and 11 (long and short) are derived from the MACD formula.\nLong Conditions:\nP > (9 − PH + 9 − PL) + (26 − PH + 26 − PL)\n4 (4)\nP > 52 − PH + 52 − PL\n2 (5)\n∑21\ni=1\n{(P−PL)−(PH−P)\n(Ph−PL)\n}\n∗ V\n∑21\ni=1 V\n> 0 (6)\nEMA (12) − EMA (26) > 0 (7)\nShort Conditions:\nP < (9 − PH + 9 − PL) + (26 − PH + 26 − PL)\n4 (8)", "tags": []}
{"fragment_id": "F_R14_p5_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p5:1", "text": "Mathematics 2022, 10, 2037 5 of 11\nP < 52 − PH + 52 − PL\n2 (9)\n∑21\ni=1\n{(P−PL)−(PH−P)\n(Ph−PL)\n}\n∗ V\n∑21\ni=1 V\n< 0 (10)\nEMA (12) − EMA (26) < 0 (11)\nwhere: p = closing price, PH = period high, PL = period low, V = period trading volume,\nEMA (12) = twelve periods exponential moving average, EMA (26) = twenty-six periods\nof the exponential moving average.\nWe start by constructing and testing a trading system using each indicator as a stand-\nalone algorithmized trading tool for both intraday and multiple days trading of the exam-\nined cryptocurrencies. We then combined IC with MACD conditions to create a system\nthat is based on the accumulation of two indicators and repeated that process for IC and\nCMF as well. Finally, we constructed Multiple Layers (MUL) system that is based on\nthe accumulation of the three indicators CI + MACD + CMF. Our aim is to ﬁnd out if\ndifferent cryptocurrencies demand different trading tools and what is the optimal number\nof indicators that will result in maximum Net Proﬁt (NP) for each crypto and time frame.\nAlong with the NP , our algorithm is designed to calculate and report real-time Proﬁt Factor\n(PF), which is the gross proﬁt divided by gross losses, and the percent of proﬁtable trades\nof all trades (PP). The system operates on different time frames bars including 5, 15, 30,45,\n60, and 120 min and on daily bars.\n4. Results", "tags": []}
{"fragment_id": "F_R14_p5_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p5:2", "text": "We start by presenting in Table 1 the trading results of our RNN system using IC\nindicator as a stand-alone indicator generating calculations that are fed to the system to\ngenerate real-time buy and sell signals.\nTable 1. Results of Ichimoku cloud trading algorithm.\nMinutes Bitcoin Ethereum BNB Solana Average\n5\nNP −1246 −42 19.1 −10.37\nPF 0.94 0.97 1.12 0.87 0.98\nPP 27% 29.4% 36% 25.5% 29.5%\n15\nNP 590 −268 −3.3 19.77\nPF 1.02 0.90 0.99 1.17 1.02\nPP 28% 25.9% 36.5% 30.4% 30.2%\n30\nNP 12,497 1344 52 46.97\nPF 1.28 1.37 1.10 1.25 1.25\nPP 31.2% 30.8% 36.2% 36% 33.6%\n45\nNP 26,679 2385 * −413 181.2 *\nPF 1.14 1.16 0.85 1.33 1.12\nPP 33.5% 33.3% 32.3% 35.8% 33.7%\n60\nNP 26,777 * −1020 44.2 160\nPF 1.17 0.93 1.02 1.34 1.12\nPP 32.9% 34.3% 32.7% 33.7% 33.4%\n120\nNP −5868 −1522 578 * 133\nPF 0.96 0.86 1.53 1.37 1.18\nPP 28.2% 30.6% 38.5% 35% 33%\nD\nNP 80,429 2128 407.9 200\nPF 6.68 2.03 4.28 4.73 4.43\nPP 48.7% 56.7% 42.9% 55.6% 51%\nNotes: NP = dollar value Net Proﬁt, PF = Proﬁt Factor, PP = Percent of Proﬁtable trade of all trades. Minutes = the\nminutes time frame for each bar, D = daily bars, * = intraday highest dollar value NP .\nTable 1 demonstrates that IC based system best be used to trade daily bars resulting in\nan average remarkable PF of 4.43 and PP of 51%. The highest PF was achieved for Bitcoin\nDaily Bars trades followed by Solana and BNB. With regard to intraday trading, 60 min", "tags": []}
{"fragment_id": "F_R14_p6_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p6:1", "text": "Mathematics 2022, 10, 2037 6 of 11\nbars are the most prominent for Bitcoin, 45 min for Ethereum and Solana, and 120 min\nfor BNB. Table 2 summarizes the results of the CMF as a stand-alone indicator that its\ncalculations are fed to our system and determine trades entry and exit.\nTable 2. Results of CMF trading algorithm.\nMinutes Bitcoin Ethereum BNB Solana Average\n5\nNP 4142 603 −17 21.2 *\nPF 1.12 1.17 0.96 1.12 1.09\nPP 55.5% 54.5% 51% 53.1% 53.5%\n15\nNP −8346 −942 −83.1 −29\nPF 0.91 0.87 0.90 0.92 0.90\nPP 52.1% 53.1% 53.5% 51.5% 52.5%\n30\nNP 4146 −117 73.6 * −10.6\nPF 1.21 0.99 1.06 0.98 1.06\nPP 53.9% 53.9% 52.3% 49.5% 52.4%\n45\nNP 70,675 * 2820 * −124 −41.9\nPF 1.13 1.06 0.95 0.98 1.03\nPP 56.8% 54.7% 50.5% 51.9% 53.5%\n60\nNP 20, 052 1093 −233 −289\nPF 1.05 1.03 0.90 0.83 0.95\nPP 53.7% 52.8% 51.2% 50.7% 52.1%\n120\nNP −76,360 −3450 −444 −130\nPF 0.80 0.88 0.76 0.89 0.83\nPP 49.93% 48.4% 48% 50.5% 49.2%\nD\nNP 12,562 555.2 113.5 23.8\nPF 1.09 1.06 1.49 1.09 1.18\nPP 50.4% 49.8% 60.9% 45.2% 51.6%\nNotes: NP = dollar value Net Proﬁt, PF = Proﬁt Factor, PP = Percent of Proﬁtable trade of all trades. Minutes = the\nminutes time frame for each bar, D= daily bars, * = intraday highest dollar value NP .\nTable 2 shows that all cryptocurrencies are traded proﬁtably using daily bars with\nan average of 1.18 PF and 51.6% PP . In terms of intraday trades, 45 min are the best time", "tags": []}
{"fragment_id": "F_R14_p6_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p6:2", "text": "frames to analyze and trade Bitcoin and Ethereum, 30 min for BNB, and 5 min for Solana.\nTable 3 shows the results of our RNN system based on MACD as a stand-alone indicator.\nTable 3 show that daily bars generate proﬁtable trades trading Bitcoin, Ethereum, and\nSolana, but not BNB using the MACD indicator. Regarding intraday trades, the system\ngenerated the most proﬁtable trades using 60 min bars for Bitcoin, Ethereum, and Solana,\nand 45 min bars for BNB. Concluding the single indicator systems, the best indicator to\ntrade daily bars is by far IC, resulting in an average PF of 4.43 compared to 1.18 and 1.2 for\nthe CMF and MACD, respectively. In terms of intraday trades, the best conﬁguration for\nBitcoin and Ethereum is CMF 45 min bars resulting in 70,675$ NP and 56.8% PP for Bitcoin\nand 2820$ NP and 54.7% PP for Ethereum. BNB is best-traded intraday using 120 min bar\nand IC resulting in 578$ NP and 1.53 PF. We now apply dual layers sets of conditions to\nour RNN systems and start by analyzing the trading results of IC and MACD systems.\nTable 4 demonstrates that the combination of IC and MACD has led to a proﬁt for all\nfour cryptocurrencies trading daily bars with a high average PF of 3.75, which is lower than\nthe average PF using IC, which was found to be the most effective indicator for trading", "tags": []}
{"fragment_id": "F_R14_p6_3", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p6:3", "text": "cryptocurrencies daily bars. While incorporating MACD with IC has slightly improved the\nresults of the daily-based system for Bitcoin than the stand-alone IC system, it dramatically\nreduces the results for BNB trades since, as Table 3 pointed out, MACD is not a useful\ntechnical indicator to trade BNB daily bars. In terms of intraday trading, the dual-based\nsystem has improved the NP of Bitcoin’s IC best conﬁguration and reduced Ethereum’s\nslightly, however, it reduces dramatically the optimal NP of BNB compared to both indi-\nvidual indicators systems. For Solana, the dual system produced better performance than", "tags": []}
{"fragment_id": "F_R14_p7_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p7:1", "text": "Mathematics 2022, 10, 2037 7 of 11\nIC alone, but the worst performance than MACD. To summarize the results, we conclude\nthat the dual indicator system has shown superiority over the individual indicator in some\ncases in inferiority in others. Table 5 demonstrates the results of the dual system that is\nbased on IC and CMF.\nTable 3. Results of MACD trading algorithm.\nMinutes Bitcoin Ethereum BNB Solana Average\n5\nNP −7949 −668 14.3 1.69\nPF 0.81 0.82 1.04 1.01 0.92\nPP 30.9% 30.4% 32% 34.3% 31.9%\n15\nNP −5469 −844 −35.9 84\nPF 0.92 0.86 0.95 1.37 1.03\nPP 34.5% 32.8% 35.9% 40.3% 35.9%\n30\nNP −17,872 3.2 106 87.9\nPF 0.84 1.01 1.09 1.21 1.04\nPP 28.6% 36.1% 37.6% 38.1% 35.1%\n45\nNP 4825 −1578 669 * 78.8\nPF 1.01 0.96 1.40 1.05 1.10\nPP 34.8% 34% 38.8% 36.6% 36%\n60\nNP 17,017 * 1776 * 329 315.8 *\nPF 1.05 1.06 1.20 1.28 1.15\nPP 32.9% 35% 37.3% 39.2% 36.1%\n120\nNP −4563 1459 −157 164.7\nPF 0.98 1.07 0.88 1.21 1.04\nPP 34.7% 34.8% 32.7% 38.5% 35.2%\nD\nNP 35,097 1584 −1.2 83.5\nPF 1.32 1.20 0.95 1.36 1.20\nPP 41.5% 38.8% 45.5% 37.7% 40.8%\nNotes: NP = dollar value Net Proﬁt, PF = Proﬁt Factor, PP = Percent of Proﬁtable trade of all trades. Minutes = the\nminutes time frame for each bar, D = daily bars, * = intraday highest dollar value NP .\nTable 4. Results of Ichimoku cloud plus MACD trading algorithm.\nMinutes Bitcoin Ethereum BNB Solana Average\n5\nNP −108.5 168 −15.6 −14\nPF 0.99 1.12 0.92 0.83 0.96", "tags": []}
{"fragment_id": "F_R14_p7_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p7:2", "text": "PP 27.4% 30.9% 31.6% 25% 28.7%\n15\nNP −63.5 −212 12.6 17.3\nPF 0.99 0.92 1.05 1.15 1.03\nPP 27.6% 27% 37.2% 30% 30.4%\n30\nNP 11,340 1416 50.9 * 50.4\nPF 1.25 1.39 1.11 1.26 1.25\nPP 32.2% 33% 35% 37.3% 34.3%\n45\nNP 19,740 2237 * −53 155.2\nPF 1.10 1.15 0.95 1.28 1.12\nPP 33.6% 33.3% 32% 34.8% 33.4%\n60\nNP 29,765 * −1189 −86.6 200.8 *\nPF 1.19 0.92 0.90 1.44 1.11\nPP 34% 34.1% 30.3% 33% 32.8%\n120\nNP −8458 −1515 −66.4 119.8\nPF 0.94 0.87 0.89 1.34 1.01\nPP 28.2% 28.2% 36.7% 35.3% 32.1%\nD\nNP 81,584 1898 91 200.8\nPF 7.07 1.86 1.2 4.79 3.75\nPP 46.3% 50% 50% 55.6% 50.4%\nNotes: NP = dollar value Net Proﬁt, PF = Proﬁt Factor, PP = Percent of Proﬁtable trade of all trades. Minutes = the\nminutes time frame for each bar, D = daily bars, * = intraday highest dollar value NP .", "tags": []}
{"fragment_id": "F_R14_p8_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p8:1", "text": "Mathematics 2022, 10, 2037 8 of 11\nTable 5. Results of Ichimoku cloud plus CMF trading algorithm.\nMinutes Bitcoin Ethereum BNB Solana Average\n5\nNP −85.5 −56 −7.9 −1.48\nPF 0.99 0.96 0.96 0.98 0.97\nPP 29.5% 32.3% 32.7% 28.8% 30.8%\n15\nNP 139 −52.8 45.3 * 25.2\nPF 1.01 0.98 1.22 1.25 1.11\nPP 28.6% 27.6% 40.2% 35.4% 32.9%\n30\nNP −4561 128 −50.6 −5.99\nPF 0.90 1.03 0.90 0.97 0.95\nPP 29% 32.1% 35% 36% 33%\n45\nNP 20,858 * 1529 * −155 252.8\nPF 1.11 1.12 0.85 1.53 1.15\nPP 33.7% 34.4% 29% 37.7% 33.7%\n60\nNP −3505 −2238 −68 119.8 *\nPF 0.98 0.83 0.91 1.25 0.99\nPP 29.6% 33.5% 32.9% 34% 32.5%\n120\nNP −14,671 −86 44.7 93\nPF 0.89 0.99 1.10 1.26 1.06\nPP 32.3% 30.7% 43.6% 33.7% 35%\nD\nNP 40,424 2292 92.5 167.7\nPF 2.52 2.18 2.14 4.02 2.71\nPP 46.7% 47.8% 43.2% 44.5% 45.5%\nNotes: NP = dollar value Net Proﬁt, PF = Proﬁt Factor, PP = Percent of Proﬁtable trade of all trades. Minutes = the\nminutes time frame for each bar, D = daily bars, * = intraday highest dollar value NP .\nTable 5 indicates that IC + CMF improved the trading results of daily bars than\nCMF alone for Bitcoin, Ethereum, and Solana, but not for BNB. Moreover, the dual-based\nmodel achieved higher NP than IC alone only for Ethereum. The average daily PF of the\ntwo-indicator-based system was 2.71 compared to 1.18 and 4.43 for CMF and IC as a stand-\nalone-based system. In terms of Intraday trading, the single indicator system performed", "tags": []}
{"fragment_id": "F_R14_p8_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p8:2", "text": "better than the dual indicators-based system for four out of ﬁve cryptocurrencies, as only\nfor Solana did the dual model increase the NP . To summarize, the dual indicator has not\nimproved the system performances for intraday trading and daily trading in most cases.\nWe now present in Table 6 the results of our Multi-Layers (MUL) system that is based on\nall three-indicators together that are generated through our system.\nTable 6 shows that for daily bars the MUL system did not perform better than IC\nstand-alone system for all the examined currencies. Moreover, the MUL system improved\nNP for all cryptocurrencies except for Ethereum (MACD) and BNB (CMF). In summary\nof swing trade (daily bars) of all six systems, the MUL system failed to be the best system\nfor all examined cryptocurrencies. Bitcoin and Solana should be traded using IC + MACD,\nEthereum with IC + CMF, and BNB with IC alone. In terms of intraday trading, the\nMUL model outperformed the single indicator system only three times out of twelve. In\nsummary, of intraday trade, comparing all six systems the MUL system has again never\nbeen able to outperform all other ﬁve systems. Bitcoin and Ethereum should be traded\nusing 45 min bars with CMF alone, BNB and Solana with MACD alone using 45 and 60 min\nbars, respectively. These results emphasize that the optimal number of indicators that are", "tags": []}
{"fragment_id": "F_R14_p8_3", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p8:3", "text": "used to trade daily bars is one or a maximum of two depending on the ﬁnancial asset, while\nintraday trading which is characterized by fast changes that need fast response should\ndepend on a single indicator. In Table 7, we rank the performances of our system by the\nNP they produced for each of the examined trading bars.", "tags": []}
{"fragment_id": "F_R14_p9_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p9:1", "text": "Mathematics 2022, 10, 2037 9 of 11\nTable 6. Results of Multi-Layers trading algorithm.\nMinutes Bitcoin Ethereum BNB Solana Average\n5\nNP −884 219 −25.7 −4.65\nPF 0.95 1.17 0.87 0.94 0.98\nPP 29.6% 34.8% 31.8% 29.7% 31.5%\n15\nNP 2281 71 29 27.3\nPF 1.08 1.03 1.13 1.29 1.13\nPP 29.4% 30.9% 38.3% 38.7% 34.3%\n30\nNP 64.6 419 −16 −12.4\nPF 1.01 1.10 0.97 0.94 1.00\nPP 28.4% 35% 37.5% 34.4% 33.8%\n45\nNP 18,502 * 1128 * −262 244 *\nPF 1.10 1.09 0.77 1.51 1.12\nPP 33.7% 34.2% 28.6% 36.3% 33.2%\n60\nNP −16,744 −2916 0.17 163\nPF 0.90 0.80 1.0 1.36 1.02\nPP 30.4% 33% 35% 34.2% 33.2%\n120\nNP −41,109 420 103 * 100.7\nPF 0.72 1.05 1.25 1.29 1.08\nPP 29.2% 30.3% 43.6% 32.6% 33.9%\nD\nNP 62,581 1499 92.5 163\nPF 4.15 1.63 1.53 3.73 2.76\nPP 50% 48% 44.6% 44.5% 46.7%\nNotes: NP = dollar value Net Proﬁt, PF = Proﬁt Factor, PP = Percent of Proﬁtable trade of all trades. Minutes = the\nminutes time frame for each bar, D = daily bars, * = intraday highest dollar value NP .\nTable 7. Summarizing table.\nMinutes RANK Bitcoin Ethereum BNB Solana\n5\n1 CMF CMF IC CMF\n2 MUL MACD MACD\n3 IC + MACD\n15\n1 MUL MUL IC + CMF MACD\n2 IC + CMF MUL MUL\n3 IC + MACD IC + CMF\n4 IC\n5 IC + MACD\n30\n1 IC IC + MACD MACD MACD\n2 IC + MACD IC CMF IC + MACD\n3 CMF MUL IC IC\n4 MUL IC+CMF IC + MACD\n45\n1 CMF CMF MACD IC + CMF\n2 IC IC MUL\n3 IC + CMF IC + MACD IC\n4 IC + MACD IC + CMF IC + MACD\n5 MUL MUL MACD\n6 MACD\n60\n1 IC + MACD MACD MACD MACD", "tags": []}
{"fragment_id": "F_R14_p9_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p9:2", "text": "2 IC CMF IC IC + MACD\n3 CMF MUL\n4 MACD IC\n5 IC + CMF\n120\n1 MACD IC MACD\n2 MUL MUL IC\n3 IC + CMF IC + MACD\n4 MUL\n5 IC + CMF\nD\n1 IC + MACD IC + CMF IC IC + MACD\n2 IC IC CMF IC\n3 MUL IC + MACD IC + CMF IC + CMF\n4 IC + CMF MACD MUL MUL\n5 MACD MUL IC + MACD MACD\n6 CMF CMF CMF\nNotes: IC = Ichimoku Cloud, MACD = Moving Average Convergence Divergence, CMF = Chaikin Money Flow.\nMUL = Multiple Layers system.", "tags": []}
{"fragment_id": "F_R14_p10_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p10:1", "text": "Mathematics 2022, 10, 2037 10 of 11\nResults of Table 7 indicate that for intraday trading, single indicators were ranked 1\nmore frequently (MACD in 9 cases, CMF in 5 cases, and IC in 3 cases) than the dual indicator\nsystems (IC + MACD in 3 cases and IC + CMF in 1 case). Moreover, The MUL system\nachieved the highest rank only in 2 cases, trading 15 min bars. Those results strengthen our\nﬁnding that the intraday trading system in most cases should be based on a single indicator.\nRegarding daily bar trading, the dominant system relies on two indicators: IC + MACD for\nBitcoin and Solana and IC + CMF for Ethereum while BNB is best traded using a system\nthat relies only on a single indicator (IC).\n5. Conclusions\nOur aim in this paper is to determine the optimal number of technical tools to achieve\nthe best trading results for both swing trade that uses daily bars and intraday that uses\nminutes bars. We designed ML systems that are based on RNN to analyze raw data, make\nessential mathematical calculations that are based on three technical indicators: IC, MACD,\nand CMF, and perform actual trades of four major cryptocurrencies, Bitcoin, Ethereum,\nBNB, and Solana. We started our analysis by documenting the results of trading each\nindicator as a stand-alone system, then we analyzed the results of dual systems that are", "tags": []}
{"fragment_id": "F_R14_p10_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p10:2", "text": "comprised of two indicators together. Finally, we tested a MUL system that integrates all\nthree indicators. We ﬁnd that as theorized, more indicators do not necessarily mean better\ntrading performance. On the contrary, the best trading results were achieved using only\none or two indicators. We documented that IC as a stand-alone indicator was the best\nsingle indicator to trade daily bars of all four cryptocurrencies. However, adding MACD\nto IC improved performance for Bitcoin and Solana trades while adding CMF improved\nperformances for Ethereum trades. Moreover, BNB best performing system relied solely on\nIC as a single indicator. The MUL system which is comprised of three indicators did not\nachieve a better result for trading daily bars of cryptocurrencies providing evidence that our\nnull hypothesis (H0) should be rejected. With regard to intraday trading, it was found that\na single indicator system produces the best performances. Bitcoin and Ethereum should be\ntraded using 45 min bars with CMF alone, BNB and Solana with MACD alone using 45\nand 60 min bars, respectively. These results, in our view, are sourced from the fast changes\ncharacterizing cryptocurrencies’ intraday trading. The limitation of this study is that it\nuses data from cryptocurrencies alone while the ﬁnancial markets consist of many assets;", "tags": []}
{"fragment_id": "F_R14_p10_3", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p10:3", "text": "therefore, future research should examine whether the found optimal level of indicators\nthat should be used in trading cryptocurrencies is identical in the number and nature of the\nindicators trading other ﬁnancial assets such as stocks, indices, and commodities.\nAuthor Contributions: Data curation, G.C. and M.Q.; Formal analysis, G.C. and M.Q.; Project\nadministration, G.C.; Software, M.Q.; Writing—original draft, G.C. and M.Q. All authors have read\nand agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nData Availability Statement: Upon Request.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n1. Bgargavi, R.; Gumparathi, S.; Anith, R. Relative strength index for developing effective trading strategy in constructing optimal\nportfolio. Int. J. Appl. Eng. Res.2017, 12, 8926–8936.\n2. Brooks, C.; Hoepner, A.G.F.; McMillan, D.; Vivian, A.; Simen, C.W. Financial data science: The birth of a new ﬁnancial research\nparadigm complementing econometrics? Eur. J. Financ.2018, 25, 1627–1636. [CrossRef]\n3. Hariri, R.H.; Fredericks, E.M.; Bowers, K.M. Uncertainty in big data analytics: Survey, opportunities, and challenges. J. Big Data\n2019, 6, 44. [CrossRef]\n4. Chatzis, S.P .; Siakoulis, V .; Petropoulos, A.; Stavroulakis, E.; Vlachogiannakis, N. Forecasting stock market crisis events using", "tags": []}
{"fragment_id": "F_R14_p10_4", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p10:4", "text": "deep and statistical machine learning techniques. Expert Syst. Appl.2018, 112, 353–371. [CrossRef]\n5. Karathanasopoulos, A.; Dunis, C.; Khalil, S. Modelling, forecasting and trading with a new sliding window approach: The crack\nspread example. Quant. Financ.2016, 16, 1875–1886. [CrossRef]", "tags": []}
{"fragment_id": "F_R14_p11_1", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p11:1", "text": "Mathematics 2022, 10, 2037 11 of 11\n6. Nelson, D.M.; Pereira, A.C.; De Oliveira, R.A. Stock Market’s Price Movement Prediction with LSTM Neural Networks. In Proceedings\nof the 2017 International Joint Conference on Neural Networks (IJCNN), Anchorage, AK, USA, 14–19 May 2017; p. 1419. [CrossRef]\n7. Fischer, T.; Krauss, C. Deep learning with long short-term memory networks for ﬁnancial market predictions. Eur. J. Oper. Res.\n2018, 270, 654–669. [CrossRef]\n8. Chen, K.; Zhou, Y.; Dai, F. A LSTM-Based Method for Stock Returns Prediction: A Case Study of China Stock Market. In\nProceedings of the IEEE International Conference on Big Data, Santa Clara, CA, USA, 29 October–1 November 2015; pp.\n2823–2824.\n9. Borovkova, S.; Tsiamas, I. An ensemble of LSTM neural networks for high-frequency stock market classiﬁcation. J. Forecast.2019,\n38, 600–619. [CrossRef]\n10. Kim, K.-J. Artiﬁcial neural networks with feature transformation based on domain knowledge for the prediction of stock index\nfutures. Intell. Syst. Account. Financ. Manag.2004, 12, 167–176. [CrossRef]\n11. Kim, H.Y.; Won, C.H. Forecasting the volatility of stock price index: A hybrid model integrating LSTM with multiple GARCH-type\nmodels. Expert Syst. Appl.2018, 103, 25–37. [CrossRef]\n12. Sun, J.; Xiao, K.; Liu, C.; Zhou, W.; Xiong, H. Exploiting intra-day patterns for market shock prediction: A machine learning", "tags": []}
{"fragment_id": "F_R14_p11_2", "source_id": "R14", "locator": "The_Complexity_of_Cryptocurrencies_Algorithmic_Tra.pdf:p11:2", "text": "approach. Expert Syst. Appl.2019, 127, 272–281. [CrossRef]\n13. Liu, Y.; Yang, A.; Zhang, J.; Yao, J. An Optimal Stopping Problem of Detecting Entry Points for Trading Modeled by Geometric\nBrownian Motion. Comput. Econ.2019, 55, 827–843. [CrossRef]\n14. Cohen, G. Forecasting Bitcoin Trends Using Algorithmic Learning Systems. Entropy 2020, 22, 838. [CrossRef] [PubMed]\n15. Brandvold, M.; Molnár, P .; Vagstad, K.; Valstad, O.C.A. Price discovery on Bitcoin exchanges.J. Int. Financ. Mark. Inst. Money\n2015, 36, 18–35. [CrossRef]\n16. Gkillas, K.; Katsiampa, P .; Konstantatos, C.; Tsagkanos, A. Discontinuous movements and asymmetries in cryptocurrency markets.\nEur. J. Financ.2022, 1–25. [CrossRef]\n17. Connor, J.T.; Martin, R.D.; Atlas, L.E. Recurrent neural networks and robust time series prediction. IEEE Trans. Neural Netw.1994,\n5, 240–254. [CrossRef] [PubMed]\n18. Liu, W.; Yang, L.; Hanzo, L. Recurrent Neural Network Based Narrowband Channel Prediction. In Proceedings of the 2006 IEEE\n63rd Vehicular Technology Conference, Melbourne, Australia, 7–10 May 2006; pp. 2173–2177.\n19. Ding, T.; Hirose, A. Fading channel prediction based on combination of complex-valued neural networks and chirp Z-transform.\nIEEE Trans. Neural Netw. Learn. Syst.2014, 25, 1686–1695. [CrossRef]", "tags": []}
{"fragment_id": "F_R15_p1_1", "source_id": "R15", "locator": "5244395.pdf:p1:1", "text": "1\nUsing Bitcoin Pricing Data to Create a Proﬁtable\nAlgorithmic Trading Strategy\nJustin Xu, Dhruv Medarametla\nAbstract—Bitcoin, especially as of late, has been an incredibly\nhigh rising, although incredibly volatile, currency. Because of this,\nan algorithmic trading model that can make accurate predictions\nof short-term market trends can take take advantage of the spikes\nin the bitcoin market while avoiding the sharp decreases, allowing\nfor substantial gains. In this project, we created an algorithm that\nwould predict the price of bitcoin in x minutes relative to the\ncurrent price, where x ∈ {5, 10, 20}. Our investment strategy\nwould then be to repeatedly invest for x minutes if the algorithm\nstated that the price increased, and do nothing otherwise. We\nmodeled this problem as a classiﬁcation problem, where the\ntwo categories were based on whether the price increased or\ndecreased. Three models were used: a simple logistic regression,\na logistic regression after Principal Component Analysis, and a\nneural network with one hidden layer with a ReLU activation\nfunction. In all three cases, our loss function was a weighted\nlogistic loss function. We found that each model did better than\nthe previous one, where our metric was the expected amount\nof gains we would make in x minutes following our strategy. All\nthree models signiﬁcantly beat the baseline of the average increase", "tags": []}
{"fragment_id": "F_R15_p1_2", "source_id": "R15", "locator": "5244395.pdf:p1:2", "text": "in bitcoin price in x minutes, suggesting that their implementation\nmight lead to a proﬁtable trading algorithm.\nINTRODUCTION\nThe topic of cryptocurrencies has become extremely popular\nrecently, and as cryptocurrencies have become more popular\nin the mainstream, some of their prices have greatly increased\nas well. The most popular cryptocurrency is Bitcoin, which\nhas risen by over 600% in the 6 months from June 15, 2017\nto December 15, 2017.[2]\nOne of the problems with Bitcoin and other cryptocurrencies\nis the high amount of volatility in their pricing, meaning that\nprice can ﬂuctuate signiﬁcantly over a small period of time.\nWhile this is an issue when it comes to investing long-term\nin cryptocurrency, this also means that creating an algorithmic\ntrading strategy for cryptocurrencies would be an interesting,\nand possibly proﬁtable, problem to work on. In this project, we\ndecided to focus purely on Bitcoin, as it was the cryptocurrency\nwith the most data and the highest volume of trading..\nSome of the most popular markets, such as Coinbase,\nBitﬁnex, and Bitstamp, all include fees for each trade made,\nmaking proﬁtable high frequency trades essentially impossible\ndue to the low expected returns from each trade. GDAX, a\nless-common exchange, combats these problems, as it does\nnot include any transaction fees if we only make trades rather", "tags": []}
{"fragment_id": "F_R15_p1_3", "source_id": "R15", "locator": "5244395.pdf:p1:3", "text": "than taking them. Making trades refers to the idea of putting\na possible trade on the market, and taking trades refers to the\nidea of accepting a possible trade already on the market.\nWe scraped data from historical GDAX prices using bitcoin-\ncharts.com [1]. This website gave us the open price, the close\nprice, the high price, the low price, and the volume traded in\nevery one-minute time interval over the past year. This meant\nthat we had about 450,000 data points.\nUsing this data, we created three classiﬁcation models. The\ninput to our algorithms is a set of numeric features created\nby examining the price data for bitcoin in the time interval\n[t−80,t] and deriving information from them, such as the\naverage price in the past 40 minutes relative to the current\nprice. For more information on the exact features, refer to the\nsection on the Dataset and Features. We then use a weighted\nlogistic regression model, a weighted logistic regression model\nwith fewer features, and a neural network to output a predicted\nbinary change, which predicts whether the price at time t+ x\nis greater than or less than the price at time t, where x ∈\n{5,10,20}.\nRELATED WORK\nA. Bitcoin Predictive Classiﬁcation Approaches\nWe read through some previous bitcoin pricing papers from\npast 229 projects and other scholarly sources. Madan, Saluja,\nand Zhao in [3], approached the problem from a classiﬁcation", "tags": []}
{"fragment_id": "F_R15_p1_4", "source_id": "R15", "locator": "5244395.pdf:p1:4", "text": "perspective. First, they predicted the sign of price change\nin 1 day, and got great accuracy, around 98%, but when\ntransitioning to 10 minute data, they noticed it was much more\nvolatile, and only got around 55%, showing that predictive\nbitcoin pricing not to be a simple problem to solve. This\nclassiﬁcation approach was similar to ours, however instead of\naiming to maximize accuracy, because we were implementing\na simple investment strategy, we decided to maximize for gains\ninstead and let accuracy be a secondary measure.\nIn Shah and Zhang’s paper in [4], they used Bayesian regres-\nsion to create a predictive cost for the cost in n minutes directly.\nThey compared regular linear regression, lasso regression, and\nsettled on utilizing bayesian regression and latent variables to\nﬁnd price ratios and used that as a predictive measure. This\ninspired us to also approach the price prediction from a ratio\nbased perspective rather than as a purely price difference way.\nThey saw a doubling of investment in around a 60 day period,\nwhich was reasonable.\nB. Utilizing Latent Variables or Trend Data\nAnother paper referenced utilized metrics that assisted in\ngeneralized time series classiﬁcation, which also seemed rel-\nevant to our problems, like trends in the twitter activity, like\nthe paper by Chen, Nikolav, and Shah [5], which approached", "tags": []}
{"fragment_id": "F_R15_p1_5", "source_id": "R15", "locator": "5244395.pdf:p1:5", "text": "the problem from analyzing the latent variables, or Qian and\nZheng in [7] approach in trying to predict market trends after\nthe release of a ﬁnancial report.", "tags": []}
{"fragment_id": "F_R15_p2_1", "source_id": "R15", "locator": "5244395.pdf:p2:1", "text": "2\nThis made us consider implementing these types of vari-\nables, but decided on a more quantitative approach utilizing\nonly the data we received in order to create a more generalized\nmodel, as well as to better understand the relationships that our\nnumerical data could tell us, which seemed more in line with\nthe vision of our project.\nC. Generalized Stock Market Strategies\nWe also researched other academic papers to generalized\nstock market changes. Thakur, Vadpey, and Ayyar [6] analyzed\nthe pros and cons of using different machine learning tech-\nniques, like linear regression, ridge, lasso, K-nearest neighbors,\nand boosting. Results showed incremental beneﬁts from an\n82% benchmark from logistic regression. However, when\nanalyzing our problem, we noticed that bitcoin was much\nmore volatile and the ˜51% benchmark that we saw from linear\nregression made some of these techniques not super relevant\nto our problem.\nDATASET AND FEATURES\nWe initially had data for 408,960 1-minute intervals, span-\nning from January 1, 2017 to October 11, 2017, which came\nfrom [1]. For each minute t, we calculated the 66 following\nfeatures:\n• Hn: High price in interval [t−n,t]\nPrice at time tfor n∈{1,2,5,10,20,40,80}\n• Ln: Low price in interval [t−n,t]\nPrice at time tfor n∈{1,2,5,10,20,40,80}\n• An: Average price in interval [t−n,t]\nPrice at time tfor n∈{5,10,20,40,80}", "tags": []}
{"fragment_id": "F_R15_p2_2", "source_id": "R15", "locator": "5244395.pdf:p2:2", "text": "• Vn: V olume of bitcoin traded in interval[t−n,t] in BTC\nfor n∈{1,2,5,10,20,40,80}\n• Pn: Proportion of increases in price over each minute\ninterval in interval [t−n,t]\nfor n∈{1,2,5,10,20,40,80}\n• ACn: Proportion of increases in change in price over\neach minute interval in interval [t−n,t]\nfor n∈{1,2,5,10,20,40,80}\n• Rn: Price at time t−n\nPrice at time tfor n∈{1,2,5,10,20,40,80}\n• WAPn: Average price over interval [t−2n,t −n]\nAverage price over interval [t−n,t]\nfor n∈{1,2,5,10,20,40}\n• APn: Average price over interval [t−2n,t −n]\nPrice at time tfor n∈{1,2,5,10,20,40}\n• VRn: V olume n minutes ago\nCurrent volumefor n∈{1,2,5,10,20,40}\nAfter the creation of these features, we realized that for most\nfeatures, there were a small number of data points ( < 0.1%)\nfor which the values of the feature were extreme outliers. As\na result, we decided to create a cap on the values that each\nfeature could take; our range for each feature included greater\nthan 99.9% of the dataset in all cases. We then changed the\noutlier values to one of the endpoints of the range, depending\non whether the outlier was less than or greater than the range.\nFor each minute t, we also calculated the 3 following output\nvariables:\n• RVx: log(Price at time t+ x\nPrice at time t ) for x∈{5,10,20}\nSome of the data was stale and a copy of the previous\ninterval’s data, due to the fact that there were occasional", "tags": []}
{"fragment_id": "F_R15_p2_3", "source_id": "R15", "locator": "5244395.pdf:p2:3", "text": "1-minute intervals where no trading had occurred or where\nGDAX had not recorded the data. As such, we created a\nfreshness factor f for each time interval, where we set f(t) to\nbe the minimum k such that the time interval t−k had fresh\ndata (note that for the vast majority of the time, f(t) = 0 ).\nWe then removed data for which ∑9\ni=0 f(t−i) ≥10. After\nthis cleaning, we also removed roughly 200 examples at the\nend of the dataset corresponding to the beginning of the year.\nWe ended up with a total of 435300 data points.\nOf this data, we divided the data into train/dev/test with\nthe ratio 60/20/20. Because the data was time-series, it was\nimportant to keep similar times together in each set to make\nour results generalizable, so this was done without randomizing\nthe data; that is, the ﬁrst 60% of the data became the training\nset, the next 20% the dev set, and the last 20% the test set.\nSo, the training set had size 261300, and the dev set and test\nset had size 87100 each.\nNote that in one of our models, we applied PCA to the\ntraining dataset. In this case, we applied PCA on the training\ndataset, and picked the ﬁrst l principal components such\nthat transforming the dev set to have exactly the l features\ncreated by multiplying the original dev set by the l principal\ncomponents resulted in the highest possible gains.\nI. M ETHODS\nA. Baselines", "tags": []}
{"fragment_id": "F_R15_p2_4", "source_id": "R15", "locator": "5244395.pdf:p2:4", "text": "We decided to make our baseline measurement the average\ngains and accuracy that we received from choosing to always\ninvest. Because Bitcoin is steadily rising in value, this baseline\nwould measure the natural appreciation of Bitcoin, and the\nnatural gains that we would receive by simply holding on to\nBitcoin.\nOur second baseline that we used was a simple heuristic\nthat we manually calculated. We analyzed the sign of change in\nprice from 10 minutes ago and compared it to the current price.\nIf the price was trending up, we invested. This was essentially\na one feature classiﬁcation problem, and it was a reasonable\nmetric for prediction that we wanted to beat out.\nWe initially approached this problem from a purely price\npredicting manner. We took our features and current price and\ndirectly predict the future price as ratio using a regular linear\nregression, meaning our loss function was the least squared\ndifference from actual price ratio 5,10,20 minutes from the", "tags": []}
{"fragment_id": "F_R15_p3_1", "source_id": "R15", "locator": "5244395.pdf:p3:1", "text": "3\ncurrent time. However, due to the fact that prices ﬂuctuated\nwildly, our preliminary tests showed grim results, as we could\nnot even beat our baseline, and we weren’t optimizing the\nmetric that we intended, which was our total gains.\nB. Weighted Logistic Regression\nThus, we decided to transition into a weighted logistic\nregression model based on the sign of the price change, so that\nour loss function would be accurately correlate to the gains we\nwere making with our strategy of either investing at each time\nstep of doing nothing.\nWe made a prediction of the sign of the price change\nby applying the indicator function on outputs, and using the\nabsolute value of the outputs as the individual weights. This\nway the loss function that we are minimizing is calculated as\nbelow, and our logistic regression aims to minimizing loss,\nwhich correlates to us maximizing our gains:\nL(ˆy,y) =\n∑\ni\nwi(−(zi log( ˆyi) + (1−zi) log(1−ˆyi)))\nwi = |yi|,zi = 1[yi >0]\nAfter running the weighted regression, we deﬁned a few\nmetrics; namely, the weighted accuracy WA and gains, G.\nThe weighted accuracy is a weighted measurement to judge\nthe accuracy of our algorithm in proportion to the gains that\nwe’re receiving. The gains correspond to the average price ratio\nincrease we get each time step.\nWA(ˆy,y) =\n∑\ni\nwi1[|ˆyi −zi|<0.5]\n∑\ni\nwi\nG(ˆy,y) =\n∑\ni\nyi1[ ˆyi >0.5]\nC. Principal Component Analysis", "tags": []}
{"fragment_id": "F_R15_p3_2", "source_id": "R15", "locator": "5244395.pdf:p3:2", "text": "In our second model, we applied Principal Component Anal-\nysis (PCA) to the data in order to ﬁgure out the correlations\nbetween our features and remove the noise in our data set.\nFirst we normalized our data by subtracting the mean of each\nfeature and dividing by the standard deviation of each feature\nfrom each training point.\nWe then created the covariance matrix\n1\nmΣm\ni=1xix⊤\ni\nwhere mis the number of time steps we have in the training\nset, and xi represented the features from the ith training data\npoint. To ﬁnd the k principal components, we took the k\neigenvectors corresponding to the k largest eigenvalues.\nBy looking at the most signiﬁcant eigenvectors, we projected\nthe relationships between our features and did some self\nselection to remove either low variance features, or highly\ncorrelated features. We decided on removing W AP, VR, and R\nas a result of our analysis.\nThen, we projected each data point onto the principal\ncomponents by taking the matrix product between the training\npoints and the matrix of concatenated eigenvectors, which got\nus our new feature set, and once again ran weighted linear\nregression.\nGiven that vi is our ith principal component, our new\nfeatures for our data would be as below for our k principal\ncomponents and X is the m by p matrix containing all our m\ntraining examples and their p features:\n[Xv1 Xv2 ... Xv k]", "tags": []}
{"fragment_id": "F_R15_p3_3", "source_id": "R15", "locator": "5244395.pdf:p3:3", "text": "To ﬁnd the optimal number of principal components to use,\nwe utilized our development set in order to ﬁnd the optimal k\nthat maximized the gains for our development set.\nD. Neural Network\nWe then decided to create a neural network because we\nsuspected that the predictive model based on our features\nwasn’t strictly linear. We constructed a single hidden layer\nneural network with a rectiﬁer (ReLU) activation function for\nthe hidden layers and sigmoid activation for the output layer.\nThe choice of sigmoid for the output layer was to mimic\nthe logistic regression we were previously running, and we\nlikewise utilized the same loss function such that we were\noptimizing for gains again. We chose ReLU because it is\ntypically used for neural networks, as it is much faster at\ntraining than other non-linear functions, and our inputs were\nall real numbers.\nWe then used mini-batch gradient descent. We divided the\ntraining data into 60 batches, and ran backpropagation on each\nbatch. We did this for a total of 30 epochs. Our approach in\nterms of evaluation and convergence were the same, but we\nwere instead utilizing a more complicated model that would\nbe a better predictor for the data.", "tags": []}
{"fragment_id": "F_R15_p4_1", "source_id": "R15", "locator": "5244395.pdf:p4:1", "text": "4\nForward Propagation Formulas for our Neural Network:\nZ1 = XW1 + b1\nA1 = ReLU(Z1)\nZ2 = A1W2 + b2\nˆy= σ(Z2)\nBackward Propagation gradients for Neural Network:\nδ2 = |y|◦(ˆy−y)\nδ1 = δ2WT\n2 ◦1[A1 >0]\n∇W1 = XT δ1 + 2λW1\n∇b1 = δ1\n∇W2 = AT\n1 δ2 + 2λW2\n∇b2 = δ2\nRESULTS AND DISCUSSION\nTo evaluate our models, we used the following three metrics:\n• Weighted Average (formula given in previous section)\n• Gains (formula given in previous section)\n• AUC: The area under the curve measuring the true\npositive rate vs. the false positive rate\nGains was our primary measure of success, as it was\nindicative of how much money we would get for our trading\nstrategy. The weighted accuracy was our secondary measure,\nwhich adjusted the classiﬁcation accuracy based on the weights\nthat we used in our loss function\nHere are the results: Weighted Accuracy\nModel x= 5 x= 10 x= 20\nLogistic Regression 0.582 0.555 0.536\nLogistic Regression with\nPCA\n0.585 0.556 0.538\nNeural Network 0.592 0.564 0.546\nGains\nModel x= 5 x= 10 x= 20\nBaseline 3.06·10−5 6.20·10−5 1.25·10−4\nLogistic Regres-\nsion\n1.51·10−4 1.67·10−4 1.95·10−4\nLogistic Regres-\nsion with PCA\n1.56·10−4 1.71·10−4 2.02·10−4\nNeural Network 1.68·10−4 1.91·10−4 2.31·10−4\nAUC\nModel x= 5 x= 10 x= 20\nLogistic Regression 0.592 0.563 0.541\nLogistic Regression with\nPCA\n0.599 0.571 0.548\nNeural Network 0.623 0.592 0.565", "tags": []}
{"fragment_id": "F_R15_p4_2", "source_id": "R15", "locator": "5244395.pdf:p4:2", "text": "The weighted logistic regression worked well as a model\nfor giving us gains signiﬁcantly higher than the baseline price\nincrease of bitcoin. Our other two models showed incremental\nchanges in weighted accuracy, gains, and AUC. Our PCA\nanalysis worked as intended in removing some of the noise\nthat was evident in our data, and also helped mitigate some of\nthe variance present between our train and development set, as\nit allowed us to optimize the number of principal components\nin order to ﬁt the development set.\nThe neural network performance metric consistently out-\nperformed our other two metrics as seen in the tables. Our\nassumption that the model wasn’t really linear was slightly\nhelped by utilizing the non-linear activation functions of the\nneural network.\nWe overﬁt our data slightly to both our training and devel-\nopment set due to the fact that in a time series data, the later\ntime steps weren’t from exactly the same distribution as the\nﬁrst 80% that we got from the training and development data.\nThis was also evident because we used the train set to\noptimize our weights, and the development set to optimize our\nparameters, and was why our test set results were consistently\nworse than both training and development set metrics.\nHowever, the intent of optimizing parameters for develop-\nment set was to mitigate some of that over-ﬁtting, and overall", "tags": []}
{"fragment_id": "F_R15_p4_3", "source_id": "R15", "locator": "5244395.pdf:p4:3", "text": "we still kept our metrics close, meaning our model wasn’t a\nperfect generalized model for the price data, but it came close.\nCONCLUSIONS AND FUTURE WORK\nAll three of our models had gains that signiﬁcantly outper-\nformed the average increase per minute in bitcoin, suggesting\nthat their usage on the market could result in larger gains than\nsimply buying and holding bitcoin.\nReducing the number of features and implementing PCA\nincreased our average gains by a small, yet nontrivial, amount,\nwhile implementing the neural network signiﬁcantly increased\nour gains. Both models beat out our basic weighted logistic\nregression, and performed signiﬁcantly better than our second\nbaseline based on analysis of price x minutes ago.\nA next step we want to take to validate our model is to\nrun it on the current GDAX bitcoin data from the past 2\nmonths, which we did not include in our dataset. Because\nbitcoin encountered a signiﬁcant spike during this period of\ntime, it would be interesting to check if our algorithm was\nstill valid.", "tags": []}
{"fragment_id": "F_R15_p5_1", "source_id": "R15", "locator": "5244395.pdf:p5:1", "text": "5\nFurthermore, we are considering possibly implementing a\nrecurrent neural network, as it would draw upon the time series\ndependency of data. We want to research the implementation\nof an algorithm based on our model on GDAX to test our\nmodel in real time and possibly make signiﬁcant returns.\nCONTRIBUTIONS\nBoth of us did preliminary research into the bitcoin market\nand the potential ways to implement our algorithmic strategies.\nWe also both formulated the features that we based upon our\ntime data, and did most of the coding and analysis of our\nmodels together. The poster design and template was started\nby Justin and ﬁnishing edits were made by Dhruv.\nJustin did further research onto current papers, and the\napproaches that some people had in tackling bitcoin and gen-\neralized stock market algorithmic trading. He also suggested\nusing PCA to assist in feature selection and utilization of the\ndevelopment set, as well as remove noise from our linear\nmodel. Justin also did the data scraping from bitcoincharts to\ncollect our data, as well as some preliminary excel algorithm\nanalysis to parse features. He also researched into neural\nnetwork packages to see what was in the realm of possibility\nof neural network frameworks before the ﬁnal design.\nDhruv came up with the ideas for the trading strategy that\nwould ﬁt our machine learning framework, as well as the", "tags": []}
{"fragment_id": "F_R15_p5_2", "source_id": "R15", "locator": "5244395.pdf:p5:2", "text": "corresponding loss function and the gains metrics for our lo-\ngistic regression models. He also performed some preliminary\nanalysis in R to ﬁgure out which features mapped were great\npredictors by themselves, as well as looking at the shapes of\nall the output and input data, and decided on the cutoff points\nfor the outlier data for each feature. For the neural network, he\ndid the math for the propagation formulas to implement our\nbatch gradient descent.\nREFERENCES\n[1] Bitcoincharts.com. (2017). Bitcoincharts Charts. [online] Available at:\nhttps://bitcoincharts.com/charts\n[2] Bitcoin Price Index - Real-time Bitcoin Price Charts. [online] CoinDesk.\nAvailable at: https://www.coindesk.com/price/ [Accessed 16 Dec. 2017].\n[3] Madan, Isaac, Shaurya Saluja, and Aojia Zhao. Automated Bitcoin\nTrading via Machine Learning Algorithms. (n.d.): 1-6. CS229. Web.\n[4] Shah, Devavrat, and Kang Zhang. Bayesian regression and Bitcoin. arXiv\npreprint arXiv:1410.1231 (2014)\n[5] G. H. Chen, S. Nikolov, and D. Shah, A latent source model for non-\nparametric time series classiﬁcation , in Advances in Neural Information\nProcessing Systems , pp. 10881096, 2013.\n[6] Thakur, Saumitra, Theo Vadpey, and Sandeep Ayyar. Predicting Stock\nPrices and Analyst Recommendations (n.d.): 1-6. CS229. Web.\n[7] Qian, Chen and Wenjie Zhang. Stock Market Trends Prediction after\nEarning Release (n.d.): 1-6. CS229. Web.", "tags": []}
{"fragment_id": "F_R16_p1_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p1:1", "text": "A Comprehensive Analysis of Machine Learning\nModels for Algorithmic Trading of Bitcoin\nAbdul Jabbar and Syed Qaisar Jalil\nAbstract—This study evaluates the performance of 41 machine\nlearning models, including 21 classifiers and 20 regressors, in\npredicting Bitcoin prices for algorithmic trading. By examining\nthese models under various market conditions, we highlight their\naccuracy, robustness, and adaptability to the volatile cryptocur-\nrency market. Our comprehensive analysis reveals the strengths\nand limitations of each model, providing critical insights for\ndeveloping effective trading strategies. We employ both machine\nlearning metrics (e.g., Mean Absolute Error, Root Mean Squared\nError) and trading metrics (e.g., Profit and Loss percentage,\nSharpe Ratio) to assess model performance. Our evaluation\nincludes backtesting on historical data, forward testing on recent\nunseen data, and real-world trading scenarios, ensuring the\nrobustness and practical applicability of our models. Key findings\ndemonstrate that certain models, such as Random Forest and\nStochastic Gradient Descent, outperform others in terms of profit\nand risk management. These insights offer valuable guidance for\ntraders and researchers aiming to leverage machine learning for\ncryptocurrency trading.\nIndex Terms—Bitcoin, Machine Learning, Trading Strategies\nI. I NTRODUCTION", "tags": []}
{"fragment_id": "F_R16_p1_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p1:2", "text": "The advent of Bitcoin and the subsequent proliferation of\ncryptocurrencies have not only disrupted traditional financial\nsystems but also introduced novel paradigms in asset trading.\nCryptocurrencies, led by Bitcoin, have carved a niche in\nfinancial markets, attracting attention from both retail and\ninstitutional investors. The allure of high returns, coupled\nwith the inherent volatility of these digital assets, has spurred\nthe development of sophisticated trading strategies. Among\nthese, algorithmic trading, leveraging the prowess of machine\nlearning models, has emerged as a key player in navigating\nthe cryptocurrency market landscape [1].\nBitcoin, the forerunner in this domain, presents a unique\nblend of challenges and opportunities for traders. Its decentral-\nized nature, coupled with the absence of regulatory oversight,\nresults in significant price fluctuations. This volatility, while\nposing risks, also creates opportunities for substantial gains,\nmaking Bitcoin an attractive asset for algorithmic trading\nstrategies. These strategies, which were once the domain\nof sophisticated institutional traders, are now increasingly\naccessible to a wider audience, thanks to advancements in\ncomputational power and machine learning techniques.\nThe integration of machine learning in trading strategies for\nBitcoin and other cryptocurrencies represents a significant shift", "tags": []}
{"fragment_id": "F_R16_p1_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p1:3", "text": "from traditional trading approaches. Machine learning models\noffer the capability to process and learn from vast datasets,\nincluding historical price movements, trading volumes, and\nDr. A. Jabbar, and Dr. S.Q. Jalil are with Neurog LLP. E-mails: abduljab-\nbar@neurog.ai, syedqaisarjalil@neurog.ai\nmarket sentiments. This ability to extract meaningful patterns\nand insights from complex and often noisy data is crucial\nin predicting future market behavior and making informed\ntrading decisions [2].\nOur research delves deep into the realm of algorithmic\ntrading for Bitcoin, employing a range of machine learning\nmodels. The primary aim is to critically analyze the per-\nformance of these models in the context of Bitcoin trading.\nWe explore various aspects of these models, including their\npredictive accuracy, response to market volatility, and the\neffectiveness of different feature sets. In doing so, this study\nsheds light on the nuances of algorithmic trading in the\ncryptocurrency market and provides a roadmap for traders and\ninvestors in navigating this volatile yet potentially lucrative\ndomain.\nA key motivation behind this study is the growing interest in\ncryptocurrency trading and the need for robust trading strate-\ngies that can adapt to the dynamic nature of these markets.\nThe extreme volatility of cryptocurrencies, while a deterrent", "tags": []}
{"fragment_id": "F_R16_p1_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p1:4", "text": "for some, presents a fertile ground for algorithmic trading\nstrategies. Machine learning models, with their adaptability\nand learning capabilities, are well-suited to capture the intri-\ncacies of these markets. However, it is imperative to critically\nassess the performance of different machine learning models,\neach with its unique strengths and limitations, to identify the\nmost effective strategies for cryptocurrency trading.\nOur research contributes to the existing literature by pro-\nviding a comprehensive analysis of various machine learn-\ning models in the context of Bitcoin trading. We not only\nassess the performance of these models but also explore the\nimplications of their results in practical trading scenarios. This\nincludes considerations of market volatility, transaction costs,\nand other factors that impact on the trading outcomes.\nThe structure of the paper is as follows: In Section 2,\nwe present a detailed literature review, examining previous\nwork in this field and identifying gaps that our study aims to\nfill. Section 3 describes the methodology, including the data\nsources, machine learning models employed, and the evalua-\ntion criteria used. Section 4 discusses the results, providing an\nin-depth analysis of the performance of each model. Section\n5 offers a discussion on the implications of our findings, both", "tags": []}
{"fragment_id": "F_R16_p1_5", "source_id": "R16", "locator": "2407.18334v1.pdf:p1:5", "text": "for traders and the broader field of financial machine learning.\nFinally, Section 6 concludes the paper, summarizing our key\nfindings and suggesting avenues for future research.\nThrough this comprehensive exploration, our study aims to\nnot only advance the understanding of algorithmic trading in\nthe cryptocurrency sphere but also to provide practical insights\nthat can be leveraged by traders and investors.\narXiv:2407.18334v1 [q-fin.TR] 9 Jul 2024", "tags": []}
{"fragment_id": "F_R16_p2_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p2:1", "text": "II. L ITERATURE REVIEW\nThe application of machine learning (ML) techniques to\npredict cryptocurrency prices has garnered significant attention\ndue to the volatile nature of these markets and the potential for\nsubstantial financial returns. This section reviews key studies\nin this domain, highlighting methodologies, findings, and how\nour study advances the current state of knowledge.\nMachine learning algorithms have been extensively em-\nployed to predict Bitcoin prices, leveraging their ability to\nhandle large datasets and capture complex patterns. Several\nstudies have explored various ML models and techniques to\nenhance prediction accuracy. For example, [3] applied Sup-\nport Vector Machine (SVM) and K-Nearest Neighbor (KNN)\nalgorithms to forecast Bitcoin prices, demonstrating that SVM\noutperforms KNN in terms of accuracy. This study emphasizes\nthe importance of machine learning in producing more accu-\nrate results compared to traditional techniques. Similarly, [4]\ninvestigated the prediction of Bitcoin prices using the prices of\nother cryptocurrencies, such as Ethereum, Zcash, and Litecoin.\nThey employed cointegration analysis, regression models, and\nARIMA models to analyze price trends and found that Zcash\nperformed best in forecasting Bitcoin prices without direct\nBitcoin price information.\nHighlighting the superiority of machine learning over tra-", "tags": []}
{"fragment_id": "F_R16_p2_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p2:2", "text": "ditional methods, [5] evaluated the forecasting performance\nof various ML algorithms using high-frequency intraday data.\nThey found that SVM achieved the highest accuracy, out-\nperforming traditional models like ARIMA, especially during\nmarket turmoil such as the COVID-19 pandemic. In a different\napproach, [6] combined a high-end multi-layer perceptron\n(MLP) with various machine learning techniques to predict\nBitcoin prices. This study achieved high prediction accuracies\nusing optimization techniques and classifiers like KNN and\nSVM.\nSeveral studies have conducted comparative analyses of dif-\nferent ML models to identify the most effective techniques for\ncryptocurrency price prediction. [7] analyzed various machine\nlearning methods for predicting Bitcoin prices, highlighting\nthe superior prediction accuracy of Artificial Neural Networks\n(ANN) and SVMs compared to traditional parametric regres-\nsion approaches. Additionally, [8] evaluated SVM, KNN, and\nLight Gradient Boosted Machine (LGBM) in predicting price\nmovements of Bitcoin, Ethereum, and Litecoin. They found\nthat KNN outperformed other models in the overall dataset,\nwhile SVM and LGBM were better for specific cryptocurren-\ncies. Supporting these findings, [9] compared the effectiveness\nof Simple Moving Average (SMA) and Radial Basis Function\nNeural Network (RBFNN) methods. The study demonstrated", "tags": []}
{"fragment_id": "F_R16_p2_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p2:3", "text": "that RBFNN significantly outperforms SMA, providing a more\naccurate tool for forecasting Bitcoin prices.\nAdvanced machine learning techniques, including ensemble\nmethods, have shown promising results in predicting cryp-\ntocurrency prices. [10] explored the predictability of major\ncryptocurrencies using linear models, random forests, and\nSVMs. The study found that ensemble approaches achieve sig-\nnificant profitability, particularly during bear market periods.\nAdditionally, [11] investigated the predictability of Bitcoin\nprices using a stacking ensemble model, integrating Random\nForest and Generalized Linear Model with Support Vector\nRegression (SVR) as a meta-learner. The study achieved high\npredictive accuracy, suggesting the effectiveness of ensemble\nmethods.\nStudies have also focused on practical applications and real-\nworld testing of ML models to validate their performance and\napplicability in actual trading scenarios. [12] applied various\nML techniques, including Logistic Regression, SVM, Random\nForest, XGBoost, and LightGBM, to predict Bitcoin price\nmovements. The study highlighted the potential of ensemble\nmodels in enhancing prediction accuracy and constructing\neffective trading strategies. Similarly, [13] conducted a com-\nparative analysis of ARIMA, Facebook Prophet, and XGBoost\nto predict the monthly Bitcoin price rate. The results indi-", "tags": []}
{"fragment_id": "F_R16_p2_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p2:4", "text": "cated that Facebook Prophet outperformed the other models,\ndemonstrating high accuracy and reliability. Furthermore, [14]\nperformed a comparative analysis of machine learning models\nfor forecasting next-day cryptocurrency returns. They found\nthat SVMs provided the highest classification accuracy and de-\nveloped a probability-based trading strategy that significantly\noutperformed standalone investments.\nSome studies have integrated sentiment analysis and techni-\ncal indicators to improve the accuracy of cryptocurrency price\npredictions. For instance, [15] applied machine learning and\nsentiment analysis techniques to predict price movements of\nmajor cryptocurrencies. The study leveraged data from Twitter\nand market data, finding that neural networks outperformed\nother models. Additionally, [16] investigated the application of\nML algorithms to forecast Bitcoin price movements. The study\nfound that Random Forest achieved the highest forecasting\nperformance on continuous datasets, while ANN performed\nbest on discrete datasets.\nVarious performance metrics have been used to evaluate\nthe effectiveness of ML models in predicting cryptocurrency\nprices. [17] compared ARIMA, Facebook Prophet, and XG-\nBoost using metrics such as RMSE, MAE, and R-squared.\nThe study demonstrated that ARIMA outperformed the other\nmodels, highlighting the importance of preprocessing and fea-", "tags": []}
{"fragment_id": "F_R16_p2_5", "source_id": "R16", "locator": "2407.18334v1.pdf:p2:5", "text": "ture selection. Similarly, [18] investigated the efficacy of ML\nalgorithms in predicting Bitcoin prices. The study found that\nRF exhibited the highest forecasting accuracy on continuous\ndatasets, while ANN performed best on discrete datasets.\nWhile existing studies have significantly advanced the field\nof cryptocurrency price prediction, they often face challenges\nrelated to model robustness, overfitting, and the ability to\nadapt to rapidly changing market conditions. Our study ad-\ndresses these challenges by integrating both machine learning\nand trading metrics (e.g., Mean Absolute Error, Root Mean\nSquared Error, Profit and Loss percentage, Sharpe Ratio) to\ncomprehensively evaluate model performance. Furthermore,\nour evaluation process includes backtesting on historical data,\nforward testing on recent unseen data, and real-world testing\nto ensure robustness and practical applicability. This multi-\nfaceted evaluation approach provides a more thorough assess-\nment of model performance compared to previous studies.\nKey findings from our study demonstrate that certain mod-\nels, such as Random Forest and Stochastic Gradient Descent,", "tags": []}
{"fragment_id": "F_R16_p3_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p3:1", "text": "outperform others in terms of profit and risk management.\nThese insights offer valuable guidance for traders and re-\nsearchers aiming to leverage machine learning for cryptocur-\nrency trading, highlighting the practical benefits and improved\naccuracy of our approach. By incorporating economic indica-\ntors and considering practical trading constraints, our study\ncontributes to the development of more efficient and reliable\nalgorithmic trading strategies in the cryptocurrency domain.\nThis comprehensive evaluation framework and the integration\nof diverse metrics set our study apart from previous research,\noffering a more robust and practical solution for Bitcoin price\nprediction and trading.\nIII. M ETHODOLOGY\nA. Data\nIn machine learning, the quality and depth of data are\ncritical, especially in complex fields like financial trading.\nFor Bitcoin trading, the challenge is even more pronounced\ndue to the market’s relatively recent development and the\nlack of centralized, comprehensive historical data. To circum-\nvent these challenges, our study leverages a detailed dataset\nof Bitcoin prices, publicly available since the inception of\nBitcoin trading in 2013. This extensive dataset is invaluable\nfor training models capable of recognizing and adapting to\na wide spectrum of market conditions, which is essential for\ndeveloping sophisticated algorithmic trading strategies.", "tags": []}
{"fragment_id": "F_R16_p3_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p3:2", "text": "The dataset for this research is meticulously divided into\nthree segments: training, backtesting, and forward testing. The\ntraining dataset spans a decade, from January 2013 to January\n2023, providing a rich historical context for the models to\nlearn from. This lengthy period is crucial to encompass the\ndiverse range of market behaviors and trends Bitcoin has\nexperienced. The backtesting phase covers six months, from\nFebruary to July 2023, and is instrumental in evaluating the\nmodels on unseen data, thus testing their ability to generalize\nbeyond the training set. This is a crucial step in preventing\noverfitting. Finally, the forward testing phase, from August\nto October 2023, serves as a real-world application of the\nmodels, ensuring they are tested against new, unencountered\ndata, thereby eliminating any survivorship bias.\nTo enhance the models’ input features, the study incorpo-\nrates a range of technical indicators alongside the raw pricing\ndata. These indicators include:\n• Accumulation/Distribution Index : A volume-based in-\ndicator designed to reflect cumulative inflows and out-\nflows of money, providing insights into the strength of a\ntrend based on volume movements.\n• Money Flow Index (MFI): This indicator combines price\nand volume to identify overbought or oversold conditions\nin an asset, offering a perspective on the intensity of\nbuying or selling pressure.", "tags": []}
{"fragment_id": "F_R16_p3_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p3:3", "text": "• Bollinger Bands : A statistical chart characterizing the\nprices and volatility of an asset over time, which includes\na moving average and two standard deviation lines.\n• Keltner Channel Width : This encompasses a volatility-\nbased envelope set above and below an exponential mov-\ning average of the price, offering insights into potential\ntrend breakouts or reversals.\n• Parabolic SAR (Stop and Reverse) : This indicator is\nused to determine the direction of an asset’s momentum\nand the point in time when this momentum has a higher-\nthan-normal probability of switching directions.\nEach of these indicators provides a unique lens through\nwhich to analyze market trends and movements, and their\nincorporation is expected to enrich the feature set available\nfor our machine learning models.\nOur methodology is further characterized by the use of\nrolling windows of various sizes: 1, 7, 14, 21, and 28 days.\nThis approach ensures that our models have access to a\ndynamic, evolving view of market conditions, as each window\nencompasses the preceding n intervals of data. Such a tech-\nnique is crucial for models that need to understand and predict\nmarket trends over different time horizons. The models that\nshow the highest performance, particularly in terms of profit\nand loss (PNL) percentage across these windows, are then\nselected for detailed examination in the subsequent sections", "tags": []}
{"fragment_id": "F_R16_p3_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p3:4", "text": "of this study.\nAn important preprocessing step applied to our dataset is\nthe log difference transformation. Mathematically, this can be\nexpressed as:\n∆ log(Pt) = log(Pt) − log(Pt−1)\nwhere Pt and Pt−1 represent the price of Bitcoin at times t and\nt−1, respectively. This transformation is effective in stabilizing\nvariance, linearizing trends, and introducing stationarity to\nthe dataset, crucial for analyzing financial time series where\nunderstanding growth rates and temporal changes is important.\nFinally, the design of our dataset is intentionally made flex-\nible to accommodate various time intervals. While the primary\nfocus is on a 24-hour trading horizon, the structure is adaptable\nto different temporal scales. This flexibility showcases the\nbroad applicability of our methodology, suitable for a range\nof trading frequencies and market conditions.\nB. Machine Learning Models\nIn this research, a diverse array of machine learning classi-\nfiers and regressors has been employed to analyze and predict\nBitcoin trading patterns. Each model has been meticulously\nselected for its unique attributes and potential effectiveness\nin capturing the complexities of the cryptocurrency mar-\nket. Classifiers are tasked with determining the trading ac-\ntion—specifically, whether to buy (go long) or sell (go short).\nIn contrast, regressors focus on predicting the magnitude of", "tags": []}
{"fragment_id": "F_R16_p3_5", "source_id": "R16", "locator": "2407.18334v1.pdf:p3:5", "text": "price changes over specified intervals. To distinguish between\nthe two, we denote classifiers with a suffix ’C’ and regressors\nwith ’R’.\n1) Classifiers: The following classifiers have been em-\nployed:\n1) Ada Boost (ABC) : This ensemble method combines\nmultiple weak learners to form a stronger model, en-\nhancing performance in varied market conditions.\n2) Bagging (BGC): Uses bootstrap aggregating to improve\nstability and reduce overfitting, crucial in volatile market\nscenarios.", "tags": []}
{"fragment_id": "F_R16_p4_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p4:1", "text": "Data\nCollection\nPreprocessing\nFeature Selection\nRolling Windows\nDataset\nMachine Learning\nDevelopment(Classifiers &\nRegressors)\nHyperparameter\nOptimization\nTraining\nEvaluation\nBacktest\nForwardtest\nRealworld\nFig. 1: Overview of the Methodology: This flowchart illustrates the comprehensive process used in our study, encompassing\nthree main modules: data, machine learning, and evaluation. The data module includes all the steps from data collection to\ndataset creation, preparing the data for use by the machine learning module. The machine learning module covers model\ndevelopment and training, including hyperparameter optimization for both classifiers and regressors. The evaluation module\ninvolves rigorous backtesting on historical data, forward testing on recent unseen data, and real-world testing to validate model\nperformance and ensure practical applicability.\n3) Bernoulli NB (BNBC) : Suited for binary classification,\nit’s effective in scenarios with binary/boolean feature\nsets.\n4) Calibrated CV (CCVC) : Improves probability estima-\ntion in classification, essential for better trade decision-\nmaking.\n5) Decision Tree (DTC) : Offers a transparent, tree-\nstructured modeling approach, useful for clear interpre-\ntation of trading signals.\n6) Extra Tree (ETC) : A Random Forest variant that\nintroduces more randomness in split decisions, aiming\nto reduce model overfitting.", "tags": []}
{"fragment_id": "F_R16_p4_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p4:2", "text": "7) Gaussian Process (GPC) : Excellent for small datasets,\ncaptures complex patterns using kernel functions, suit-\nable for nuanced market analysis.\n8) K Neighbors (KNC) : A non-parametric method that\nclassifies based on the proximity to nearest neighbors,\nuseful in identifying market trends.\n9) Linear Discriminant Analysis (LDAC) : Effective in\nfinding linear combinations of features for class separa-\ntion, suitable for linearly separable market data.\n10) Linear SVC (LSVC) : Applies Support Vector Classifi-\ncation in scenarios with linear separability, efficient for\nclear market trend data.\n11) Logistic Regression (LRC) : A fundamental model for\nbinary classification, ideal for straightforward buy or sell\ndecisions.\n12) Logistic Regression CV (LRCVC) : Integrates logistic\nregression with cross-validation, optimizing for the best\nmodel parameters.\n13) MLP (MLPC): A neural network-based model, capable\nof capturing complex, non-linear relationships in market\ndata.\n14) Passive Aggressive (PAC) : Suitable for large-scale\nlearning, it updates models based on prediction errors,\nadapting swiftly to market changes.\n15) Perceptron (PC): A simple, yet effective linear classifier\nfor large datasets, efficient in handling vast market data.\n16) Quadratic Discriminant Analysis (QDAC) : Assumes\nGaussian distribution for class separation, effective in", "tags": []}
{"fragment_id": "F_R16_p4_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p4:3", "text": "markets exhibiting normal distribution patterns.\n17) Random Forest (RFC): An ensemble of decision trees,\nknown for high accuracy and robustness against overfit-\nting in complex market environments.\n18) Ridge (RC) : A linear model with L2 regularization,\nadept at handling multicollinearity in financial datasets.\n19) SGD (SGDC) : Utilizes stochastic gradient descent for\noptimized computational efficiency, crucial in high-\nfrequency trading scenarios.\n20) SVC (SVC) : Versatile in handling both non-linear and\nhigh-dimensional data, adaptable to various market con-\nditions.\n21) Radius Neighbors (RNC) : Classifies based on a fixed\nradius, useful in spatial or locality-based market analy-\nses.\n2) Regressors: The regressors used in this study are as\nfollows:\n1) Ada Boost (ABR) : Applies an ensemble technique\nfocusing on challenging data points, enhancing accuracy\nin regression tasks.\n2) Bagging (BGR): Employs bootstrap sampling to create\nmultiple models, reducing variance and improving pre-\ndictions in regression.\n3) Decision Tree (DTR) : An interpretable model for re-\ngression, useful in capturing non-linear relationships in\nprice movements.\n4) Extra Tree (ETR) : Improves on Random Forest by\nrandomizing decision trees, enhancing regression per-", "tags": []}
{"fragment_id": "F_R16_p5_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p5:1", "text": "formance in unpredictable markets.\n5) Gaussian Process (GPR) : Ideal for small datasets with\ncomplex patterns, offers probabilistic outputs beneficial\nfor risk assessment.\n6) K Neighbors (KNR) : Predicts values based on the\nproximity of neighbors, effective in markets with spatial\ncorrelation.\n7) Linear SVR (LSVR) : Adapts Support Vector Regres-\nsion for linear contexts, efficient in markets with linear\nprice movements.\n8) MLP (MLPR) : A neural network approach for model-\ning complex regression patterns in financial markets.\n9) Random Forest (RFR) : Known for high accuracy in\nregression, leveraging an ensemble of decision trees to\npredict price changes.\n10) Ridge (RR) : Utilizes L2 regularization to mitigate\noverfitting in regression, essential for stable financial\npredictions.\n11) SGD (SGDR) : Implements stochastic gradient descent\nfor efficient regression analysis in large datasets.\n12) SVR (SVRR) : A versatile kernel-based method, effec-\ntive for both linear and non-linear regression tasks in\ntrading.\n13) ARD (ARDR) : Uses Automatic Relevance Determina-\ntion to adapt regression models to the inherent structure\nof the data.\n14) Bayesian Ridge (BRR) : Combines ridge regression\nwith Bayesian inference, offering flexible modeling in\nuncertain market conditions.\n15) Gradient Boosting (GBR) : Constructs an additive\nmodel in a forward stage-wise fashion, useful in pro-", "tags": []}
{"fragment_id": "F_R16_p5_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p5:2", "text": "gressive market trend analysis.\n16) Lars (LaR) : Efficient in high-dimensional data regres-\nsion, providing solutions along a regularization path.\n17) Linear Regression (LiR) : The foundational regression\nmodel, establishing linear relationships between market\nvariables.\n18) RANSAC (RanR) : Fits models robustly to subsets\nof data, effectively dealing with outliers in financial\ndatasets.\n19) Theil Sen (TSR) : A non-parametric approach resilient\nto outliers, suitable for complex multivariate regression\nin trading.\n20) Radius Neighbors (RNR) : Utilizes a fixed radius for\nneighborhood-based regression, applicable in spatially\ncorrelated market environments.\nThe selection of these diverse models is based on their estab-\nlished effectiveness in predictive modeling, particularly in the\nfinancial markets where accuracy, adaptability, and robustness\nare of utmost importance. This wide range of models ensures\na comprehensive analysis, allowing us to identify the most\neffective strategies for Bitcoin trading prediction.\nC. Rolling Windows and Training Process\nThe concept of rolling windows is pivotal in time series\nanalysis, especially in financial markets where data is sequen-\ntial and market conditions are dynamic. A rolling window\napproach involves using a window of a fixed size that moves\nthrough the dataset over time. For each position of the window,", "tags": []}
{"fragment_id": "F_R16_p5_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p5:3", "text": "a subset of data is selected, which is then used for training\nthe model. This technique is crucial in capturing the evolving\nnature of financial markets, as it allows models to learn from\nthe most recent trends and patterns.\nIn the context of machine learning for Bitcoin trading,\nrolling windows are essential for several reasons. Firstly, they\nenable models to adapt to changing market conditions, which\nis crucial in a volatile market like Bitcoin. By training on the\nmost recent data, the models stay updated with current mar-\nket dynamics, enhancing their predictive accuracy. Secondly,\nrolling windows help in mitigating the risk of overfitting.\nModels trained on a specific period might perform well on\nthat period but fail to generalize to new data. By continuously\nupdating the training dataset, rolling windows ensure that\nmodels are not overly tuned to a specific historical period.\nIn this study, five different rolling window sizes were\nused: 1, 7, 14, 21, and 28 days. These sizes were chosen to\ncapture various market dynamics, from short-term fluctuations\nto longer-term trends. Each window size provides a different\nperspective on the data, allowing models to learn patterns\nand trends over different time horizons. For instance, a 1-\nday window focuses on very short-term movements, while a\n28-day window captures broader market trends.", "tags": []}
{"fragment_id": "F_R16_p5_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p5:4", "text": "Each machine learning model in our study was trained\nagainst each rolling window size. This process involved se-\nquentially moving the window through the entire dataset,\ntraining the model on the data within the window at each\nstep. For example, with a 7-day window, the model would be\ntrained on data from days 1 to 7, then on data from days 2\nto 8, and so on, until the end of the dataset. This approach\nensures that each model is exposed to a wide range of market\nconditions, enhancing its ability to generalize and adapt.\nThe use of multiple window sizes allows us to analyze the\nperformance of each model under different market conditions.\nIt provides insights into which models are better at capturing\nshort-term trends versus long-term trends. This is particularly\nimportant in Bitcoin trading, where market conditions can\nchange rapidly. Models that perform well across multiple\nwindow sizes are likely to be more robust and versatile,\nmaking them more reliable for real-world trading applications.\nAfter training, each model’s performance was evaluated\nbased on its predictive accuracy within each window. The\nmodel with the highest performance in terms of predictive\naccuracy and profitability (PNL) for each window size was\nthen selected for further analysis. This approach allows us\nto identify the most effective models for Bitcoin trading,", "tags": []}
{"fragment_id": "F_R16_p5_5", "source_id": "R16", "locator": "2407.18334v1.pdf:p5:5", "text": "considering both short-term and long-term market behaviors.\nD. Hyperparameter Optimization\nHyperparameters are the configurable settings used to tune\nthe performance of machine learning models. Unlike model\nparameters, which are learned during training, hyperparam-\neters are set prior to the training process and can have a\nsignificant impact on the effectiveness of the models. Proper\nhyperparameter optimization is critical in machine learning,", "tags": []}
{"fragment_id": "F_R16_p6_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p6:1", "text": "particularly in financial applications like Bitcoin trading,\nwhere the optimal model configuration can substantially in-\nfluence predictive accuracy and profitability.\nFor the purpose of hyperparameter optimization in this\nstudy, we employed Optuna [19], an open-source hyperparam-\neter optimization framework. Optuna is designed for automat-\ning the process of finding the best hyperparameters, making it\nan ideal tool for our complex machine learning tasks. It uses a\nBayesian optimization technique to search the hyperparameter\nspace efficiently, focusing on combinations that are more likely\nto yield better model performance. This approach is especially\nbeneficial given the large number of models and the extensive\nrange of hyperparameters involved in our study.\nIn our implementation with Optuna, each model underwent\n100 trials of hyperparameter tuning. In each trial, Optuna var-\nied the hyperparameters within predefined ranges, searching\nfor the combination that maximized the model’s performance.\nThe hyperparameters varied included learning rates, regular-\nization strengths, the number of layers and neurons in neural\nnetwork models, and other model-specific parameters. The\nvariation in these hyperparameters was guided by Optuna’s\noptimization algorithm, which adapted its search strategy\nbased on the results of previous trials, thereby progressively", "tags": []}
{"fragment_id": "F_R16_p6_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p6:2", "text": "honing in on the most promising hyperparameter values.\nThe primary metric for evaluating the performance of the\nmodels during the hyperparameter optimization process was\nthe Profit and Loss (PNL) percentage. PNL was chosen as it\ndirectly reflects the financial efficacy of the models in trading\nscenarios. For each model, the hyperparameter combination\nthat yielded the highest PNL percentage during the backtesting\nphase was identified as the optimal set. This approach ensured\nthat the selected hyperparameters were not only statistically\neffective but also financially practical in terms of trading\nperformance.\nThe optimization of hyperparameters is particularly im-\nportant in the volatile and unpredictable domain of Bitcoin\ntrading. Bitcoin markets exhibit unique characteristics and can\nbehave differently from traditional financial markets. There-\nfore, fine-tuning the models to adapt to these idiosyncrasies\nthrough hyperparameter optimization is essential to achieve\nthe best possible predictive performance.\nE. Backtest and Forward Test Procedures\nIn financial machine learning applications, backtesting and\nforward testing are crucial steps for evaluating the effective-\nness and robustness of models. Backtesting involves testing\nthe models against historical data to assess their performance,\nwhile forward testing (also known as paper trading) tests the", "tags": []}
{"fragment_id": "F_R16_p6_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p6:3", "text": "models on more recent, unseen data to evaluate how well they\nmight perform in real-world trading scenarios.\nFor the purpose of this research, the dataset was divided\ninto three distinct segments: training, backtesting, and forward\ntesting. The training set, spanning from January 2013 to\nJanuary 2023, was used to train the models. The backtesting\nphase covered data from February to July 2023, providing a\nrecent historical dataset to evaluate the trained models. The\nforward testing phase, encompassing data from August to\nOctober 2023, served as a real-world test bed to assess the\nmodels’ performance on new, unseen data.\nIV. R ESULTS AND DISCUSSION\nA. Evaluation Metrics\nIn the domain of algorithmic trading, the performance of\nclassifiers and regressors is quantified through a series of\nestablished metrics. Each metric provides unique insights into\nthe model’s predictive accuracy, risk management, and overall\neconomic viability. Below is a detailed explanation of each\nmetric employed in this study:\n• Profit and Loss (PNL) Percentage: This metric mea-\nsures the total percentage gain or loss of a trading strategy\nover a specified period. It is calculated by summing up\nindividual trade outcomes (profit or loss) and dividing by\nthe total investment. A positive PNL indicates profitabil-\nity, while a negative PNL suggests a loss.\n• Sharpe Ratio: Named after Nobel laureate William F.", "tags": []}
{"fragment_id": "F_R16_p6_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p6:4", "text": "Sharpe, this ratio is used to understand the return of\nan investment compared to its risk. It is calculated by\nsubtracting the risk-free rate of return from the average\nreturn of the investment and dividing the result by the\ninvestment’s standard deviation. A higher Sharpe Ratio\nindicates a more desirable risk-adjusted return [20].\n• R-squared (R2): R2 is a statistical measure that rep-\nresents the proportion of the variance for a dependent\nvariable that’s explained by an independent variable or\nvariables in a regression model. An R2 of 1 indicates\nthat the regression predictions perfectly fit the data.\n• Accuracy: In classification tasks, accuracy is the fraction\nof predictions our model got right, defined as the number\nof correct predictions divided by the total number of\npredictions. It is a useful metric when the classes in the\ndataset are nearly balanced.\n• F1 Score: The F1 score is the harmonic mean of precision\nand recall and is particularly useful when the class\ndistribution is imbalanced. It is calculated as 2 times the\nproduct of precision and recall divided by the sum of\nprecision and recall.\n• Precision: Precision is defined as the number of true\npositives divided by the number of true positives plus the\nnumber of false positives. It is a measure of a classifier’s\nexactness. A high precision relates to a low false positive\nrate.", "tags": []}
{"fragment_id": "F_R16_p6_5", "source_id": "R16", "locator": "2407.18334v1.pdf:p6:5", "text": "• Recall: Recall, also known as sensitivity or true positive\nrate, is the number of true positives divided by the number\nof true positives plus the number of false negatives. It is\na measure of a classifier’s completeness.\n• Mean Absolute Error (MAE): For regression models,\nMAE is a metric that sums the absolute differences\nbetween predicted and actual values and then takes the\naverage. It gives an idea of how wrong the predictions\nwere in terms of an average amount.\n• Mean Squared Error (MSE): MSE is the average of\nthe squares of the errors of the predictions. It penalizes\nlarger errors more than smaller ones, due to squaring each\ndifference.", "tags": []}
{"fragment_id": "F_R16_p7_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p7:1", "text": "Backtest Forwardtest\nClassifier Rolling\nwindow PNL\n(%) Sharpe R2 Accuracy F1\nscore PrecisionRecall No. of\nTrades\nPNL\n(%) Sharpe R2 Accuracy F1\nscore PrecisionRecall No. of\nTrades\nAdaBoostClassifier 21 89.26 6.47 0.92 0.53 0.64 0.51 0.88 55 -9.24 -0.81 0 0.49 0.59 0.48 0.77 40\nBaggingClassifier 28 121.73 7.17 0.89 0.6 0.65 0.57 0.74 74 -21.67 -2.78 0.5 0.53 0.59 0.51 0.7 40\nBernoulliNB 21 113.31 6.14 0.89 0.58 0.59 0.56 0.63 106 29.78 5.17 0.84 0.52 0.53 0.5 0.56 38\nCalibratedClassifierCV 28 92 7.59 0.86 0.52 0.66 0.51 0.94 24 -2.78 0.05 0.11 0.42 0.54 0.44 0.72 30\nDecisionTreeClassifier 28 62.22 3.81 0.62 0.5 0.63 0.5 0.88 41 -8.17 -1.12 0.28 0.44 0.57 0.45 0.77 28\nExtraTreeClassifier 28 103.73 6.14 0.9 0.57 0.65 0.54 0.82 49 12.16 2.46 0.22 0.49 0.58 0.48 0.74 31\nGaussianProcessClassifier21 47.36 3.42 0.57 0.52 0.58 0.5 0.69 90 -24.72 -3.37 0.52 0.43 0.46 0.42 0.51 40\nKNeighborsClassifier 28 103.84 5.44 0.96 0.56 0.57 0.55 0.61 95 -5.14 0 0.57 0.49 0.47 0.47 0.47 51\nLinearDiscriminantAnalysis28 88.7 4.65 0.85 0.51 0.58 0.5 0.67 82 -6.84 -0.23 0.24 0.5 0.54 0.48 0.6 49\nLinearSVC 28 73.36 4.12 0.81 0.53 0.55 0.52 0.58 102 -6.63 -0.21 0.49 0.5 0.54 0.48 0.6 49\nLogisticRegression 28 97.44 5.43 0.87 0.53 0.59 0.52 0.69 94 -20.12 -1.96 0.34 0.49 0.52 0.47 0.58 51\nLogisticRegressionCV 28 111.04 6.57 0.81 0.56 0.68 0.53 0.96 36 6.81 1.47 0.32 0.52 0.63 0.5 0.86 31", "tags": []}
{"fragment_id": "F_R16_p7_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p7:2", "text": "MLPClassifier 28 112.04 4.94 0.77 0.55 0.62 0.53 0.75 74 -28.13 -3.23 0.46 0.43 0.51 0.44 0.63 41\nPassiveAggressiveClassifier21 83.33 4.94 0.84 0.53 0.57 0.5 0.66 87 -40.23 -5.67 0.84 0.36 0.41 0.36 0.47 42\nPerceptron 28 75.61 4.84 0.9 0.58 0.6 0.57 0.64 90 -55.87 -8.62 0.85 0.37 0.33 0.33 0.33 35\nQuadraticDiscriminantAnalysis21 90.09 12.36 0.89 0.53 0.66 0.5 0.97 28 -3.98 -0.16 0.53 0.51 0.61 0.49 0.79 29\nRandomForestClassifier 21 87.75 6.3 0.79 0.53 0.65 0.5 0.92 40 15.38 8.68 0.84 0.52 0.66 0.5 0.98 10\nRidgeClassifier 28 94.36 4.17 0.78 0.51 0.59 0.5 0.71 74 -7.02 -0.23 0.35 0.48 0.53 0.47 0.63 51\nSGDClassifier 28 104.29 5.16 0.87 0.52 0.57 0.51 0.64 90 -15.64 -1.4 0.68 0.49 0.53 0.47 0.6 49\nSVC 28 106.92 5.24 0.81 0.52 0.63 0.51 0.84 60 -12.8 -1.34 0.85 0.46 0.56 0.46 0.72 35\nRadiusNeighborsClassifier1 26.97 13.22 0.8 0.46 0.63 0.46 0.99 6 12.78 5.08 0.08 0.49 0.65 0.48 0.98 6\nTABLE I: Performance Metrics of Classifiers: A Comparative Analysis of Backtest and Forwardtest Results\n• Root Mean Squared Error (RMSE): RMSE is the\nsquare root of the mean of the squared errors. It is com-\nmonly used in regression analysis to verify experimental\nresults, and like MSE, gives more weight to larger errors.\n• Number of Trades: This metric indicates the count of\ntrades executed based on the model’s recommendations.\nIt can provide an understanding of the model’s trading", "tags": []}
{"fragment_id": "F_R16_p7_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p7:3", "text": "frequency and has implications for transaction costs and\nmarket liquidity.\nThese metrics collectively provide a holistic view of the\nmodels’ performance, enabling us to not only assess the\nprofitability and accuracy of predictions but also to gauge the\nrisk and reliability of the trading strategies derived from the\nmodels.\nThese metrics were chosen to provide a comprehensive eval-\nuation of the models’ performance. PNL, Sharpe Ratio, and\nNumber of Trades directly relate to the financial effectiveness\nof the models. In contrast, R2, Accuracy, F1 Score, Precision,\nRecall, MAE, MSE, and RMSE offer insights into the models’\npredictive accuracy and error characteristics. A combination of\nthese metrics allows for a balanced assessment, considering\nboth financial viability and statistical accuracy.\nB. Classifier Results Interpretation\nTable I provides a quantitative evaluation of classifier mod-\nels over two distinct phases: backtesting and forward testing.\nThe performance of each classifier is contextualized by a set\nof metrics, and the rolling window sizes are instrumental\nin capturing temporal market dynamics. The top-performing\nmodels in each phase are highlighted, indicating their superior\nability to navigate the complexities of market prediction.\n1) Backtest Insights: The backtest phase reveals the in-\ntrinsic strength of the classifiers when applied to historical", "tags": []}
{"fragment_id": "F_R16_p7_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p7:4", "text": "data. For instance, the highlighted BaggingClassifier, with a\nrolling window of 28 days, achieved an exceptional PNL,\nsuggesting that its ensemble approach is particularly suited to\ngrasp long-term trends. Conversely, the BernoulliNB classifier\ndemonstrates a high degree of precision in the shorter rolling\nwindow of 21 days, indicating its potential effectiveness in\nshort-term market movement prediction. The MLPClassifier’s\nbalanced metrics, particularly its F1 score, suggest a well-\ntuned model that avoids overfitting, evidenced by its ability to\nmaintain high precision and recall.\n2) Forward Test Observations: The forward testing phase\nis critical for assessing the real-world applicability of the\nclassifiers. The Random Forest Classifier, which maintained\na consistent performance across both phases, indicates not\njust a strong fit to the data but also adaptability to evolving\nmarket conditions. The sharp increase in Sharpe Ratio for the\nQuadratic Discriminant Analysis and RadiusNeighborsClassi-\nfier from backtest to forward test underscores their potential\nfor yielding profitable strategies when applied in real-time,\ndespite their less impressive backtest PNL. These results\nunderscore the importance of evaluating models on unseen\ndata to gauge their practical utility.\n3) Rolling Window and Model Responsiveness: The vary-", "tags": []}
{"fragment_id": "F_R16_p7_5", "source_id": "R16", "locator": "2407.18334v1.pdf:p7:5", "text": "ing rolling window sizes play a significant role in the clas-\nsifiers’ ability to capture different market conditions. Larger\nwindows may allow classifiers to integrate longer-term trends\ninto their predictions, which can be crucial for capturing\nmacroeconomic movements that affect asset prices. Smaller\nwindows, on the other hand, may enable classifiers to react\nmore quickly to short-term market volatility, which could be\nadvantageous in rapidly changing trading environments.\n4) Interpreting the Discrepancies Between Backtest and\nForward Test Results: The highlighted models exhibit varied\nperformances when transitioning from backtest to forward test\nenvironments. Such discrepancies may stem from overfitting", "tags": []}
{"fragment_id": "F_R16_p8_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p8:1", "text": "Backtest Forwardtest\nRegressor Rolling\nwindow PNL\n(%) Sharpe R2 MAE MSE RMSE No. of\nTrades\nPNL\n(%) Sharpe R2 MAE MSE RMSE No. of\nTrades\nAdaBoostRegressor 28 94.69 7.62 0.79 0.0183 0.0007 0.0255 25 9.68 2.73 0.6 0.0117 0.0004 0.0198 16\nBaggingRegressor 21 102.04 6.56 0.92 0.0179 0.0006 0.0251 88 11.01 1.99 0.01 0.0128 0.0004 0.0205 52\nDecisionTreeRegressor 21 97.31 6.34 0.92 0.0181 0.0006 0.0252 64 13.41 2.39 0.1 0.0122 0.0004 0.0198 38\nExtraTreeRegressor 28 101.03 4.92 0.7 0.0183 0.0007 0.0255 80 -6.84 -0.38 0.81 0.0121 0.0004 0.0201 39\nGaussianProcessRegressor 28 90.8 4.89 0.73 0.0183 0.0007 0.0256 20 -0.05 -1000 0 0.0118 0.0004 0.0198 0\nKNeighborsRegressor 28 106.01 6.71 0.94 0.0186 0.0006 0.0255 76 11.62 2.09 0.13 0.0133 0.0004 0.0204 41\nLinearSVR 21 71.57 4.7 0.86 0.0181 0.0007 0.0256 93 24.49 4.13 0.3 0.0124 0.0004 0.0199 38\nMLPRegressor 28 76.92 4.6 0.86 0.1229 0.0236 0.1536 88 -20.99 -2.96 0.71 0.229 0.0768 0.2771 34\nRandomForestRegressor 28 84.01 14.67 0.91 0.0183 0.0007 0.0257 8 3.38 2.72 0.01 0.0117 0.0004 0.0198 10\nRidge 21 37.35 2.42 0.45 0.0197 0.0007 0.0264 84 20.92 3.76 0.56 0.0163 0.0005 0.0221 45\nSGDRegressor 28 81.28 5.06 0.87 0.0184 0.0007 0.0256 63 34.01 5.34 0.8 0.0117 0.0004 0.0195 38\nSVR 7 76.74 4.86 0.73 0.0272 0.0013 0.0355 81 -24.45 -2.81 0.61 0.0261 0.0012 0.0341 50", "tags": []}
{"fragment_id": "F_R16_p8_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p8:2", "text": "ARDRegression 28 76.33 4.99 0.8 0.0183 0.0007 0.0257 42 17.54 3.6 0.12 0.0119 0.0004 0.0199 13\nBayesianRidge 28 59.04 4.67 0.85 0.0185 0.0007 0.0256 47 15.2 2.67 0.09 0.0118 0.0004 0.0195 29\nGradientBoostingRegressor28 80.81 6.44 0.83 0.0185 0.0006 0.0254 30 0.43 2.36 0.05 0.0123 0.0004 0.02 6\nLars 21 48.78 3.18 0.74 0.0461 0.004 0.063 105 31.69 4.88 0.66 0.0525 0.0051 0.0711 42\nLinearRegression 28 48.98 3.14 0.5 0.1156 0.0211 0.1452 79 27.64 4.57 0.66 0.1197 0.0231 0.1519 38\nRANSACRegressor 21 47.17 2.97 0.57 0.1399 0.0316 0.1777 97 -6.12 -0.16 0.08 0.1487 0.0345 0.1857 49\nTheilSenRegressor 7 81.96 4.45 0.75 0.1429 0.0474 0.2176 70 -7.02 -0.42 0.73 0.1724 0.0564 0.2374 31\nRadiusNeighborsRegressor 1 42.09 3.65 0.67 0.0175 0.0007 0.0255 42 1.69 1.12 0.16 0.0126 0.0004 0.0211 20\nTABLE II: Performance Metrics of Regressors: Evaluating Predictive Strength Across Market Conditions\nto historical data patterns that do not extrapolate well into\nfuture market states. The BaggingClassifier, while performing\noptimally in backtesting, shows a decrease in PNL during\nforward testing. This could indicate a model finely tuned\nto past conditions but less adaptable to unforeseen market\nshifts. In contrast, the Random Forest Classifier demonstrates\nrobustness, with a more consistent PNL, suggesting a model\nthat captures underlying market drivers that persist over time.", "tags": []}
{"fragment_id": "F_R16_p8_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p8:3", "text": "5) Assessing Model Robustness and Economic Significance:\nRobustness in financial models is demonstrated by consistent\nperformance across both backtesting and forward testing.\nEconomic significance, however, is derived from the model’s\nability to produce actionable insights leading to profitable\ntrades. The BernoulliNB classifier, for instance, maintains a\nhigh PNL in both phases, reinforcing its potential for real-\nworld application. The Sharpe Ratios, especially in forward\ntest results, reflect the models’ capabilities to deliver returns\nabove the risk-free rate, which is crucial for long-term invest-\nment strategies.\nC. Regressor Results Interpretation\nIn parallel, Table II lays out the regressors’ performance,\nwhere the highlighted models exhibit noteworthy predictive\npower. Each regressor is scrutinized under metrics that collec-\ntively portray its predictive accuracy and economic impact.\n1) Backtest Insights: During the backtest period, the SG-\nDRegressor distinguished itself with a notable PNL and the\nhighest Sharpe Ratio, suggesting effective risk management\ncombined with profitability. This is further corroborated by\nits relatively high R2 value, reflecting the model’s capability\nto capture the variance in price movement effectively. The\nGradientBoostingRegressor and Lars, both highlighted for\ntheir substantial PNL, also demonstrate solid R2 scores, which", "tags": []}
{"fragment_id": "F_R16_p8_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p8:4", "text": "points to their models’ good explanatory power.\n2) Forward Test Observations: Transitioning to forward\ntesting, the SGDRegressor maintains a strong performance,\nindicating robustness and potential for real-world application.\nThe Lars regressor shows an increase in both PNL and Sharpe\nRatio, suggesting that its simpler, linear approach is well-\nsuited for the forward test market conditions. The consistency\nin the performance of the RadiusNeighborsRegressor, with\nminimal trades, accentuates its precision in trade selection,\nwhich is vital for strategies aiming to minimize transaction\ncosts.\n3) Rolling Window and Model Predictive Dynamics: The\nregressors’ results highlight the significance of selecting an\nappropriate rolling window size, which directly influences\ntheir ability to assimilate and predict based on the market’s\nhistorical data. The rolling window’s impact is evident in the\nmodels’ varied performance across the two testing phases,\nwith different window lengths aligning with specific market\nbehaviors that the models have learned to predict.\n4) Analysis of Regressor Robustness: The robustness of\nregressors is evaluated through their ability to maintain predic-\ntive accuracy from backtesting to live-market forward testing.\nThe SGDRegressor, with its high Sharpe Ratio and con-\nsistent PNL, exemplifies a model with a stable foundation,", "tags": []}
{"fragment_id": "F_R16_p8_5", "source_id": "R16", "locator": "2407.18334v1.pdf:p8:5", "text": "likely to withstand market volatilities. The Rolling Window’s\nsignificance is evident in the models’ ability to incorporate\nrelevant market data into their predictive framework, with\nlonger windows capturing more extensive market trends.\n5) Economic and Predictive Implications: The economic\nimplications of the regressors’ performance are multifaceted.\nA high PNL is desirable but must be coupled with low\npredictive error metrics, such as MAE and RMSE, to be eco-\nnomically significant. The Lars model, for instance, illustrates\nthis with an improved Sharpe Ratio and a lower RMSE in\nforward testing, suggesting a model that not only forecasts", "tags": []}
{"fragment_id": "F_R16_p9_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p9:1", "text": "/uni00000029/uni00000048/uni00000045/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000030/uni00000044/uni00000055/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000024/uni00000053/uni00000055/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000030/uni00000044/uni0000005c/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni0000002d/uni00000058/uni00000051/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni0000002d/uni00000058/uni0000004f/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000024/uni00000058/uni0000004a/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000036/uni00000048/uni00000053/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000032/uni00000046/uni00000057/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000031/uni00000052/uni00000059/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000027/uni00000048/uni00000046/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni0000002d/uni00000044/uni00000051/uni00000003/uni00000015/uni00000013/uni00000015/uni00000017\n/uni00000015/uni00000013\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013", "tags": []}
{"fragment_id": "F_R16_p9_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p9:2", "text": "/uni00000014/uni00000015/uni00000013/uni00000033/uni00000031/uni0000002f/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000025/uni0000002a/uni00000026\n/uni00000025/uni00000031/uni00000025/uni00000026\n/uni00000030/uni0000002f/uni00000033/uni00000026\n/uni00000035/uni00000029/uni00000026\n/uni00000035/uni00000031/uni00000026\n/uni00000025/uni00000044/uni00000046/uni0000004e/uni00000057/uni00000048/uni00000056/uni00000057\n/uni00000029/uni00000052/uni00000055/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000057/uni00000048/uni00000056/uni00000057\n/uni00000035/uni00000048/uni00000044/uni0000004f/uni00000003/uni0000003a/uni00000052/uni00000055/uni0000004f/uni00000047\nFig. 2: Profit and Loss (PNL) Trajectories of Top Classifiers in Real-World Trading Scenarios\naccurately but does so with economic prudence.\nD. Closing Evaluation\nThe detailed analysis of classifiers and regressors under-\nscores the multifaceted nature of financial prediction. The\nhighlighted models in the tables provide a benchmark for what\ncan be achieved with careful tuning and selection of rolling\nwindows. These results emphasize the necessity of a com-\nprehensive evaluation framework that incorporates a variety\nof performance metrics to assess model efficacy thoroughly.\nThe findings from the backtest and forward test phases offer\ninvaluable insights for developing resilient trading strategies", "tags": []}
{"fragment_id": "F_R16_p9_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p9:3", "text": "capable of adapting to the ever-evolving patterns of financial\nmarkets.\nE. Hyperparameter Optimization: Tuning for Peak Perfor-\nmance\nIn the quest for optimal model performance, hyperparameter\noptimization serves as the fine-tuning process that can make\nor break the predictive power of classifiers and regressors. The\nhyperparameter optimization for classifiers was meticulously\nperformed using advanced techniques that explored the depth\nand breadth of the parameter space, striking a balance between\nmodel complexity and generalization capability. The regressors\nunderwent a similar process, with each model’s unique param-\neters adjusted to navigate the intricate landscape of financial\ntime series forecasting. This iterative and methodical approach\nensured that the final model configurations were not just suited\nto historical patterns but were also robust and flexible enough\nto adapt to new, unseen market data.\nF . Analysis of Top Models on Real-World Data\nAn empirical evaluation of the top-performing classifiers\nwas conducted to assess their ability to generalize beyond\nbacktesting and forward testing scenarios. This analysis is\ncrucial to determine the models’ viability in live-market con-\nditions, where unpredictability and external factors play a\nsignificant role.\n1) Interpreting Classifier Performance in the Real World:", "tags": []}
{"fragment_id": "F_R16_p9_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p9:4", "text": "Figure 2 illustrates the Profit and Loss (PNL) trajectories of the\ntop classifiers over a timeline that spans backtesting, forward\ntesting, and into the real-world application phase. Each line\nrepresents the PNL progression of a model, providing insights\ninto their performance stability and adaptability to real market\nconditions.\nThe shaded areas—red for backtesting, green for forward\ntesting, and blue for the real-world phase—contextualize the\ntimeline of each model’s deployment. Across the transition\nfrom controlled testing environments to the real world, the\nfollowing observations are made:\n• Consistency of Performance: The models that maintain\na steady trajectory from backtesting through to real-world\ntrading, such as the Random Forest Classifier (RFC),\nindicate a strong ability to adapt to evolving market\nconditions without overfitting to historical data.\n• Adaptability to Market Shifts: Some models, like\nthe Multi-Layer Perceptron Classifier (MLPC), show re-", "tags": []}
{"fragment_id": "F_R16_p10_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p10:1", "text": "/uni00000029/uni00000048/uni00000045/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000030/uni00000044/uni00000055/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000024/uni00000053/uni00000055/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000030/uni00000044/uni0000005c/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni0000002d/uni00000058/uni00000051/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni0000002d/uni00000058/uni0000004f/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000024/uni00000058/uni0000004a/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000036/uni00000048/uni00000053/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000032/uni00000046/uni00000057/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000031/uni00000052/uni00000059/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni00000027/uni00000048/uni00000046/uni00000003/uni00000015/uni00000013/uni00000015/uni00000016/uni0000002d/uni00000044/uni00000051/uni00000003/uni00000015/uni00000013/uni00000015/uni00000017\n/uni00000015/uni00000013\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000033/uni00000031/uni0000002f/uni00000003/uni0000000b/uni00000008/uni0000000c", "tags": []}
{"fragment_id": "F_R16_p10_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p10:2", "text": "/uni0000002e/uni00000031/uni00000035\n/uni00000025/uni0000002a/uni00000035\n/uni00000028/uni00000037/uni00000035\n/uni00000036/uni0000002a/uni00000027/uni00000035\n/uni0000002f/uni00000044/uni00000035\n/uni0000002f/uni0000004c/uni00000035\n/uni00000025/uni00000044/uni00000046/uni0000004e/uni00000057/uni00000048/uni00000056/uni00000057\n/uni00000029/uni00000052/uni00000055/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000057/uni00000048/uni00000056/uni00000057\n/uni00000035/uni00000048/uni00000044/uni0000004f/uni00000003/uni0000003a/uni00000052/uni00000055/uni0000004f/uni00000047\nFig. 3: Real-World Profit and Loss (PNL) Performance of Top Regressors\nsilience in the face of market volatility, as evidenced by\ntheir PNL performance remaining robust or improving\nwhen transitioning to real-world trading.\n• Real-World Viability: The Bagging Classifier (BGC)\nand BernoulliNB Classifier (BNBC) demonstrate signif-\nicant real-world viability, highlighted by their sustained\nPNL levels in the live market phase. This suggests that\nthese models have captured fundamental market drivers\nthat are applicable in ongoing trading.\n• Economic Significance: The Ridge Classifier (RNC),\nwhile showing a dip in the forward test phase, recovers\nin the real-world application, pointing to economic strate-\ngies embedded within the model that may only become\nevident under actual market pressures.", "tags": []}
{"fragment_id": "F_R16_p10_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p10:3", "text": "• Volatility and Risk Management: The volatility in the\nPNL trajectories for some classifiers indicates the varying\nrisk profiles and the models’ sensitivity to market fluctua-\ntions. Effective risk management strategies are imperative\nfor these models to ensure that high volatility does not\nerode profitability.\nThe detailed visualization of PNL trajectories in Figure 2\nserves as a testament to the models’ capabilities and provides a\npredictive lens through which investors can gauge the potential\nsuccess of deploying these models in live trading scenarios.\nThe analysis confirms that while backtest and forwardtest\nperformances are indicative, the ultimate test for any trading\nmodel lies in its real-world application.\n2) Real-World Performance of Regressors: Figure 3\npresents the PNL performance of selected regressor models as\nthey transition from the controlled environments of backtesting\nand forward testing into actual market deployment. The PNL\ntrajectories provide a longitudinal view of each model’s ability\nto navigate and capitalize on real market trends.\nThe shaded regions represent different evaluation phases:\nbacktesting (red), forward testing (green), and the real-world\ntrading period (blue). The regressors’ performance trends\nacross these phases offer a multifaceted perspective on their\npredictive capabilities and economic utility:", "tags": []}
{"fragment_id": "F_R16_p10_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p10:4", "text": "• KNeighborsRegressor (KNR) displays a relatively sta-\nble PNL during backtesting, which declines during for-\nward testing but shows recovery in real-world conditions.\nThis pattern suggests a sensitivity to market conditions\nthat may require adaptive parameter adjustments or dy-\nnamic feature selection to maintain profitability.\n• BaggingRegressor (BGR) and ExtraTreesRegressor\n(ETR) both demonstrate high PNL in the backtest phase,\nwith the BGR maintaining this performance in the for-\nward test phase, indicating a robust model less prone\nto overfitting and capable of capturing persistent market\nsignals.\n• Stochastic Gradient Descent Regressor (SGDR) shows\na consistent increase in PNL across all phases, highlight-\ning its strength in adapting to new data. Its performance\nin the real-world phase, in particular, underscores the\npotential of SGD-based models for financial time series\nforecasting.\n• Lasso Regression (LaR) and Linear Regression (LiR)\nexhibit significant PNL volatility post-backtesting. The", "tags": []}
{"fragment_id": "F_R16_p11_1", "source_id": "R16", "locator": "2407.18334v1.pdf:p11:1", "text": "divergence in their PNL during the real-world phase\ncould reflect their varying degrees of regularization and\nfeature weighting, which impact their ability to handle\nnon-stationary market data.\n• Performance Fluctuations: The fluctuations and drops\nin PNL for some models from backtesting to real-world\napplication highlight the challenges of model generaliza-\ntion and the impact of market volatility. These variations\ncall for ongoing model recalibration and robust risk\nmanagement strategies to mitigate potential drawdowns.\nThe PNL trajectories in Figure 3 underscore the impor-\ntance of rigorous model evaluation. Models that demonstrate\nresilience and adaptability in forward testing are more likely\nto perform well in real-world trading, but the ultimate litmus\ntest for any trading strategy is its ability to sustain profitability\nin the live market. This graph illustrates not only the successes\nbut also the limitations of the tested regressors, guiding future\nmodel refinement and the development of adaptive trading\nsystems.\nV. C ONCLUSION\nThis study evaluated the performance of 41 machine learn-\ning models, comprising 21 classifiers and 20 regressors, for\nBitcoin price prediction in algorithmic trading. Through rig-\norous backtesting, forward testing, and real-world testing, we\nidentified that models like Random Forest and Stochastic Gra-", "tags": []}
{"fragment_id": "F_R16_p11_2", "source_id": "R16", "locator": "2407.18334v1.pdf:p11:2", "text": "dient Descent exhibit superior performance in terms of profit\nand risk management. The integration of both machine learn-\ning metrics (e.g., Mean Absolute Error, Root Mean Squared\nError) and trading metrics (e.g., Profit and Loss percentage,\nSharpe Ratio) provided a comprehensive assessment of model\nperformance.\nOur findings underscore the necessity for a multi-faceted\nevaluation approach to ensure the practical utility of trading\nmodels. Many models that performed well in backtesting\ndid not translate effectively to forward tests and real-world\nscenarios, highlighting the limitations of relying solely on\nbacktesting. By incorporating economic indicators and con-\nsidering practical trading constraints, our study offers a robust\nand practical solution for Bitcoin price prediction and trading.\nFuture research should extend these findings to other cryp-\ntocurrencies and investigate the impact of different economic\nindicators on model performance. Additionally, exploring\nemerging machine learning techniques can further enhance\npredictive accuracy and trading effectiveness. This study pro-\nvides valuable insights for traders and researchers aiming to\nleverage machine learning for more strategic and profitable\ncryptocurrency trading. Future work will also focus on refin-\ning our multi-faceted evaluation framework and exploring its", "tags": []}
{"fragment_id": "F_R16_p11_3", "source_id": "R16", "locator": "2407.18334v1.pdf:p11:3", "text": "application in different market conditions to further validate\nand improve the robustness of trading models.\nREFERENCES\n[1] F. Fang, C. Ventre, M. Basios, L. Kanthan, D. Martinez-Rego, F. Wu,\nand L. Li, “Cryptocurrency trading: a comprehensive survey,” Financial\nInnovation, vol. 8, no. 1, p. 13, 2022.\n[2] F. Dakalbab, M. A. Talib, Q. Nassir, and T. Ishak, “Artificial intelligence\ntechniques in financial trading: A systematic literature review,” Journal\nof King Saud University-Computer and Information Sciences, p. 102015,\n2024.\n[3] P. Nagamani, G. J. Anand, S. G. Prasanna, B. S. Raju, and M. S. Satish,\n“Bitcoin price prediction using machine learning algorithms,” in Second\nInternational Conference on Emerging Trends in Engineering (ICETE\n2023). Atlantis Press, 2023, pp. 389–396.\n[4] N. Maleki, A. Nikoubin, M. Rabbani, and Y . Zeinali, “Bitcoin price\nprediction based on other cryptocurrencies using machine learning and\ntime series analysis,” Scientia Iranica, 2020.\n[5] E. Akyildirim, O. Cepni, S. Corbet, and G. S. Uddin, “Forecasting mid-\nprice movement of bitcoin futures using machine learning,” Annals of\nOperations Research, pp. 1–32, 2021.\n[6] S. Ziweritin, “Height-end multi-layer perceptron and machine learning\nmethods of forecasting bitcoin price time series,” Authorea Preprints,\n2023.\n[7] R. Siva, B. Subrahmanian et al. , “Analyzing the machine learning", "tags": []}
{"fragment_id": "F_R16_p11_4", "source_id": "R16", "locator": "2407.18334v1.pdf:p11:4", "text": "methods to predict bitcoin pricing,”World Journal of Advanced Research\nand Reviews, vol. 21, no. 1, pp. 1288–1294, 2024.\n[8] L. Al Hawi, S. Sharqawi, Q. A. Al-Haija, and A. Qusef, “Empirical\nevaluation of machine learning performance in forecasting cryptocur-\nrencies,” Journal of Advances in Information Technology, vol. 14, no. 4,\n2023.\n[9] M. A. S. Yudono, A. D. W. M. Sidik, I. H. Kusumah, A. Suryana, A. P.\nJunfithrana, A. Nugraha, M. Artiyasa, E. Edwinanto, and Y . Imamulhak,\n“Bitcoin usd closing price (btc-usd) comparison using simple moving\naverage and radial basis function neural network methods,” FIDELITY:\nJurnal Teknik Elektro, vol. 4, no. 2, pp. 29–34, 2022.\n[10] H. Sebasti ˜ao and P. Godinho, “Forecasting and trading cryptocurrencies\nwith machine learning under changing market conditions,” Financial\nInnovation, vol. 7, pp. 1–30, 2021.\n[11] S. A. Gyamerah, “Are bitcoins price predictable? evidence from ma-\nchine learning techniques using technical indicators,” arXiv preprint\narXiv:1909.01268, 2019.\n[12] S. Yao, D. Ma, and Y . Zhang, “Prediction of bitcoin price movements\nbased on machine learning method and strategy construction.”\n[13] A. P. Kumar and S. Sunny, “Comparative analysis of machine learning\nmodels for predicting bitcoin price rate,” International Research Journal\nof Modernization in Engineering, Technology and Science , vol. 3, pp.\n234–238, 2021.", "tags": []}
{"fragment_id": "F_R16_p11_5", "source_id": "R16", "locator": "2407.18334v1.pdf:p11:5", "text": "[14] A. Falcon and T. Lyu, “Daily cryptocurrency returns forecasting and\ntrading via machine learning,” Journal of Student Research , vol. 10,\nno. 4, 2021.\n[15] F. Valencia, A. G ´omez-Espinosa, and B. Vald ´es-Aguirre, “Price move-\nment prediction of cryptocurrencies using sentiment analysis and ma-\nchine learning,” Entropy, vol. 21, no. 6, p. 589, 2019.\n[16] J. Parra-Moyano, D. Partida, and M. Gessl, “Your sentiment matters:\nA machine learning approach for predicting regime changes in the\ncryptocurrency market,” in The 56th Hawaii International Conference\non System Sciences. HICSS 2023 . Hawaii International Conference on\nSystem Sciences (HICSS), 2023, pp. 920–929.\n[17] M. Iqbal, M. Iqbal, F. Jaskani, K. Iqbal, and A. Hassan, “Time-series\nprediction of cryptocurrency market using machine learning techniques,”\nEAI Endorsed Transactions on Creative Technologies , vol. 8, no. 28,\n2021.\n[18] T. W. Septiarini, M. R. Taufik, M. Afif, and A. R. Masyrifah, “A\ncomparative study for bitcoin cryptocurrency forecasting in period 2017-\n2019,” in Journal of Physics: Conference Series , vol. 1511, no. 1. IOP\nPublishing, 2020, p. 012056.\n[19] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, “Optuna: A next-\ngeneration hyperparameter optimization framework,” in Proceedings\nof the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining , 2019.", "tags": []}
{"fragment_id": "F_R16_p11_6", "source_id": "R16", "locator": "2407.18334v1.pdf:p11:6", "text": "[20] W. F. Sharpe, “The sharpe ratio,” The Journal of Portfolio Management,\nvol. 21, no. 1, pp. 49–58, 1994.", "tags": []}
{"fragment_id": "F_R17_p1_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p1:1", "text": "Expert Systems With Applications 238 (2024) 121806\nAvailable online 28 September 2023\n0957-4174/© 2023 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\nContents lists available at ScienceDirect\nExpert Systems With Applications\njournal homepage: www.elsevier.com/locate/eswa\nA profitable trading algorithm for cryptocurrencies using a Neural Network\nmodel\nMimmo Parentea, Luca Rizzutia,∗, Mario Trerotolaa,b\na Dipartimento di Scienze Aziendali - Management and Innovation Systems, Università degli Studi di Salerno, Salerno, Italy\nb Dipartimento di Automatica e Informatica, Politecnico di Torino, Torino, Italy\nA R T I C L E I N F O\nKeywords:\nCryptocurrencies\nMachine learning\nNeural network\nPrice prediction\nAlgorithmic trading\nExplainable AI\nBacktesting\nShapley values\nA B S T R A C T\nAlgorithmic trading enables the execution of orders using a set of rules determined by a computer program.\nOrders are submitted based on an asset’s expected price in the future, an approach well suited for high-volatility\nmarkets, such as those trading in cryptocurrencies. The goal of this study is to find a reliable and profitable\nmodel to predict the future direction of a crypto asset’s price based on publicly available historical data. We", "tags": []}
{"fragment_id": "F_R17_p1_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p1:2", "text": "first develop a novel labeling scheme and map this problem into a Machine Learning classification problem.\nThe model is then validated on three major cryptocurrencies through an extensive backtest over a bull, bear\nand flat market. Finally, the contribution of each feature to the classification output is analyzed.\n1. Introduction\nA financial trading system on public market exchanges comprises\na set of rules and tools that help the trader agent make the best\ndecisions during the investment phase. These rules and tools are trading\nalgorithms applied to data relating to one or more financial assets, in\norder to identify and exploit profit opportunities. Multiple data sources\ncan be used to make trading decisions, and one of the most widely\npublicized projects using such techniques in financial applications is\nStandard & Poor’s Neural Fair Value 25portfolio. This uses an artificial\nneural network to select 25 stocks on a weekly basis from a total\nof 3000 stocks, with the aim of outperforming the market by calcu-\nlating a stock’s weekly fair value based on fundamental analysis. In\nthe field of securities trading, the utility of complex models such as\nNeural Networks (NN), Support Vector Machines (SVM) and hybrid\nmodels has been extensively studied and promising results have been\nobtained (Kumbure et al., 2022). However, information regarding the", "tags": []}
{"fragment_id": "F_R17_p1_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p1:3", "text": "incorporation of such methods into trading floor operations tends to re-\nmain hidden to the public, for commercial proprietary reasons (Gerlein\net al., 2016). Hence, there is an ample of room to explore automated\ntrading using modern machine learning (ML) approaches.\nIn this paper, we are interested in cryptocurrency trading. We aim\nto explore the possibility to learn generic price patterns (that is, not\nbound to specific assets) and evaluate them over a large period of time\non off-label data. In particular, we propose a viable trading strategy\nthat, once trained a NN on a large quantity of market historical data of\n∗ Corresponding author.\nE-mail addresses: parente@unisa.it (M. Parente), lrizzuti@unisa.it (L. Rizzuti), mario.trerotola@polito.it (M. Trerotola).\nhundreds of crypto assets, might automatically operate on the market to\nget positive profits. Our novel approach, differently from others, takes\ninto account a single rich dataset of many different cryptocurrencies\nwhich turns out to be, as we show later, very effective compared to\nother approaches which take into account historical data of one or few\nassets at a time.\nCryptocurrency-related assets have seen a significant increase in\nmarket acceptance and have developed rapidly in recent years. As a\nresult, many hedge funds and investors are beginning to include this", "tags": []}
{"fragment_id": "F_R17_p1_4", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p1:4", "text": "type of asset in their financial portfolios, which has had a significant\nimpact on the overall market and has generated considerable interest\nin trading algorithms for cryptocurrencies. According to Fang et al.\n(2022), the field of cryptocurrency trading research has experienced\na significant surge in interest and activity in recent years. Specifically,\nthe authors note that a staggering 85% of all the published scientific\npapers on algorithmic trading of cryptocurrency-related assets have\nbeen published just in the last five years. This suggests that the field\nof cryptocurrency trading research is rapidly evolving and that there\nis a growing interest in developing new strategies and approaches for\ntrading cryptocurrencies.\nWhile strategies applied to traditional financial markets can be\nadapted to the cryptocurrency market, they have certain unique char-\nacteristics that require new research efforts from the scientific commu-\nnity. Standard trading approaches are based on fundamental and techni-\ncal analysis. The fundamental approach aims to determine whether an\nhttps://doi.org/10.1016/j.eswa.2023.121806\nReceived 26 May 2023; Received in revised form 22 September 2023; Accepted 22 September 2023", "tags": []}
{"fragment_id": "F_R17_p2_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p2:1", "text": "Expert Systems With Applications 238 (2024) 121806\n2\nM. Parente et al.\nasset is currently traded at its fair market value using financial metrics\nand by conducting thorough research of the asset’s business-related\ninformation. Technical analysis, on the other hand, analyzes price and\nvolume series in order to predict an asset’s future value, although this\nis founded on the assumption that future market prices can, in fact, be\npredicted.\nAccording to the Efficient Market Hypothesis, Malkiel (2003), mar-\nket agents are rational and new information is immediately reflected in\nthe price, whereas the Adaptive Market Hypothesis (Chu et al., 2019)\nsuggests that investors may behave irrationally in response to market\nvolatility, thereby creating buying opportunities. This can be ascribed\nto human behavior, such as loss aversion, overconfidence and overreac-\ntion, which can increase market volatility in certain circumstances. The\ninefficiency of the major cryptocurrencies is investigated by Zhang et al.\n(2018) concluding that ‘‘results indicate that all these cryptocurrencies\nare inefficient markets’’. This makes the cryptocurrency market a fertile\nscenario for applying technical analysis using automated agents.\nIn fact, using algorithms for the trading might weaken these human\nbehaviors. Another peculiarity of this scenario is that digital exchanges,", "tags": []}
{"fragment_id": "F_R17_p2_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p2:2", "text": "and especially cryptocurrency exchanges, have lower fees than tradi-\ntional brokers, which can make a significant difference in the high\nvolume and frequencies of trades typical in this market. At present, a\ntransaction fee on cryptocurrency exchanges can be as low as 0.1% per\ntrade (at the time of writing, such fees are reduced to 0% on specific\ncurrency pairs). Additionally, most exchanges provide free trading API,\nthus further reducing barriers to entry for algorithmic trading.\n1.1. Our contribution\nWe have developed a trading algorithm based on a Multi-Layer Per-\nceptron (MLP) as a classifier with three classes, Buy, Holdand Sell.\nWe designed a complete usual analysis pipeline: first we gathered\nprice and volume time-series from a popular cryptocurrency exchange,\nthen preprocessed them by feature extraction and labeling, and finally\ntrained and tested the MLP. One of the peculiarities of our work is\nthe dataset labeling algorithm, which is based on two thresholds to\nintercept significant market movements and two temporal windows,\none in the past and one in the future for the forecasting of price trends.\nA true trading strategy must be profitable in every phase of the\nmarket cycle: both in the bull market characterizing the past 10 years,\nin the bear/highly volatile market such as the one at the time of\nwriting and in the flat one, like the period 2019–2021. Our findings", "tags": []}
{"fragment_id": "F_R17_p2_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p2:3", "text": "show that a perfectly validated model with good performance standard\nmetrics might perform poorly when used in a real simulation of a highly\nvolatile market. To further validate our model, we set a simulation on\na real scenario based on long term off-label historical data for different\ncurrencies (backtest phase).\nWe built a massive dataset of hundreds of cryptocurrencies spanning\nseveral years at a 4-h time resolution. The strategy was then evaluated\nover two different time intervals in order to assess its behavior on\na long-term and a short-term real case scenario, which yielded very\npositive results in terms of Return On Investment (ROI). We then\nconcluded the study with an analysis of the feature importance.\nThe Python source code developed for this research, datasets and\noutput of the analysis can be found at Parente et al. (2023).\nThis paper is organized as follows: in Section 2 we review the\nrelevant literature, analyzing it according to the different types of data\nsources used to predict cryptocurrency market trends. In Section 3\nwe show the preprocessing and feature extraction pipeline and the\nalgorithm for data labeling. In Section 4 we describe the design of the\nMLP neural networks and rank these models according to the accuracy\nachieved in the testing phase. In Section 5 we report the results of the", "tags": []}
{"fragment_id": "F_R17_p2_4", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p2:4", "text": "simulation. In Section 6 we describe the feature importance analysis\nconducted. Finally in Section 7 we give some conclusions and hints on\nfuture directions to be explored.\n2. Related works\nForecasting the asset price or its direction calls for significant work\non data integration and feature extraction from raw market data and/or\nother sources.\nIn Kraaijeveld and De Smedt (2020), Sattarov et al. (2020) and\nValencia et al. (2019) sentiment analysis is employed on Twitter data\nfeeds to predict the prices of major cryptocurrencies using standard\nML models (Random Forest Regressor, SVM, MLP). Kim et al. (2016)\nuses messages from cryptocurrency-related web forums as primary\ndata sources employing a probabilistic model based on Averaged One-\nDependence Estimators (AODE).\nIn Guo et al. (2021) data are taken from blockchain, coin exchanges\nand Google Trends to predict Bitcoin price. The authors derived new\nfeatures from the size of transactions and employed a proprietary\nmodel based on wavelet decomposition for preprocessing data and a\nCausal Multi-Head Attention Temporal Convolutional Network (WT-\nCATCN) for the modeling layer. Similarly, Li and Du (2023) encodes\ntransactions found in the Bitcoin blockchain into a graph, searches\nfor recurrent patterns, and correlates them with price changes using\nstandard ML models (MLP, SVM). The authors in Saad et al. (2020) find", "tags": []}
{"fragment_id": "F_R17_p2_5", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p2:5", "text": "that the Ethereum price is strongly positively correlated with various\nuser activities on its blockchain, but they also identify an inverse\ncorrelation with the crude oil trend, probably due to the rise in energy\ncosts.\nSome research methods borrowed from traditional market method-\nologies seek correlations between cryptocurrencies and macroeconomic\nand/or financial indicators. Walther et al. (2019) evaluates macro\nand financial indexes to model the volatility of prices for five major\ncryptocurrencies by utilizing a GARCH-MIDAS framework (Engle et al.,\n2013). Similarly, Parvini et al. (2022) predicts the daily Bitcoin price by\nusing data from other commodities and indexes, transforming them into\nthe time–frequency domain using wavelet decomposition, and feeding\nthe data into a Long Short-Term Memory (LSTM) network. Kim et al.\n(2021) also uses macroeconomic indexes and blockchain information\nto forecast the Ethereum price.\nIn addition to external data sources, some papers exploit the in-\nformation in the order book. The order book contains the limit orders\non both Buy and Sell sides, registered by the market participants. The\norders placed by big market players, are often used as a source of\ninformation to predict the near-term direction of the asset’s prices. Alec\nand Kercheval (2015), Tsantekidis et al. (2017) use Convolutional NN", "tags": []}
{"fragment_id": "F_R17_p2_6", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p2:6", "text": "and SVM models respectively. Similarly, Guo et al. (2018) uses order\nbook data to complement the price and volume information in order to\nextract features and predict prices.\nSeasonality is another area of research borrowed from standard\nassets whose effects are evaluated on cryptocurrencies, see Baur et al.\n(2019) and Kaiser (2019).\nMany papers rely on prices and volumes time series, basing the\nfeature extraction on technical analysis or using techniques for time\nseries forecasting/regression. In Lahmiri and Bekiros (2019) the daily\nprices of the 3 topmost cryptocurrencies are used to forecast the next\nday’s prices with LSTM and Generalized Regression Neural Network.\nSimilarly, Alonso-Monsalve et al. (2020) uses different neural network\narchitectures to assess the predictability of trend direction in a minute\ntime frame and finds that the hybrid LSTM with the Convolutional layer\nbetween the input and the Neural Network yields the best results.\nThe labeling of datasets for supervised learning is an open problem\nthat is, broadly speaking, tackled using variations of two main ideas:\nprice regression and price direction forecasting. In the former the\ndifferences lie in the size of the time frame used to assign a label\n(minutes, hours, days, etc.) and in the latter the difference is whether 2\nor 3 labels are used, forBuy/Sell or Buy/Hold/Sell. In this paper", "tags": []}
{"fragment_id": "F_R17_p2_7", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p2:7", "text": "we use three labels for the classification (as Kraaijeveld & De Smedt,\n2020 and Tsantekidis et al., 2017).", "tags": []}
{"fragment_id": "F_R17_p3_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p3:1", "text": "Expert Systems With Applications 238 (2024) 121806\n3\nM. Parente et al.\nFig. 1. Cryptocurrency samples distribution over time.\n3. The dataset\nIn this section we describe the dataset used to apply our supervised\nML algorithm. It consists of raw data gathered from the price and\nvolume data time series of a popular cryptocurrency exchange. Then we\nshow how the features were extracted from the raw data by computing\ncandlestick patterns and financial indicators, and by moving average\ncrossovers and temporal data. Finally, we design an algorithm to label\nthe observations, parameterized in such a way that it can be used to\nmaximize classification accuracy and the profitability of a given trading\nstrategy.\n3.1. Data collection\nData were downloaded from a popular crypto exchange platform\nexposing web based APIs and were collected in the usual OHLC +\nvolume format, which encodes price variation in a given time frame in\nterms of High, Open, Low, Close pricesand Volume exchanged during the\ngiven time frame. As usualOHLC is represented by ‘‘candlesticks’’ in the\nasset’s price charts. As said in Section 1, we collected data in a single\ndataset consisting of 402 different crypto assets and set the time frame\nlength at 4 h. The assets were all those available on the platform from\nAugust 17, 2017 to December 4, 2022. For those assets not available at", "tags": []}
{"fragment_id": "F_R17_p3_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p3:2", "text": "the starting date, data were collected as soon as they were listed on the\nexchange. All the cryptocurrencies are those paired with the stable coin\nUnited States Dollar Tether (USDT). After the feature extraction phase,\nwe obtained a dataset of 1.5 million samples with the distribution\nshown in Fig. 1.\n3.2. Feature extraction\nExtracting features from raw data is a delicate step in the standard\nML pipeline process since it pertains to the field of making information\nof a phenomenon (social, physical, etc...) easily accessible to an ML\nmodel. An established method to summarize the trading market behav-\niors are the technical indicators. These, by using past prices, compute\na quantitative estimation for the future price direction. The input of\nthose indicators are past prices and volumes and they often use plain\nor exponential moving averages of the past prices and/or volumes (e.g\nMACD, CCI, ADX, . . . ). The rationale behind our choice of technical\nindicators is to use simpler indicators (MA crossovers, Z-Scores) often\nused in computation of more complex indicators (MACD, CCI, ADX,\n. . . ), and few other commonly used indicators, to let the NN discover\nuseful patterns. We complemented these indicators with some temporal\ninformation and with the standard candlesticks patterns.\nCandlestick patterns. Candlestick charts are typically used to visualize", "tags": []}
{"fragment_id": "F_R17_p3_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p3:3", "text": "price fluctuations. A series of candlesticks may form a pattern and\nconstitute a technical analysis tool that can suggest future price move-\nments on the basis of past price behavior. By definition, the patterns are\nindependent of the size of the time frame (Murphy, 1999). Our study\nconsiders 23 of the most popular candlestick patterns, both single and\nmultiple candles, in their bullish and bearish version, e.g. Three Black\nCrows, Doji, Engulfing, Hammer. In this paper we use all the patterns\nmentioned in Pring (1991).\nTechnical indicators, known as oscillators, can assume values in a zero-\ncentered interval and can be used as-is as features in ML applications.\nWe use 6 of the most common technical indicators: Bollinger bands,\nULTOSC, RSI, Close price percentage variation, Z-Score and volume\nZ-Score. Moreover, we also consider the Exponential Moving Average\ncrossover so as to take into account the trends and trend reversals.\nSee Kardile et al. (2021) and Murphy (1999) for more on technical\nindicators for financial markets.\n• Bollinger bands. These are used to check whether prices are high\nor low on a relative basis. Given n periods, the price calculated\nusing the n-period moving average is used as a reference price.\nTwo lines are plotted above and below one standard deviation\naway from the reference price. The standard deviation of the price", "tags": []}
{"fragment_id": "F_R17_p3_4", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p3:4", "text": "in the previous n-periods is used as a measure of volatility.\n• RSI. The Relative Strength Index is a momentum indicator that\nmeasures the magnitude of recent price changes in order to\nevaluate overbought and oversold conditions. A lower RSI value\nindicates that the asset is oversold, while a higher value means\nthat the asset is overbought.\n• ULTOSC. The Ultimate Oscillator uses the values of three differ-\nent moving averages with multiple time periods (or cycles), to\nidentify overbought and oversold conditions in the market, thus\nimproving the accuracy of the signals generated by the indicator.\n• Close price percentage variation. This measures the percentage\ndifference between the current price and the previous close price.\n• Z-Score. This uses the z-score of the close price in a given number\nof time frames. In our implementation we used 30 close price past\nsamples to compute the Z-Score of the actual time frame.\n• Volume Z-score: In order to convert transaction volumes for\ncomparing those of different cryptocurrencies, we used z-score\nnormalization.\n• EMA crossovers. The Exponential Moving Average crossovers are\nan established source of information for trend following and\ninversion. We considered 4 EMA crossovers based on 1 and 20,\n20 and 50, 50 and 100, and 1 and 50 periods.\nTemporal information. In Section 2 we noted that temporal information", "tags": []}
{"fragment_id": "F_R17_p3_5", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p3:5", "text": "adds a statistical edge to price direction prediction. Thus, we have also\nadded three other features based on time. Every single OHLC sample\nhas an associated timestamp, used to extract the month of the year,\nthe day of the week and the number of samples in the day (six a day).\nSummarizing, the overall feature vector has 36 entries: 23 candlestick\npatterns, 6 financial indicators, 4 EMA crossovers, and 3 temporal\nfeatures.\n3.3. Labeling algorithm\nWe propose the utilization of three distinct labels for classification\npurposes, analogously as in Kraaijeveld and De Smedt (2020) and\nTsantekidis et al. (2017). The choice of using three labels aims to\nenhance the accuracy of the classification process, with respect to the\nuse of only two, by addressing potential ambiguities that may arise\nwhen determining whether to open or close a position. By considering\nalso the Hold label, we try to avoid confusion that may arise when\ndeciding whether to refrain from opening a Buy position initially or\nto avoid triggering subsequent Buy orders after the same position has\nbeen opened. Furthermore, the use of the Hold label is a critical", "tags": []}
{"fragment_id": "F_R17_p4_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p4:1", "text": "Expert Systems With Applications 238 (2024) 121806\n4\nM. Parente et al.\nFig. 2. Visual representation of label boundary based on the 𝛼 and 𝛽 parameters.\nstep towards aligning our classification algorithm to real-world trading\npractices.\nThe analysis we conducted uses a supervised ML model, thus each\nsample of the dataset has to be labeled for the model’s training phase.\nFirst let us define the temporal windows and the return for a single\ntrade operation.\nDefinition 1. Given a time frame at time 𝑡and a cryptocurrency 𝑐, a\nForward Window of size 𝑘,  𝑡,𝑐(𝑘), is the sequence of time frames\n𝑡,𝑡 + 1,… ,𝑡 + 𝑘− 1and a Backward Windowof size 𝑘, 𝑡,𝑐(𝑘), is the\nsequence of time frames 𝑡− 𝑘+ 1,𝑡 − 𝑘+ 2,… ,𝑡.\nIn the following we drop the subscripts 𝑡and 𝑐 when the context is\nclear. Opening a position in the market at the open price of the time\nframe at time 𝑡, 𝑂𝑝𝑒𝑛𝑡, and closing the position at the close price of the\ntime frame 𝑡+ 𝑘, 𝐶𝑙𝑜𝑠𝑒𝑡+𝑘, yields a revenue or a loss, formally defined\nas follows.\nDefinition 2. The return of the trade of a cryptocurrency 𝑐, opened\nat time 𝑡 and closed at the time frame 𝑡+ 𝑘, is given by:\n𝑡,𝑐(𝑘) = (1 −𝑓) ⋅ 𝐶𝑙𝑜𝑠𝑒𝑡+𝑘 − (1 +𝑓) ⋅ 𝑂𝑝𝑒𝑛𝑡\n𝑂𝑝𝑒𝑛𝑡\nwhere 𝑓 is the fee applied by the exchange for each trade operation. 1\nHere too we drop the subscripts 𝑡and 𝑐and retain only the duration\n𝑘 of the opened position, when clear from the context.", "tags": []}
{"fragment_id": "F_R17_p4_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p4:2", "text": "The labeling algorithm, depicted in Fig. 3, has two parameters, 𝛼\nand 𝛽, which are used to set the thresholds of the return values (𝑘),\nwhere 𝑘 is the size of the Forward window. The former is used to\nestablish a low value below which it is not convenient to place a trading\norder and the latter is a high value above which we consider a price\nvariation not influenced by the technical framework but by exogenous\nforces. In this extreme scenario we likewise do not place orders. In\nsummary, in order to trigger a Buy, Sell or Hold signal, given\n𝑂𝑝𝑒𝑛𝑡 we want to predict the price variation at the end of the Forward\nwindow 𝐶𝑙𝑜𝑠𝑒𝑡+𝑘. Fig. 2 depicts the behavior of the labeling algorithm\nas a function of the 𝛼 and 𝛽 parameters.\nThe algorithm in Fig. 3 initiates with five input parameters: the\narray of closing prices 𝑐𝑙𝑜𝑠𝑒𝑃𝑠, the Backward and Forward window\nsizes ( 𝑏𝑎𝑐𝑘𝑊 and 𝑓𝑜𝑟𝑊 respectively), and the two threshold values\n𝛼 and 𝛽.\nThe first step of the algorithm is to update the close prices with their\nExponential Moving Average (𝐸𝑀𝐴), utilizing the 𝑏𝑎𝑐𝑘𝑊 window size.\nThe algorithm then enters a loop on each such close price and\ncomputes its return. Next, the algorithm determines if the absolute\nvalue of the return falls within the range ]𝛼,𝛽[. If this condition is met,\nthe algorithm then distinguishes between positive and negative returns.", "tags": []}
{"fragment_id": "F_R17_p4_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p4:3", "text": "A positive return labels the time frame as Buy, while a negative return\nlabels the time frame Sell.\nOn the other hand, if the condition is not met, the algorithm\nproceeds to the ‘‘Set label to Hold’’ block. This situation arises when\nthe return value is either too small (less than 𝛼) or too large (greater\nthan 𝛽), indicating that it is advisable to avoid making any transactions\nat that time.\n1 The fee is given here as a percentage of the investment and, for\nsimplicity’s sake, is assumed to be equal for both a buy and a sell operation.\nFig. 4 depicts a price chart. The rectangles represent the windows\naround a price sample, marked with an arrow. The Backward window\nis of size 5 and the Forward window is of size 2. The 5EMA of the close\nprices in the backward window is the starting point of a trend, shown\nas a dotted line, giving the price direction to compute the label of the\ncurrent price sample.\n4. MLP models, training and testing\nIn this section we introduce the classification model and the method-\nology applied for the training and the testing phase. We adopted the\nMLP model with four layers as a classifier, trained on the dataset\nlabeled with the labeling algorithm reported in Fig. 3.\n4.1. Multi-layer perceptron\nThe MLP consists of an input layer, two hidden densely connected\nlayers and a categorical output layer with three nodes. When defining", "tags": []}
{"fragment_id": "F_R17_p4_4", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p4:4", "text": "the MLP architecture in terms of the number of layers and neurons, we\nfollowed two guidelines: the universal approximation theorem (Hornik\net al., 1989) and the principle that an MLP should have the least num-\nber of neurons to generalize well (Hunter et al., 2012). The universal\napproximation theorem ensures that a feedforward neural network with\none input layer, one hidden layer and one output layer can approximate\nany function of the input with any desired degree of accuracy, provided\nthat a sufficient number of hidden units are available.\nUsing a bottom-up approach, see e.g. Setiono (2001), we began with\na small number of neurons on each hidden layer and compared the\naccuracy on the train and test set. On undersized networks, these two\nnumbers are comparable up to the second digit after the decimal point.\nAs we added neurons to each hidden layer, the network began to overfit\non the training set. We selected the minimum number of neurons that\nexhibited slight overfitting and provided the highest accuracy on the\ntest set. This approach allowed us to avoid managing overfitting with\ndropout layers or l1/l2 penalty terms on the weights of the network.\nSpecifically, the input layer has 128 nodes, the two hidden layers\nhave 64 and 32 nodes, respectively, all with the LeakyReLU activation\nfunction. Finally, the output layer has three nodes with the Softmax", "tags": []}
{"fragment_id": "F_R17_p4_5", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p4:5", "text": "activation function, which ensures a probabilistic output for the three\nclasses.\n4.2. Window sizes,𝛼 and 𝛽 parameters\nForward and Backward windows, and 𝛼 and 𝛽, described in Sec-\ntion 3.3, are the parameters and the thresholds used to label the dataset.\nHere we describe how these values are computed. The thresholds 𝛼\nand 𝛽 emerge from a statistical analysis of the open-close percentage\nchange in prices for the whole dataset. The value of 𝛼 is set to the 85-\nth percentile and the value of 𝛽 is set to the 99.7-th percentile. The\nabsolute values of the thresholds are thus 𝛼 = 0.038 and 𝛽 = 0.24. The\n𝛽 marking the outlier boundary, see Fig. 2, is incremented by 10% each\ntime the size of the Forward window is increased. Thus, for windows\nof size 2, 𝛽 = 0.24 + 0.024, for size 3, 𝛽 = 0.24 + 2 ∗ 0.024 and so on.\nThis choice of 𝛼 leads to an unbalanced dataset towards the Hold\nclass, which represented approximately the 70%, resulting in poor clas-\nsification results. To overcome this, we used a random undersampling\non the majority class to balance the dataset, see e.g. Buda et al. (2018).", "tags": []}
{"fragment_id": "F_R17_p5_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p5:1", "text": "Expert Systems With Applications 238 (2024) 121806\n5\nM. Parente et al.\nFig. 3. Labeling algorithm flowchart.\nFig. 4. A price chart with backward and forward windows of sizes 5 and 2,\nrespectively, with a positive price variation.\nFinally, to obtain the best combination of Forward and Backward\nwindow sizes, a grid search was conducted using the sizes from 1 to 5,\nhence considering 25 labeling schemes. The training and testing dataset\nwas built with data on currencies from August 17, 2017 to December\n31, 2021, and from April 1, 2022 to December 4, 2022 with the 25\nTable 1\nTop 5 models ranked by accuracy (third column) and the corresponding combination\nof Backward and Forward window sizes (first and second column).\n𝐵𝑎𝑐𝑘𝑊 𝐹𝑜𝑟𝑊 Acc Buy Hold Sell Samples\nPre Rec Pre Rec Pre Rec\n5 1 0.72 0.71 0.75 0.67 0.59 0.76 0.81 463k\n4 1 0.70 0.69 0.72 0.65 0.60 0.74 0.77 422k\n3 1 0.67 0.65 0.67 0.65 0.58 0.69 0.74 379k\n5 2 0.66 0.66 0.66 0.61 0.57 0.70 0.75 610k\n4 2 0.64 0.64 0.64 0.61 0.55 0.68 0.73 582k\ndifferent labeling schemes (the first three months of the year 2022\nwere later used to select the best model by profitability as described\nin Section 5).\nFor each combination of Forward and Backward window sizes, we\ntrained and tested the model three times, with a proportion of 70%–\n30%, using different random seeds and averaging the results. We then", "tags": []}
{"fragment_id": "F_R17_p5_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p5:2", "text": "selected the top five accuracy value combinations ofForward and Back-\nward windows as reported in Table 1 . The different window lengths\nmay lead to considerable variations in the size of the samples reported\nin the last column (due to the rebalancing of the majority class).\n4.3. Comparisons with other models\nOur choice to use MLP model has derived from a thoroughly ex-\nperimentation of our dataset on others classifiers: XGBoost, Logit and\nSGDLinear. Here follows the analyses we have done on all these models.", "tags": []}
{"fragment_id": "F_R17_p6_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p6:1", "text": "Expert Systems With Applications 238 (2024) 121806\n6\nM. Parente et al.\nTable 2\nModel comparison for the combination of forW and backW whose accuracy is\nmaximized: forW = 5 and backW = 1.\nModel Accuracy Buy Hold Sell Samples\nPrec Rec Prec Rec Prec Rec\nMLP 0.72 0.71 0.75 0.67 0.59 0.76 0.81 463k\nXGB 0.70 0.71 0.73 0.65 0.58 0.74 0.78 463k\nLogit 0.64 0.67 0.72 0.52 0.41 0.69 0.77 463k\nSGDLinear 0.59 0.58 0.84 0.60 0.03 0.60 0.87 463k\nXGBoost. XGBoost is a supervised classification algorithm that employs\nan ensemble technique to construct a model. By combining predictions\nfrom multiple models, it aims to attain superior predictive performance.\nThis model supports multi-class problems and exhibits high ability\nin solving non-linear problems. In our experimental setup, we utilize\nXGBoost, a variant of the Random Forest algorithm that incorporates\na gradient boosting mechanism. During the learning process, XGBoost\ntrains a random forest, but instead of aggregating trees, it utilizes\ngradient boosted trees that learn from errors at each boosting round.\nIn the construction of each tree, a loss function with a regularization\nterm, is optimized to maximize classification accuracy.\nLogit. Logit is a binary classifier that learns a vector of weights[𝑤1,… ,\n𝑤𝑛] and a bias term 𝑏 and uses a logistic binary function to map the", "tags": []}
{"fragment_id": "F_R17_p6_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p6:2", "text": "linear combination of weight vector and the new observations into a\n0,1 classes.\nSGDLinear. The SGDLinear binary classifier is a ML algorithm that\nlearns a vector of weights by using the Stochastic Gradient Descent\n(SGD) technique, updating model parameters incrementally with in-\ndividual data samples. It can handle large datasets efficiently and is\nsuitable for online learning, making it a popular choice for various clas-\nsification tasks. (Let us note that in our experiments, it outperforms the\nSVM classifier by many order of magnitude in terms of computational\ntime.)\nTo compare the binary classifiers Logit and SGDLinear with our\nmodel which adopts three classes, we used One-vs-Rest strategy. A\nnumber 𝑁 of binary classifiers were trained, one for each class vs\nthe other two remaining classes. To predict a new observation, the 𝑁\nclassifiers are queried and the values constitute the membership of the\nobservation to the specific class, then 𝑎𝑟𝑔𝑚𝑎𝑥(𝑐𝑙𝑎𝑠𝑠1,𝑐𝑙𝑎𝑠𝑠2,… ,𝑐𝑙𝑎𝑠𝑠𝑁)\nis chosen as the predicted class.\nWe implemented the above models using a hyperparameters grid\nsearching and have reported their performance in Table 2. The MLP\nand XGBoost performance overcome all linear models, with the former\nslightly better. MLP and XGBoost clearly excel at modeling highly non\nlinear phenomena, differences in the performance metrics are small", "tags": []}
{"fragment_id": "F_R17_p6_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p6:3", "text": "but constant during many runs with different seeds. MLP is clearly the\nbest of the group, this has led to our choice. The linear models exhibit\nbehaviors in line with other papers (Akyildirim et al., 2021; Jaquart\net al., 2021; Ozer & Okan Sakar, 2022) in terms of accuracy, but the\nperformances are still below those of MLP and XGB. It is worth noting\nthat the recall of theHold class for the SGDLinear model is surprisingly\nquite low.\n5. Backtesting\nIn Section 4 we described the design and the training of an MLP and\nselected the top five combinations, in order of accuracy, of Forward\nand Backward window sizes used to label the dataset (see Table 1).\nThen we computed the profit on a simple trading strategy using the\nsame data on these 5 models and we obtained a different ranking of\nthe combination of window sizes. Hence, profitability is an evident and\nirrefutable metric when assessing algorithmic trading strategies (see\ne.g. Olorunnimbe & Viktor, 2023) and we were, therefore, induced to\nfurther investigate this situation. Backtesting is a standard method for\nTable 3\nPerformance and metrics for the model selected for the backtest.\nAcc Buy Hold Sell Samples\nPre Rec F1 Pre Rec F1 Pre Rec F1\nTest 0.66 0.65 0.68 0.67 0.65 0.52 0.57 0.70 0.78 0.73 201k\nTrain 0.68 0.66 0.70 0.68 0.66 0.53 0.58 0.71 0.80 0.75 469k\nDummy 0.33 0.32 0.32 0.32 0.32 0.32 0.32 0.36 0.36 0.36 201k\nTable 4", "tags": []}
{"fragment_id": "F_R17_p6_4", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p6:4", "text": "MLP and dummy model backtesting generated ROI by different stop-losses. Models\ntrained on labels generated with 𝑏𝑎𝑐𝑘𝑊 = 5 and 𝑓𝑜𝑟𝑊 = 2.\ncoin ROIs Stop loss\nMLP Long MLP Short Dum Long Dum Short\nALGOUSDT 0,22 0,89 0,27 0,80 0,00\nBTCUSDT 0,23 0,84 0,06 0,69 0,00\nETHUSDT 0,08 0,76 0,08 0,69 0,00\nALGOUSDT 0,29 1,41 0,02 0,67 0,01\nBTCUSDT 2,33 0,94 0,09 0,67 0,01\nETHUSDT 2,40 1,68 0,12 0,78 0,01\nALGOUSDT 0,65 1,44 0,03 0,57 0,025\nBTCUSDT 17,76 0,82 0,15 0,67 0,025\nETHUSDT 24,18 1,76 0,21 0,69 0,025\nALGOUSDT 11,89 1,85 0,01 0,58 0,05\nBTCUSDT 53,25 1,05 0,23 0,56 0,05\nETHUSDT 82,58 1,75 0,09 0,59 0,05\nALGOUSDT 36,18 2,08 0,01 0,47 0,10\nBTCUSDT 61,22 1,18 0,14 0,50 0,10\nETHUSDT 165,91 2,30 0,08 0,51 0,10\nevaluating ex-post the performance of an algorithmic trading strategy\nwhich uses profitability as a measure of the goodness of the model\nadopted. To simulate a real trading scenario, it uses historical data\nand does not open multiple long or short positions on the same asset\nsimultaneously. At the end of the simulation period, all positions are\nclosed and the profit is computed.\nThe model to be used in backtesting was selected by means of\na specially designed simulation algorithm implementing a standard\nstrategy (see e.g. Pring, 1991): the algorithm scans the list of OHLC\nprices searching for the first Buy, which is the signal to enter the", "tags": []}
{"fragment_id": "F_R17_p6_5", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p6:5", "text": "market, whereupon it places an order. The algorithm then looks for\nthe next Sell label and the position is closed. The earning/loss is\ncapitalized and the strategy restarts, thus obtaining compound interest.\nIn our accounts we included the commission fees of 0.1% applied by\nsome market brokers at the time of writing. The strategy used the data\nfrom the first three months of 2022, (not used in the training/testing\nphases, see Section 4.2). The period was chosen since the Bitcoin and\nEthereum assets were following a downward trend (final price < initial\nprice). The model with the windows sizes (5,2), the fourth in Table 1,\nobtained the greatest return, averaged over all the assets, and it was\nthus selected for the final backtesting. This model was then trained\non the data from August 17, 2017 to December 4, 2022 for all the\ncurrencies apart from the data relative toBitcoin, Ethereum and Algorand\nwhich were later used for backtesting.\nThe result of the test is reported in Table 3. The performance metrics\nare comparable to those in Table 1, even though the data for Bitcoin,\nEthereum and Algorand are missing. We interpret this as an indicator\nof good model generalizationon generic technical patterns not bound to\nspecific coins.\nThe size of the dataset is different because of the differences in the\ncomposition of the datasets: those in Table 1 had all the coins except", "tags": []}
{"fragment_id": "F_R17_p6_6", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p6:6", "text": "for 3 months of data for everyone, in Table 3 there are all coins except\nthose 3 selected for the backtest.\nOnce the model had been selected, trained and tested, we extracted\nthe features to finalize the backtesting for the above three currencies\nover 5 years and 4 months for Bitcoin and Ethereum and 3 years and\n6 months for Algorand, and then queried the model. Trading strategies", "tags": []}
{"fragment_id": "F_R17_p7_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p7:1", "text": "Expert Systems With Applications 238 (2024) 121806\n7\nM. Parente et al.\nFig. 5. ETHUSDT backtest.\ncan be described unambiguously using regular expressions on an al-\nphabet composed of three symbols: Hold, Buy, Sell. In this way the\ntrading strategy described above can be written as:\n(𝙱𝚞𝚢[𝙱𝚞𝚢|𝙷𝚘𝚕𝚍]∗𝚂𝚎𝚕𝚕)+ (1)\nwhere ∗ and + are the transitive closures, see Hopcroft et al. (2006) for\nmore on regular expressions.\nTo assess the quality of the equity curves, we compared our model\nwith a baseline dummy classifier that randomly responds with the\nsame distribution of labels in the unbalanced original dataset, where\nthe Hold, Buy, Sell labels are approximately in the 70, 15, 15\nproportion, respectively. The overall return 𝑅 for 𝑛 trades on the coin\n𝑐, is then computed as follows:\n𝑅=\n𝑛∏\n𝑖=1\n(1 +𝑖\n𝑐(2)) (2)\nwhere: 𝑛 is the number of trades executed and 𝑖\n𝑐(2) is the return of a\nsingle trade computed according to Definition 2 on a Forward window\nof size 2.\nTable 4 reports the ROIs of all the periods for every single backtest,\nboth for the MLP and the Dummy models, for 5 different stop-loss\nthresholds, computed as:\n𝑅𝑂𝐼 = 𝐹𝑖𝑛𝑎𝑙𝑉𝑎𝑙𝑢𝑒𝑜𝑓𝐼𝑛𝑣𝑒𝑠𝑡𝑚𝑒𝑛𝑡 − 𝐼𝑛𝑖𝑡𝑎𝑙𝑉𝑎𝑙𝑢𝑒𝑜𝑓𝐼𝑛𝑣𝑒𝑠𝑡𝑚𝑒𝑛𝑡\n𝐼𝑛𝑖𝑡𝑎𝑙𝑉𝑎𝑙𝑢𝑒𝑜𝑓𝐼𝑛𝑣𝑒𝑠𝑡𝑚𝑒𝑛𝑡 (3)\nTo better simulate a real scenario, a safeguard stop-loss has been\nimplemented. Precisely, we have experimented five values of stop-loss,", "tags": []}
{"fragment_id": "F_R17_p7_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p7:2", "text": "ranging from 0% to 10%, see Table 4. Wide stop-losses show a better\nperformance than narrow ones, thus in the following we consider the\n10% stop-loss threshold.\nThe ROIs are largely positive: the peak is reached for Ethereum\nin the long period with a ROI of 165.91. We have highlighted the\nwhole period (called long) and a period ranging from May 15 to\nDecember 4, 2022, called short. We chose the latter interval to better\nassess the model under heavily volatile market conditions impacted\nby two exogenous events. The first of these occurred in June 2022:\nthe TerraUSD-Classic (USTC) lost its Dollar peg, wiping out about\n$500 billion of market capitalization (see Briola et al., 2023 for a\ndetailed description of the event). The second crash occurred in Novem-\nber 2022 and involved a popular cryptocurrency exchange: FTX. This\nconstitutes a good example of an exogenous-force-driven crash; Jalan\nTable 5\nMLP and dummy model backtesting number of transactions and max return.\nPeriod Coin MLP Max Ret Dummy Max Ret MLP trs Dummy trs\nlong ALGO 0.54 −0.99 376 643\nshort ALGO 0.18 −0.42 36 110\nlong BTC 0.29 −0.77 246 893\nshort BTC 0.16 −0.44 28 106\nlong ETH 0.76 −0.91 379 930\nshort ETH 0.26 −0.41 37 111\nand Matkovskyy (2023) concludes that the major fault lay with the\nmanagement and not in the crypto environment.\nIn order to acquire a complete overview of the trading, we also", "tags": []}
{"fragment_id": "F_R17_p7_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p7:3", "text": "reported the number of trades and the maximum return per trade\nobtained in the period in Table 5.\nAs expected, the Dummy model returns slowly decrease to zero,\nyielding negative ROIs, reported in the figures in logarithmic scale. The\nlong backtest shown in Figs. 5(a), 6(a) and 7(a) depicts a curve that is\ntypical of a trading strategy with a statistical edge over a random one:\na slow but stable increase in the return.\nThe short backtests in Figs. 5(b), 6(b) and 7(b) during the crashes\nexpose the behavior expected from a strategy, mostly based on lagged\nindicators. In the next section will illustrate how lagged indicators\ndetermine most of the output of the MLP classifier. The initial Terra-\nLuna crash is smoothed by the MLP model in comparison to the Dummy\nmodel. The same behavior can be inferred from the FTX crash, near the\nend of the charts.\n5.1. Comparisons with recent papers\nComparing methodologies on price forecasting and trading strate-\ngies is a complex task. A fair comparison of different results requires\nto have some invariants in the experiment setup. To compare models\nfrom different methodologies a not empty intersection of the datasets is\nrequired. This way researchers can use shared data to evaluate models\nusing the same metrics. This is on the data science side. On the financial\nside the same problem arises because of the added step of backtesting,", "tags": []}
{"fragment_id": "F_R17_p7_4", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p7:4", "text": "since this requires a shared period of the data in common, at the same\n(or similar) time sampling interval, to be used as evaluation set. Just\nto mention, the model is a component of a trading strategy: it acts as\nan oracle that guesses the future price, but the final decisions is up to", "tags": []}
{"fragment_id": "F_R17_p8_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p8:1", "text": "Expert Systems With Applications 238 (2024) 121806\n8\nM. Parente et al.\nFig. 6. BTCUSDT backtest.\nFig. 7. ALGOUSDT backtest.\nthe trading strategy. It uses information like commission fees, analysis\nof previous model prediction, (e.g. buy only after two buy signals in a\nrow), money management. Finally often there is a difficulty to acquire\ninformation to reproduce the tests, since the source code is missing.\nIn what follows we compare our work with five recent papers (Aky-\nildirim et al., 2021; Alonso-Monsalve et al., 2020; Cavalli & Amoretti,\n2021; Jaquart et al., 2021; Ozer & Okan Sakar, 2022) and in Tables 6\nand 7 we report a summary. Each paper uses a 2 classes labeling and a\ntimeframe of 1 min, or 1, 4, and 24 h, spanning over different periods\nof time. The data sources are OHLC+V for all of them, plus some\nother sources, like Twitter (Tw), Blockchain (Bch) and others financial\nindices. We have chosen their best accuracy to compare with ours. For\npapers that backtest their models, to compare ROIs, we wrote specific\ncode that apply our own strategy to the common coins on the same\nperiod. We reported the results o backtest comparisons in Table 7.\nIn the paper (Alonso-Monsalve et al., 2020) the price direction for\nsix popular coins (Bitcoin, Dash, Ether, Litecoin, Monero, and Ripple) is\nforecasted, spanning one year (third quarter of 2018 to second quarter", "tags": []}
{"fragment_id": "F_R17_p8_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p8:2", "text": "of 2019) at a time frame of 1 min. They use technical indicators and\nmoving averages as features to feed different neural networks: CLSTM,\nMLP, CNN, RBFNN. To compare their metrics with ours, since they\nreport for each model the accuracy of the prediction on each coin, we\nhave computed the mean accuracy of the best model, the CLSTM, for\nthe six coins.\nThe paper (Ozer & Okan Sakar, 2022) forecasts price direction for\nBitcoin, Ethereum and Litecoin. Data belongs to the period between\nJuly 1, 2017 and April 30, 2021. The features have been determined\nfrom technical indicators. The authors use 5 different classical models.\nWe report the accuracy of the logistic classifier, which is their best\naverage accuracy computed. A backtest was conducted on all of the\nthree coins at 4H and 1D time frame. The timespan and the coin set\nhas a non empty intersection with ours, and this allowed us to compare\nthe results of BTC and ETH. The period spans from the beginning of\nMay 2020 to the end of April 2021, a period with a sideway trend that", "tags": []}
{"fragment_id": "F_R17_p9_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p9:1", "text": "Expert Systems With Applications 238 (2024) 121806\n9\nM. Parente et al.\nTable 6\nComparing post 2020 similar works by accuracy.\nRef Coins Model Acc TF Period Source\nAlonso-Monsalve et al. (2020) 6 Topmost CLSTM 68.17 1M 1 Year OHLC+V\nOzer and Okan Sakar (2022) BTC, ETH, LTC Various 55.9 4H 4 Years OHLC+V\nJaquart et al. (2021) BTC LSTM 56 1H 9 Month OHLC+V, others\nAkyildirim et al. (2021) 12 Topmost SVM 53 1H 5 Year OHLC+V\nCavalli and Amoretti (2021) BTC CNN 74.2 1D 7 Years Price, Tw, Bch\nOur paper 402 coins MLP 72 4H 5 Year OHLC+V\nTable 7\nComparison of ROIs.\nCoin Our paper Ozer and Okan Sakar (2022) Period Fee\nBtc 39 63 May 2020–Apr 2021 0.1%\nEth 66.5 106 May 2020–Apr 2021 0.1%\nCoin Our paper Jaquart et al. (2021) Period Fee\nBtc −0.6 Negative Set–Dic 2019 30 Bps – 0.3%\nCoin Our paper Cavalli and Amoretti (2021) Period Fee\nBtc 99.6 96.1 Feb–Oct 2020 no fee\nBtc −4.4 −28.3 Feb–Mar 2020 no fee\nends with a strong bullish move. The backtest was done with buy-only\nstrategy, like ours. The main difference in the experiment is the number\nof trades, our system trades at a lower frequency. Nothing can be said to\nthe edge of the strategy in a bearish market, as the trend in the period\nis sideway with a short bullish in the last months.\nThe paper (Jaquart et al., 2021) predicts Bitcoin price direction.\nData span from March 4, 2019 to December 10, 2019 and use Twitter", "tags": []}
{"fragment_id": "F_R17_p9_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p9:2", "text": "sentiments, blockchain transactions and a variety of minute level prices\nfor commodities and indices: Bitcoin, gold, oil, the indices MSCI World,\nS&P 500, and VIX. The models used are neural networks and decision\ntrees, plain or boosted ensemble. The accuracy is reported for each\nmodel for 1, 5, 15 and 60 min time frame. In Table 6 we reported the\naccuracy of the best model in the 60 min time frame. The authors did\na backtest from September to December 2019 and exhibit a ROI on\nthe best model (LSTM) of 115% but without commission fees applied.\nThe Bitcoin trend in this interval is bearish and it is stated that by\napplying fee at 0.3% turns the ROI negative, without quantifying the\nloss. The peculiarity of the work is the very high frequency trading\nstrategy, which executes 2852 trades during the backtesting, (ours just\n4). We can try to estimate the loss by supposing that no compounding\nis applied and 0.1% fees, the transactions reported sum to a 258% loss\ndue to the fees plus the earning or loss for the trades.\nIn Akyildirim et al. (2021) the authors predict price direction of the\n12 most popular cryptocurrencies (BTC, BCH, DSH EOS, ETC, ETH, IOT,\nLTC, OMG, XMR, XRP, ZEC), whose data span from August 10, 2017 to\nJune 23, 2018. The dataset is composed by OHLC+V data at different\ntime frames along with trading data taken from a popular broker. The", "tags": []}
{"fragment_id": "F_R17_p9_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p9:3", "text": "features are computed by OHLC+V and pure traded data and then\nby transforming them using technical indicators, moving averages and\nlog returns for different time intervals. The labels are computed by\n𝑆𝑖𝑔𝑛𝑢𝑚(𝐶𝑙𝑜𝑠𝑒−𝑂𝑝𝑒𝑛). The accuracy reported in the paper is the average\nfor the 1H time frame, which is also close to the best accuracy (low\nvariance) on the test set.\nThe paper (Cavalli & Amoretti, 2021) forecasts price direction\nfor Bitcoin. Data span from April 28, 2013 to February 15, 2020.\nThe feature set uses data from market prices, Twitter sentiment and\nblockchain. The model adopted is a 1D Convolutional Neural Network.\nThe backtest is arranged in two different runs, the first one from\nFebruary to October 2020 (bullish) and the second one from February\nto March 2020 (a short period downtrend). No commission fees are\napplied to the transactions. In the first run the performance values are\nsimilar to ours, though in a slight advantage. In the second run the\nsystem registered a loss of 28% (without commission fees), whereas\nour system exhibits a loss of 4.4% (with fees).\n6. Feature importance\nThe field of interpretability/explainability in ML pertains to the\ncomprehension of a model’s behavior in predicting outcomes by map-\nping input to output. It is a continuously evolving area of study that\naims to enhance the transparency of ML models (Doshi-Velez & Kim,", "tags": []}
{"fragment_id": "F_R17_p9_4", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p9:4", "text": "2017). Despite some exceptions (e.g. linear models, decision trees,\nrandom forests), where explanation of the model’s behavior is acces-\nsible in terms of how much a feature determines the global model\nbehavior, Molnar (2022), MLP models have no direct way for their\ninterpretation and are often treated as a black box oracle. Global\nmodel behavior provides a general understanding of how the model\nworks: which features are most important, and how they interact with\neach other. Global explanations are usually computed based on the\nanalysis of multiple instances and are more suitable for generalizing\nthe behavior of the model.\nSHAP (SHapley Additive exPlanations), Lundberg and Lee (2017), is\na game theoretic approach to explain the output of any black-box ML\nmodel. SHAP computes a value for the feature importance attribution,\nwhich is an implementation of the popular Shapley Values (Shapley,\n1953). It is used to solve an attribution problem, by distributing the\nprediction of a model for a specific input to its base set of features\nand showing how influential a feature is in making a decision. Clearly,\nthis approach provides a local explanation for each specific input of\nthe model. In the implementation, we encoded the label Buy with\n−1, Hold with 0 and Sell with 1. SHAP values are in the range\nof the classifier output, and thus the values are in the ] −1,1[ range", "tags": []}
{"fragment_id": "F_R17_p9_5", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p9:5", "text": "although they may sometimes assume values slightly outside the range\nboundaries.\nOverall feature importance can be inferred by computing the mean\nof SHAP absolute values for each feature, as shown in Fig. 8.\nThe chart in Fig. 9 shows the top-10 contributions of each feature\nto each prediction. More precisely this chart can be used to spot\ncorrelations between feature values and SHAP values using colors. For\nexample, the Bollinger feature has a clear correlation with the output\nof the classifier, increasing the Bollinger value (on the vertical axis)\nand the output steers from Sell to Buy (on the horizontal axis). It\nis counterintuitive since in the trading literature a higher value of the\nBollinger indicator means trend inversion. However, the model learned\nBollinger values as a trend continuation feature. EmaCross1_21 and RSI,\ndisplay a similar behavior. EmaCross21_50, instead, shows an inverse\ncorrelation with its SHAP value. The remaining features do not exhibit\na behavior that can be correlated with SHAP values.\nAs a local explanation, the SHAP values may be used to ask how a\nspecific input can be explained in terms of feature values, or in other", "tags": []}
{"fragment_id": "F_R17_p10_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p10:1", "text": "Expert Systems With Applications 238 (2024) 121806\n10\nM. Parente et al.\nFig. 8. Top-10 feature mean contributions.\nFig. 9. Feature impact on model output.\nwords, how a specific feature value steers the output of the classifier\ninto the prediction. Waterfall plots achieve this by charting the con-\ntribution of each feature to the prediction. The chart in Fig. 10 shows\nthe SHAP values for the ten most important features. The vertical axis\nlists the features and their standardized values, while the horizontal\naxis reports the classifier output. The arrows show the extent to which\nevery feature contributes to the prediction.\nThe impact of the remaining 26 features on the output is almost 0.\nIn Fig. 11, the output strays a few hundredths from the expected\nvalue 0 for the Hold label.", "tags": []}
{"fragment_id": "F_R17_p11_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p11:1", "text": "Expert Systems With Applications 238 (2024) 121806\n11\nM. Parente et al.\nFig. 10. Prediction of an input vector in terms of top-10 features whose standardized values are on the left.\nFig. 11. Prediction of an input vector in terms of top-10 features whose standardized values are on the left.\nIn conclusion, the investigation of the feature importance indicates\nthat the top-10 most valuable features are comprised solely of techni-\ncal indicators, moving average crossovers and temporal information.\nCandlestick patterns are seen to be relatively ineffective due to their\ninfrequency or absence in some cases, and their ability to provide\nrelevant information for determining the output of the classifier is\nlimited.\n7. Conclusion\nThe trading automation poses various challenges related to the\nfeature extraction and labeling process, as well as the practical im-\nplementation of predictive models in real or simulated market set-\ntings. Prediction of the trend direction of a blockchain-based asset\nprice presents multiple technical complexities due to poor regulation\nby authorities and market manipulations resulting in the pumping\nand dumping of selected assets (Eigelshoven & Ullrich, 2021; Fratrič\nP. Sileno et al., 2022; Gandal et al., 2018) which leads to high volatility.\nHigher price fluctuations imply higher profit opportunities, but these", "tags": []}
{"fragment_id": "F_R17_p11_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p11:2", "text": "profit opportunities are not easy to spot. The proposed approach re-\nformulates the traditional forecasting model as a classification process.\nIn particular, we have shown that a simpler approach supported by a\nmassive dataset and smart labeling can achieve a comparable or better\naccuracy than more complex models. For example, a recent work (Ozer\n& Okan Sakar, 2022) achieves a 56% accuracy on a binary labeled\nscheme. Under the assumption of a balanced dataset, this is only 6%\nover a random baseline. Whereas we have achieved a 66% accuracy,\nwhich is an excellent performance compared to a 33% baseline with a\n3-classes problem.\nWe have developed a comprehensive pipeline for predicting short-\nterm price trends and leveraging them to generate profitable trading\nstrategies. Our findings reveal that the accuracy of price trend predic-\ntions is highly dependent on the use of a large dataset that enables the\nidentification of patterns via technical analysis tools. We have shown\nthat our trading system is profitable in every market conditions: bull,\nflat and in the more difficult bear/high volatile ones.\nMoreover, we have also implemented a backtesting process proving\nthat a technical analysis approach can provide a clear statistical edge\nover random trading operations or a simple ‘‘buy and hold’’ approach.\nIn fact the backtest phase has shown a desirable characteristic in a", "tags": []}
{"fragment_id": "F_R17_p11_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p11:3", "text": "trading system: the low volatility of the equity curves. Furthermore,\nby employing various stop-loss thresholds, it is possible to fine-tune\nthe risk profile showing thus that high thresholds corroborate the well-\nestablished principle that higher risks are typically associated with\nhigher rewards. It is important to note that our results do not guarantee\nthat the same level of performance can be replicated in different time\nperiods or with different coins. Moreover real market setups have to\ncope with liquidity issues that minor coins often experience, which may\nunpredictably and adversely impact the system’s performance.", "tags": []}
{"fragment_id": "F_R17_p12_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p12:1", "text": "Expert Systems With Applications 238 (2024) 121806\n12\nM. Parente et al.\nBased on the analysis of feature importance, it has been determined\nthat the top ten valuable features exclusively consist of technical in-\ndicators, moving average crossovers, and temporal information, with\ncandlestick patterns showing a relatively low level of effectiveness.\nThese results lay the groundwork for future research endeavors\nthat aim to augment the existing pipeline with additional information\ngleaned from multi-timeframe price action analysis. This approach\ncan be accomplished by combining the pipeline’s key components,\nsuch as the large dataset, labeling algorithm, best features and neural\nnetwork, with novel information derived from a detailed analysis of\nprice behavior across multiple timeframes.\nFurthermore, technical analysis has ample room to explore with the\nsupport of ML techniques. Technical indicators are just one tool, other\nsources of information used by traders to spot price directions com-\nprises technical patterns, supports and resistances (fixed and dynamic),\nFibonacci levels, cross assets analysis.\nAs a prospective avenue for future research, it is worthwhile to\nsubject our trading system to empirical testing across diverse financial\nmarkets, including FOREX, individual stocks, stock indexes, commodi-\nties, and CFDs (Contracts for Difference). This endeavor is based on the", "tags": []}
{"fragment_id": "F_R17_p12_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p12:2", "text": "premise that these markets exhibit congruence in terms of adhering to\nsimilar technical patterns.\nNonetheless, prudent consideration must be given to CFDs, as they\nrepresent leveraged derivatives characterized by elevated risk exposure.\nWhile CFDs generally exhibit substantial correlation with the prices of\nunderlying securities, this association is not always exact. The notable\nfeature of CFDs lies in their requirement for a comparatively small\namount of capital (via margin trading) to facilitate trading activities.\nThis aspect significantly reduces the entry threshold for implementing\nthe trading system in actual financial markets, making the exploration\nof real-world experimentation a feasible pursuit.\nCRediT authorship contribution statement\nMimmo Parente: Conceptualization, Data curation, Formal anal-\nysis, Investigation, Methodology, Supervision, Validation, Writing –\noriginal draft, Writing – review & editing. Luca Rizzuti: Conceptu-\nalization, Data curation, Formal analysis, Investigation, Methodology,\nSoftware, Validation, Writing – original draft, Writing – review & edit-\ning. Mario Trerotola:Conceptualization, Data curation, Investigation,\nInitial writing of software, Validation, Writing – review & editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing finan-", "tags": []}
{"fragment_id": "F_R17_p12_3", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p12:3", "text": "cial interests or personal relationships that could have appeared to\ninfluence the work reported in this paper.\nData availability\nData are publicly available at the url indicated in the bibliography.\nReferences\nAkyildirim, E., Goncu, A., & Sensoy, A. (2021). Prediction of cryptocurrency returns\nusing machine learning. Annals of Operations Research, 297, 3–36.\nAlec, N., & Kercheval, Y. Z. (2015). Modelling high-frequency limit order book dynamics\nwith support vector machines. Quantitative Finance, 15, 1315–1329.\nAlonso-Monsalve, S., Suárez-Cetrulo, A. L., Cervantes, A., & Quintana, D. (2020). Con-\nvolution on neural networks for high-frequency trend prediction of cryptocurrency\nexchange rates using technical indicators. Expert Systems with Applications, 149,\nArticle 113250.\nBaur, D. G., Cahill, D., Godfrey, K., & (Frank) Liu, Z. (2019). Bitcoin time-of-day, day-\nof-week and month-of-year effects in returns and trading volume. Finance Research\nLetters, 31, 78–92.\nBriola, A., Vidal-Tomás, D., Wang, Y., & Aste, T. (2023). Anatomy of a stablecoin’s\nfailure: The Terra-Luna case. Finance Research Letters, 51, Article 103358.\nBuda, M., Maki, A., & Mazurowski, M. A. (2018). A systematic study of the\nclass imbalance problem in convolutional neural networks. Neural Networks, 106,\n249–259.\nCavalli, S., & Amoretti, M. (2021). Cnn-based multivariate data analysis for bitcoin", "tags": []}
{"fragment_id": "F_R17_p12_4", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p12:4", "text": "trend prediction. Applied Soft Computing, 101, Article 107065.\nChu, J., Zhang, Y., & Chan, S. (2019). The adaptive market hypothesis in the high\nfrequency cryptocurrency market. International Review of Financial Analysis , 64,\n221–231.\nDoshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine\nlearning. arXiv:1702.08608.\nEigelshoven, F., & Ullrich, A. (2021). Cryptocurrency market manipulation: A systematic\nliterature review. In International conference on information systems(p. 1225).\nEngle, R. F., Ghysels, E., & Sohn, B. (2013). Stock market volatility and macroeconomic\nfundamentals. The Review of Economics and Statistics, 95, 776–797.\nFang, F., Ventre, C., Basios, M., Kanthan, L., Martinez-Rego, D., Wu, F., & Li, L. (2022).\nCryptocurrency trading: A comprehensive survey. Financial Innovation, 8, 1–59.\nFratrič P. Sileno, G., Klous, S., & van Engers, T. (2022). Manipulation of the bitcoin\nmarket: an agent-based study. Financial Innovation, 8, 60.\nGandal, N., Hamrick, J., Moore, T., & Oberman, T. (2018). Price manipulation in the\nbitcoin ecosystem. Journal of Monetary Economics, 95, 86–96.\nGerlein, E. A., McGinnity, M., Belatreche, A., & Coleman, S. (2016). Evaluating machine\nlearning classification for financial trading: An empirical approach. Expert Systems\nwith Applications, 54, 193–207.", "tags": []}
{"fragment_id": "F_R17_p12_5", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p12:5", "text": "Guo, T., Bifet, A., & Antulov-Fantulin, N. (2018). Bitcoin volatility forecasting with\na glimpse into buy and sell orders. In 2018 IEEE international conference on data\nmining (pp. 989–994).\nGuo, H., Zhang, D., Liu, S., Wang, L., & Ding, Y. (2021). Bitcoin price forecasting:\nA perspective of underlying blockchain transactions. Decision Support Systems, 151,\nArticle 113650.\nHopcroft, J. E., Motwani, R., & Ullman, J. D. (2006). Introduction to automata theory,\nlanguages, and computation(3rd ed.). USA: Addison-Wesley Longman Publishing Co.\nInc..\nHornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are\nuniversal approximators. Neural Networks, 2, 359–366.\nHunter, D., Yu, H., Pukish, . M. S., III, Kolbusz, J., & Wilamowski, B. M. (2012).\nSelection of proper neural network sizes and architectures–A comparative study.\nIEEE Transactions on Industrial Informatics, 8, 228–240.\nJalan, A., & Matkovskyy, R. (2023). Systemic risks in the cryptocurrency market:\nEvidence from the ftx collapse. Finance Research Letters, 53, Article 103670.\nJaquart, P., Dann, D., & Weinhardt, C. (2021). Short-term bitcoin market prediction\nvia machine learning. The Journal of Finance and Data Science, 7, 45–66.\nKaiser, L. (2019). Seasonality in cryptocurrencies. Finance Research Letters, 31.\nKardile, R., Ugale, T., & Mohanty, S. N. (2021). Stock price predictions using crossover", "tags": []}
{"fragment_id": "F_R17_p12_6", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p12:6", "text": "sma. In 2021 9th international conference on reliability, Infocom technologies and\noptimization (trends and future directions)(pp. 1–5).\nKim, H. M., Bock, G. W., & Lee, G. (2021). Predicting ethereum prices with machine\nlearning based on blockchain information. Expert Systems with Applications, 184,\nArticle 115480.\nKim, Y. B., Kim, J. G., Kim, W., Im, J. H., Kim, T. H., Kang, S. J., & Kim, C. H. (2016).\nPredicting fluctuations in cryptocurrency transactions based on user comments and\nreplies. PLoS One, 11, 1–17.\nKraaijeveld, O., & De Smedt, J. (2020). The predictive power of public twitter sentiment\nfor forecasting cryptocurrency prices. Journal of International Financial Markets,\nInstitutions and Money, 65, Article 101188.\nKumbure, M. M., Lohrmann, C., Luukka, P., & Porras, J. (2022). Machine learning\ntechniques and data for stock market forecasting: A literature review. Expert Systems\nwith Applications, 197, Article 116659.\nLahmiri, S., & Bekiros, S. (2019). Cryptocurrency forecasting with deep learning chaotic\nneural networks. Chaos, Solitons & Fractals, 118, 35–40.\nLi, X., & Du, L. (2023). Bitcoin daily price prediction through understanding blockchain\ntransaction pattern with machine learning methods. Journal of Combinatorial\nOptimization, 45, 1–24.\nLundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model", "tags": []}
{"fragment_id": "F_R17_p12_7", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p12:7", "text": "predictions. Advances in Neural Information Processing Systems, 30.\nMalkiel, B. G. (2003). The efficient market hypothesis and its critics. Journal of Economic\nPerspectives, 17, 59–82.\nMolnar, C. (2022). Interpretable machine learning (2nd ed.). CM.\nMurphy, J. J. (1999). Technical analysis of the financial markets: A comprehensive\nguide to trading methods and applications. Penguin.\nOlorunnimbe, K., & Viktor, H. (2023). Deep learning in the stock market—A systematic\nsurvey of practice, backtesting, and applications. Artificial Intelligence Review, 56,\n2057–2109.\nOzer, F., & Okan Sakar, C. (2022). An automated cryptocurrency trading system based\non the detection of unusual price movements with a time-series clustering-based\napproach. Expert Systems with Applications, 200, Article 117017.", "tags": []}
{"fragment_id": "F_R17_p13_1", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p13:1", "text": "Expert Systems With Applications 238 (2024) 121806\n13\nM. Parente et al.\nParente, M., Rizzuti, L., & Trerotola, M. (2023). Cryptotrading.zip. http://dx.doi.org/\n10.6084/m9.figshare.22953377.v1, Figshare URL: https://figshare.com/articles/\nsoftware/CryptoTrading_zip/22953377.\nParvini, N., Abdollahi, M., Seifollahi, S., & Ahmadian, D. (2022). Forecasting bitcoin\nreturns with long short-term memory networks and wavelet decomposition: A\ncomparison of several market determinants. Applied Soft Computing, 121, Article\n108707.\nPring, M. (1991). Technical analysis explained: the successful investor’s guide to spotting\ninvestment trends and turning points. McGraw-Hill.\nSaad, M., Choi, J., Nyang, D., Kim, J., & Mohaisen, A. (2020). Toward characterizing\nblockchain-based cryptocurrencies for highly accurate predictions. IEEE Systems\nJournal, 14, 321–332.\nSattarov, O., Jeon, H. S., Oh, R., & Lee, J. D. (2020). Forecasting bitcoin price\nfluctuation by twitter sentiment analysis. In 2020 international conference on\ninformation science and communications technologies(pp. 1–4).\nSetiono, R. (2001). Feedforward neural network construction using cross validation.\nNeural Computation, 13, 2865–2877.\nShapley, L. S. (1953). A value for n-person games. In H. W. Kuhn, & A. W. Tucker\n(Eds.), Contributions to the theory of games II(pp. 307–317). Princeton: Princeton\nUniversity Press.", "tags": []}
{"fragment_id": "F_R17_p13_2", "source_id": "R17", "locator": "1-s2.0-S0957417423023084-main.pdf:p13:2", "text": "Tsantekidis, A., Passalis, N., Tefas, A., Kanniainen, J., Gabbouj, M., & Iosifidis, A.\n(2017). Forecasting stock prices from the limit order book using convolutional\nneural networks. In 2017 IEEE 19th conference on business informatics(pp. 7–12).\nValencia, F., Gómez-Espinosa, A., & Valdés-Aguirre, B. (2019). Price movement predic-\ntion of cryptocurrencies using sentiment analysis and machine learning. Entropy,\n21.\nWalther, T., Klein, T., & Bouri, E. (2019). Exogenous drivers of bitcoin and cryp-\ntocurrency volatility –A mixed data sampling approach to forecasting. Journal of\nInternational Financial Markets, Institutions and Money, 63, Article 101133.\nZhang, W., Wang, P., Li, X., & Shen, D. (2018). The inefficiency of cryptocurrency\nand its cross-correlation with dow jones industrial average. Physica A. Statistical\nMechanics and its Applications, 510, 658–670.", "tags": []}
{"fragment_id": "F_R18_p1_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p1:1", "text": "An Application of Deep Reinforcement Learning to Algorithmic Trading\nThibaut Th´ eatea,∗, Damien Ernsta\naMonteﬁore Institute, University of Li` ege (All´ ee de la d´ ecouverte 10, 4000 Li` ege, Belgium)\nAbstract\nThis scientiﬁc research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the\nalgorithmic trading problem of determining the optimal trading position at any point in time during a trading activity\nin the stock market. It proposes a novel DRL trading policy so as to maximise the resulting Sharpe ratio performance\nindicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new\nDRL approach is inspired from the popular DQN algorithm and signiﬁcantly adapted to the speciﬁc algorithmic trading\nproblem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of\nartiﬁcial trajectories from a limited set of stock market historical data. In order to objectively assess the performance\nof trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology.\nFollowing this new performance assessment approach, promising results are reported for the TDQN algorithm.\nKeywords: Artiﬁcial intelligence, deep reinforcement learning, algorithmic trading, trading policy.\n1. Introduction", "tags": []}
{"fragment_id": "F_R18_p1_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p1:2", "text": "For the past few years, the interest in artiﬁcial intelli-\ngence (AI) has grown at a very fast pace, with numerous\nresearch papers published every year. A key element for\nthis growing interest is related to the impressive successes\nof deep learning (DL) techniques which are based on deep\nneural networks (DNN) - mathematical models directly in-\nspired by the human brain structure. These speciﬁc tech-\nniques are nowadays the state of the art in many appli-\ncations such as speech recognition, image classiﬁcation or\nnatural language processing. In parallel to DL, another\nﬁeld of research has recently gained much more attention\nfrom the research community: deep reinforcement learn-\ning (DRL). This family of techniques is concerned with\nthe learning process of an intelligent agent (i) interacting\nin a sequential manner with an unknown environment (ii)\naiming to maximise its cumulative rewards and (iii) us-\ning DL techniques to generalise the information acquired\nfrom the interaction with the environment. The many re-\ncent successes of DRL techniques highlight their ability to\nsolve complex sequential decision-making problems.\nNowadays, an emerging industry which is growing ex-\ntremely fast is the ﬁnancial technology industry, generally\nreferred to by the abbreviation FinTech. The objective of\nFinTech is pretty simple: to extensively take advantage", "tags": []}
{"fragment_id": "F_R18_p1_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p1:3", "text": "of technology in order to innovate and improve activities\nin ﬁnance. In the coming years, the FinTech industry is\n∗Corresponding author.\nEmail addresses: thibaut.theate@uliege.be (Thibaut\nTh´ eate),dernst@uliege.be (Damien Ernst)\nexpected to revolutionise the way many decision-making\nproblems related to the ﬁnancial sector are addressed, in-\ncluding the problems related to trading, investment, risk\nmanagement, portfolio management, fraud detection and\nﬁnancial advising, to cite a few. Such complex decision-\nmaking problems are extremely complex to solve as they\ngenerally have a sequential nature and are highly stochas-\ntic, with an environment partially observable and poten-\ntially adversarial. In particular, algorithmic trading, which\nis a key sector of the FinTech industry, presents particu-\nlarly interesting challenges. Also called quantitative trad-\ning, algorithmic trading is the methodology to trade using\ncomputers and a speciﬁc set of mathematical rules.\nThe main objective of this research paper is to an-\nswer the following question: how to design a novel trad-\ning policy (algorithm) based on AI techniques that could\ncompete with the popular algorithmic trading strategies\nwidely adopted in practice? To answer this question, this\nscientiﬁc article presents and analyses a novel DRL solu-\ntion to tackle the algorithmic trading problem of deter-", "tags": []}
{"fragment_id": "F_R18_p1_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p1:4", "text": "mining the optimal trading position (long or short) at any\npoint in time during a trading activity in the stock market.\nThe algorithmic solution presented in this research paper\nis inspired by the popular Deep Q-Network (DQN) algo-\nrithm, which has been adapted to the particular sequential\ndecision-making problem at hand. The research question\nto be answered is all the more relevant as the trading envi-\nronment presents very diﬀerent characteristics from those\nwhich have already been successfully solved by DRL ap-\nproaches, mainly signiﬁcant stochasticity and extremely\npoor observability.\narXiv:2004.06627v3 [q-fin.TR] 9 Oct 2020", "tags": []}
{"fragment_id": "F_R18_p2_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p2:1", "text": "The scientiﬁc research paper is structured as follows.\nFirst of all, a brief review of the scientiﬁc literature around\nthe algorithmic trading ﬁeld and its main AI-based contri-\nbutions is presented in Section 2. Afterwards, Section 3 in-\ntroduces and rigorously formalises the particular algorith-\nmic trading problem considered. Additionally, this section\nmakes the link with the reinforcement learning (RL) ap-\nproach. Then, Section 4 covers the complete design of the\nTDQN trading strategy based on DRL concepts. Subse-\nquently, Section 5 proposes a novel methodology to objec-\ntively assess the performance of trading strategies. Section\n6 is concerned with the presentation and discussion of the\nresults achieved by the TDQN trading strategy. To end\nthis research paper, Section 7 discusses interesting leads\nas future work and draws meaningful conclusions.\n2. Literature review\nTo begin this brief literature review, two facts have to\nbe emphasised. Firstly, it is important to be aware that\nmany sound scientiﬁc works in the ﬁeld of algorithmic trad-\ning are not publicly available. As explained in Li (2017),\ndue to the huge amount of money at stake, private FinTech\nﬁrms are very unlikely to make their latest research results\npublic. Secondly, it should be acknowledged that making a\nfair comparison between trading strategies is a challenging", "tags": []}
{"fragment_id": "F_R18_p2_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p2:2", "text": "task, due to the lack of a common, well-established frame-\nwork to properly evaluate their performance. Instead, the\nauthors generally deﬁne their own framework with their\nevident bias. Another major problem is related to the\ntrading costs which are variously deﬁned or even omitted.\nFirst of all, most of the works in algorithmic trading are\ntechniques developed by mathematicians, economists and\ntraders who do not exploit AI. Typical examples of clas-\nsical trading strategies are the trend following and mean\nreversion strategies, which are covered in detail in Chan\n(2009), Chan (2013) and Narang (2009). Then, the major-\nity of works applying machine learning (ML) techniques in\nthe algorithmic trading ﬁeld focus on forecasting. If the\nﬁnancial market evolution is known in advance with a rea-\nsonable level of conﬁdence, the optimal trading decisions\ncan easily be computed. Following this approach, DL tech-\nniques have already been investigated with good results,\nsee e.g. Ar´ evalo et al. (2016) introducing a trading strat-\negy based on a DNN, and especially Bao et al. (2017) using\nwavelet transforms, stacked autoencoders and long short-\nterm memory (LSTM). Alternatively, several authors have\nalready investigated RL techniques to solve this algorith-\nmic trading problem. For instance, Moody and Saﬀell\n(2001) introduced a recurrent RL algorithm for discover-", "tags": []}
{"fragment_id": "F_R18_p2_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p2:3", "text": "ing new investment policies without the need to build fore-\ncasting models, and Dempster and Leemans (2006) used\nadaptive RL to trade in foreign exchange markets. More\nrecently, a few works investigated DRL techniques in a sci-\nentiﬁcally sound way to solve this particular algorithmic\ntrading problem. For instance, one can ﬁrst mention Deng\net al. (2017) which introduced the fuzzy recurrent deep\nneural network structure to obtain a technical-indicator-\nfree trading system taking advantage of fuzzy learning to\nreduce the time series uncertainty. One can also mention\nCarapu¸ co et al. (2018) which studied the application of the\ndeep Q-learning algorithm for trading in foreign exchange\nmarkets. Finally, there exist a few interesting works study-\ning the application of DRL techniques to algorithmic trad-\ning in speciﬁc markets, such as in the ﬁeld of energy, see\ne.g. the article Boukas et al. (2020).\nTo ﬁnish with this short literature review, a sensi-\ntive problem in the scientiﬁc literature is the tendency to\nprioritise the communication of good results or ﬁndings,\nsometimes at the cost of a proper scientiﬁc approach with\nobjective criticism. Going even further, Ioannidis (2005)\neven states that most published research ﬁndings in cer-\ntain sensitive ﬁelds are probably false. Such concern ap-\npears to be all the more relevant in the ﬁeld of ﬁnancial", "tags": []}
{"fragment_id": "F_R18_p2_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p2:4", "text": "sciences, especially when the subject directly relates to\ntrading activities. Indeed, Bailey et al. (2014) claims that\nmany scientiﬁc publications in ﬁnance suﬀer from a lack\nof a proper scientiﬁc approach, instead getting closer to\npseudo-mathematics and ﬁnancial charlatanism than rig-\norous sciences. Aware of these concerning tendencies, the\npresent research paper intends to deliver an unbiased sci-\nentiﬁc evaluation of the novel DRL algorithm proposed.\n3. Algorithmic trading problem formalisation\nIn this section, the sequential decision-making algorith-\nmic trading problem studied in this research paper is pre-\nsented in detail. Moreover, a rigorous formalisation of this\nparticular problem is performed. Additionally, the link\nwith the RL formalism is highlighted.\n3.1. Algorithmic trading\nAlgorithmic trading, also called quantitative trading,\nis a subﬁeld of ﬁnance, which can be viewed as the ap-\nproach of automatically making trading decisions based on\na set of mathematical rules computed by a machine. This\ncommonly accepted deﬁnition is adopted in this research\npaper, although other deﬁnitions exist in the literature.\nIndeed, several authors diﬀerentiate the trading decisions\n(quantitative trading) from the actual trading execution\n(algorithmic trading). For the sake of generality, algo-\nrithmic trading and quantitative trading are considered", "tags": []}
{"fragment_id": "F_R18_p2_5", "source_id": "R18", "locator": "2004.06627v3.pdf:p2:5", "text": "synonyms in this research paper, deﬁning the entire auto-\nmated trading process. Algorithmic trading has already\nproven to be very beneﬁcial to markets, the main beneﬁt\nbeing the signiﬁcant improvement in liquidity, as discussed\nin Hendershott et al. (2011). For more information about\nthis speciﬁc ﬁeld, please refer to Treleaven et al. (2013)\nand Nuti et al. (2011).\nThere are many diﬀerent markets suitable to apply al-\ngorithmic trading strategies. Stocks and shares can be\n2", "tags": []}
{"fragment_id": "F_R18_p3_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p3:1", "text": "traded in the stock markets, FOREX trading is concerned\nwith foreign currencies, or a trader could invest in com-\nmodity futures, to only cite a few. The recent rise of\ncryptocurrencies, such as the Bitcoin, oﬀers new inter-\nesting possibilities as well. Ideally, the DRL algorithms\ndeveloped in this research paper should be applicable to\nmultiple markets. However, the focus will be set on stock\nmarkets for now, with an extension to various other mar-\nkets planned in the future.\nIn fact, a trading activity can be viewed as the man-\nagement of a portfolio, which is a set of assets including\ndiverse stocks, bonds, commodities, currencies, etc. In\nthe scope of this research paper, the portfolio considered\nconsists of one single stock together with the agent cash.\nThe portfolio value vt is then composed of the trading\nagent cash value vc\nt and the share value vs\nt, which contin-\nuously evolves over time t. Buying and selling operations\nare simply cash and share exchanges. The trading agent\ninteracts with the stock market through an order book,\nwhich contains the entire set of buying orders ( bids) and\nselling orders ( asks). An example of a simple order book\nis depicted in Table 1. An order represents the willingness\nof a market participant to trade and is composed of a price\np, a quantity q and a side s (bid or ask). For a trade to", "tags": []}
{"fragment_id": "F_R18_p3_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p3:2", "text": "occur, a match between bid and ask orders is required, an\nevent which can only happen if pbid\nmax ≥pask\nmin, with pbid\nmax\n(pask\nmin) being the maximum (minimum) price of a bid (ask)\norder. Then, a trading agent faces a very diﬃcult task in\norder to generate proﬁt: what, when, how, at which price\nand which quantity to trade. This is the algorithmic trad-\ning complex sequential decision-making problem studied\nin this scientiﬁc research paper.\nTable 1: Example of a simple order book\nSide s Quantity q Price p\nAsk 3000 107\nAsk 1500 106\nAsk 500 105\nBid 1000 95\nBid 2000 94\nBid 4000 93\n3.2. Timeline discretisation\nSince trading decisions can be issued at any time, the\ntrading activity is a continuous process. In order to study\nthe algorithmic trading problem described in this research\npaper, a discretisation operation of the continuous time-\nline is performed. The trading timeline is discretized into\na high number of discrete trading time steps tof constant\nduration ∆t. In this research paper, for the sake of clarity,\nthe increment (decrement) operations t+1 (t−1) are used\nto model the discrete transition from time step t to time\nstep t+ ∆t (t−∆t).\nThe duration ∆ t is closely linked to the trading fre-\nquency targeted by the trading agent (very high trading\nfrequency, intraday, daily, monthly, etc.). Such discretisa-\ntion operation inevitably imposes a constraint with respect", "tags": []}
{"fragment_id": "F_R18_p3_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p3:3", "text": "to this trading frequency. Indeed, because the duration ∆t\nbetween two time steps cannot be chosen as small as pos-\nsible due to technical constraints, the maximum trading\nfrequency achievable, equal to 1 /∆t, is limited. In the\nscope of this research paper, this constraint is met as the\ntrading frequency targeted is daily, meaning that the trad-\ning agent makes a new decision once every day.\n3.3. Trading strategy\nThe algorithmic trading approach is rule based, mean-\ning that the trading decisions are made according to a set\nof rules: a trading strategy. In technical terms, a trading\nstrategy can be viewed as a programmed policy π(at|it),\neither deterministic or stochastic, which outputs a trad-\ning action at according to the information available to the\ntrading agent it at time step t. Additionally, a key char-\nacteristic of a trading strategy is its sequential aspect, as\nillustrated in Figure 1. An agent executing its trading\nstrategy sequentially applies the following steps:\n1. Update of the available market information it.\n2. Execution of the policy π(at|it) to get action at.\n3. Application of the designated trading action at.\n4. Next time step t→t+ 1, loop back to step 1.\nFigure 1: Illustration of a trading strategy execution\nIn the following subsection, the algorithmic trading se-\nquential decision-making problem, which shares similari-", "tags": []}
{"fragment_id": "F_R18_p3_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p3:4", "text": "ties with other problems successfully tackled by the RL\ncommunity, is casted as an RL problem.\n3.4. Reinforcement learning problem formalisation\nAs illustrated in Figure 2, reinforcement learning is\nconcerned with the sequential interaction of an agent with\nits environment. At each time step t, the RL agent ﬁrstly\nobserves the RL environment of internal state st, and re-\ntrieves an observation ot. It then executes the action at\nresulting from its RL policy π(at|ht) where ht is the RL\nagent history and receives a reward rt as a consequence of\nits action. In this RL context, the agent history can be\nexpressed as ht = {(oτ,aτ,rτ)|τ = 0,1,...,t }.\nReinforcement learning techniques are concerned with\nthe design of policies π maximising an optimality crite-\nrion, which directly depends on the immediate rewards rt\nobserved over a certain time horizon. The most popular\n3", "tags": []}
{"fragment_id": "F_R18_p4_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p4:1", "text": "optimality criterion is the expected discounted sum of re-\nwards over an inﬁnite time horizon. Mathematically, the\nresulting optimal policy π∗is expressed as the following:\nπ∗= argmax\nπ\nE[R|π] (1)\nR=\n∞∑\nt=0\nγtrt (2)\nThe parameter γ is the discount factor ( γ ∈ [0,1]).\nIt determines the importance of future rewards. For in-\nstance, if γ = 0, the RL agent is said to be myopic as\nit only considers the current reward and totally discards\nthe future rewards. When the discount factor increases,\nthe RL agent tends to become more long-term oriented.\nIn the extreme case where γ = 1, the RL agent considers\neach reward equally. This key parameter should be tuned\naccording to the desired behaviour.\nFigure 2: Reinforcement learning core building blocks\n3.4.1. RL observations\nIn the scope of this algorithmic trading problem, the\nRL environment is the entire complex trading world grav-\nitating around the RL agent. In fact, this trading envi-\nronment can be viewed as an abstraction including the\ntrading mechanisms together with every single piece of in-\nformation capable of having an eﬀect on the trading ac-\ntivity of the agent. A major challenge of the algorithmic\ntrading problem is the extremely poor observability of this\nenvironment. Indeed, a signiﬁcant amount of information\nis simply hidden to the trading agent, ranging from some\ncompanies’ conﬁdential information to the other market", "tags": []}
{"fragment_id": "F_R18_p4_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p4:2", "text": "participants’ strategies. In fact, the information available\nto the RL agent is extremely limited compared to the com-\nplexity of the environment. Moreover, this information\ncan take various forms, both quantitative and qualitative.\nCorrectly processing such information and re-expressing\nit using relevant quantitative ﬁgures while minimising the\nsubjective bias is capital. Finally, there are signiﬁcant time\ncorrelation complexities to deal with. Therefore, the infor-\nmation retrieved by the RL agent at each time step should\nbe considered sequentially as a series of information rather\nthan individually.\nAt each trading time step t, the RL agent observes\nthe stock market whose internal state is st ∈ S. The\nlimited information collected by the agent on this complex\ntrading environment is denoted by ot ∈O. Ideally, this\nobservation space Oshould encompass all the information\ncapable of inﬂuencing the market prices. Because of the\nsequential aspect of the algorithmic trading problem, an\nobservation ot has to be considered as a sequence of both\nthe information gathered during the previous τ time steps\n(history) and the newly available information at time step\nt. In this research paper, the RL agent observations can\nbe mathematically expressed as the following:\not = {S(t′), D(t′), T(t′), I(t′), M(t′), N(t′), E(t′)}t\nt′=t−τ\n(3)\nwhere:", "tags": []}
{"fragment_id": "F_R18_p4_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p4:3", "text": "• S(t) represents the state information of the RL agent\nat time step t (current trading position, number of\nshares owned by the agent, available cash).\n• D(t) is the information gathered by the agent at time\nstep tconcerning the OHLCV (Open-High-Low-Close-\nVolume) data characterising the stock market. More\nprecisely, D(t) can be expressed as follows:\nD(t) = {pO\nt , pH\nt , pL\nt , pC\nt , Vt} (4)\nwhere:\n– pO\nt is the stock market price at the opening of\nthe time period [ t−∆t, t[.\n– pH\nt is the highest stock market price over the\ntime period [ t−∆t, t[.\n– pL\nt is the lowest stock market price over the time\nperiod [t−∆t, t[.\n– pC\nt is the stock market price at the closing of\nthe time period [ t−∆t, t[.\n– Vt is the total volume of shares exchanged over\nthe time period [ t−∆t, t[.\n• T(t) is the agent information regarding the trading\ntime step t (date, weekday, time).\n• I(t) is the agent information regarding multiple tech-\nnical indicators about the stock market targeted at\ntime step t. There exist many technical indicators\nproviding extra insights about diverse ﬁnancial phe-\nnomena, such as moving average convergence diver-\ngence (MACD), relative strength index (RSI) or av-\nerage directional index (ADX), to only cite a few.\n• M(t) gathers the macroeconomic information at the\ndisposal of the agent at time step t. There are many\ninteresting macroeconomic indicators which could po-", "tags": []}
{"fragment_id": "F_R18_p4_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p4:4", "text": "tentially be useful to forecast markets’ evolution,\nsuch as the interest rate or the exchange rate.\n4", "tags": []}
{"fragment_id": "F_R18_p5_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p5:1", "text": "• N(t) represents the news information gathered by\nthe agent at time step t. These news data can be\nextracted from various sources such as social media\n(Twitter, Facebook, LinkedIn), the newspapers, spe-\nciﬁc journals, etc. Complex sentiment analysis mod-\nels could then be built to extract meaningful quanti-\ntative ﬁgures (quantity, sentiment polarity and sub-\njectivity, etc.) from the news. The beneﬁts of such\ninformation has already been demonstrated by sev-\neral authors, see e.g. Leinweber and Sisk (2011),\nBollen et al. (2011) and Nuij et al. (2014).\n• E(t) is any extra useful information at the disposal of\nthe trading agent at time stept, such as other market\nparticipants trading strategies, companies’ conﬁden-\ntial information, similar stock market behaviours,\nrumours, experts’ advice, etc.\nObservation space reduction:\nIn the scope of this research paper, it is assumed that\nthe only information considered by the RL agent is the\nclassical OHLCV data D(t) together with the state infor-\nmation S(t). Especially, the reduced observation space O\nencompasses the current trading position together with a\nseries of the previous τ+1 daily open-high-low-close prices\nand daily traded volume. With such an assumption, the\nreduced RL observation ot can be expressed as the follow-\ning:\not =\n{\n{pO\nt′ , pH\nt′ , pL\nt′ , pC\nt′ , Vt′ }t\nt′=t−τ, Pt\n}\n(5)", "tags": []}
{"fragment_id": "F_R18_p5_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p5:2", "text": "with Pt being the trading position of the RL agent at time\nstep t (either long or short, as explained in the next sub-\nsection of this research paper).\n3.4.2. RL actions\nAt each time stept, the RL agent executes a trading ac-\ntion at ∈A resulting from its policy π(at|ht). In fact, the\ntrading agent has to answer several questions: whether,\nhow and how much to trade? Such decisions can be mod-\nelled by the quantity of shares bought by the trading agent\nat time step t, represented by Qt ∈Z. Therefore, the RL\nactions can be expressed as the following:\nat = Qt (6)\nThree cases can occur depending on the value of Qt:\n• Qt >0: The RL agent buys shares on the stock mar-\nket, by posting new bid orders on the order book.\n• Qt <0: The RL agent sells shares on the stock mar-\nket, by posting new ask orders on the order book.\n• Qt = 0: The RL agent holds, meaning that it does\nnot buy nor sell any shares on the stock market.\nActually, the real actions occurring in the scope of a\ntrading activity are the orders posted on the order book.\nThe RL agent is assumed to communicate with an external\nmodule responsible for the synthesis of these true actions\naccording to the value of Qt: the trading execution system.\nDespite being out of the scope of this paper, it should be\nmentioned that multiple execution strategies can be con-\nsidered depending on the general trading purpose.", "tags": []}
{"fragment_id": "F_R18_p5_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p5:3", "text": "The trading actions have an impact on the two com-\nponents of the portfolio value, namely the cash and share\nvalues. Assuming that the trading actions occur close to\nthe market closure at price pt ≃pC\nt , the updates of these\ncomponents are governed by the following equations:\nvc\nt+1 = vc\nt −Qt pt (7)\nvs\nt+1 = (nt + Qt)  \nnt+1\npt+1 (8)\nwith nt ∈ Z being the number of shares owned by the\ntrading agent at time step t. In the scope of this research\npaper, negative values are allowed for this quantity. De-\nspite being surprising at ﬁrst glance, a negative number\nof shares simply corresponds to shares borrowed and sold,\nwith the obligation to repay the lender in shares in the\nfuture. Such a mechanism is particularly interesting as it\nintroduces new possibilities for the trading agent.\nTwo important constraints are assumed concerning the\nquantity of traded shares Qt. Firstly, contrarily to the\nshare value vs\nt which can be both positive or negative, the\ncash value vc\nt has to remain positive for every trading time\nsteps t. This constraint imposes an upper bound on the\nnumber of shares that the trading agent is capable of pur-\nchasing, this volume of shares being easily derived from\nEquation 7. Secondly, there exists a risk associated with\nthe impossibility to repay the share lender if the agent\nsuﬀers signiﬁcant losses. To prevent such a situation from", "tags": []}
{"fragment_id": "F_R18_p5_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p5:4", "text": "happening, the cash value vc\nt is constrained to be suﬃ-\nciently large when a negative number of shares is owned, in\norder to be able to get back to a neutral position ( nt = 0).\nA maximum relative change in prices, expressed in % and\ndenoted ϵ ∈ R+, is assumed by the RL agent prior to\nthe trading activity. This parameter corresponds to the\nmaximum market daily evolution supposed by the agent\nover the entire trading horizon, so that the trading agent\nshould always be capable of paying back the share lender\nas long as the market variation remains below this value.\nTherefore, the constraints acting upon the RL actions at\ntime step t can be mathematically expressed as follows:\nvc\nt+1 ≥0 (9)\nvc\nt+1 ≥−nt+1 pt (1 + ϵ) (10)\n5", "tags": []}
{"fragment_id": "F_R18_p6_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p6:1", "text": "with the following condition assumed to be satisﬁed:\n⏐⏐⏐⏐\npt+1 −pt\npt\n⏐⏐⏐⏐≤ϵ (11)\nTrading costs consideration:\nActually, the modelling represented by Equation 7 is\ninaccurate and will inevitably lead to unrealistic results.\nIndeed, whenever simulating trading activities, the trading\ncosts should not be neglected. Such omission is generally\nmisleading as a trading strategy, highly proﬁtable in simu-\nlations, may be likely to generate large losses in real trad-\ning situations due to these trading costs, especially when\nthe trading frequency is high. The trading costs can be\nsubdivided into two categories. On the one hand, there\nare explicit costs which are induced by transaction costs\nand taxes. On the other hand, there are implicit costs,\ncalled slippage costs, which are composed of three main\nelements and are associated to some of the dynamics of\nthe trading environment. The diﬀerent slippage costs are\ndetailed hereafter:\n• Spread costs:These costs are related to the diﬀer-\nence between the minimum ask price pask\nmin and the\nmaximum bid price pbid\nmax, called the spread. Because\nthe complete state of the order book is generally too\ncomplex to eﬃciently process or even not available,\nthe trading decisions are mostly based on the mid-\ndle price pmid = (pbid\nmax+pask\nmin)/2. However, a buying\n(selling) trade issued at pmid inevitably occurs at a\nprice p ≥pask\nmin (p ≤pbid", "tags": []}
{"fragment_id": "F_R18_p6_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p6:2", "text": "max). Such costs are all the\nmore signiﬁcant that the stock market liquidity is\nlow compared to the volume of shares traded.\n• Market impact costs:These costs are induced by\nthe impact of the trader’s actions on the market.\nEach trade (both buying and selling orders) is po-\ntentially capable of inﬂuencing the price. This phe-\nnomenon is all the more important that the stock\nmarket liquidity is low with respect to the volume of\nshares traded.\n• Timing costs: These costs are related to the time\nrequired for a trade to physically happen once the\ntrading decision is made, knowing that the market\nprice is continuously evolving. The ﬁrst cause is the\ninevitable latency which delays the posting of the\norders on the market order book. The second cause\nis the intentional delays generated by the trading\nexecution system. For instance, a large trade could\nbe split into multiple smaller trades spread over time\nin order to limit the market impact costs.\nAn accurate modelling of the trading costs is required\nto realistically reproduce the dynamics of the real trad-\ning environment. While explicit costs are relatively easy\nto take into account, the valid modelling of slippage costs\nis a truly complex task. In this research paper, the in-\ntegration of both costs into the RL environment is per-\nformed through a heuristic. When a trade is executed, a", "tags": []}
{"fragment_id": "F_R18_p6_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p6:3", "text": "certain amount of capital equivalent to a percentage C of\nthe amount of money invested is lost. This parameter was\nrealistically chosen equal to 0.1% in the forthcoming sim-\nulations.\nPractically, these trading costs are directly withdrawn\nfrom the trading agent cash. Following the heuristic pre-\nviously introduced, Equations 7 can be re-expressed with\na corrective term modelling the trading costs:\nvc\nt+1 = vc\nt −Qt pt − C |Qt|pt  \nTrading costs\n(12)\nMoreover, the trading costs have to be properly consid-\nered in the constraint expressed in Equation 10. Indeed,\nthe cash value vc\nt should be suﬃciently large to get back\nto a neutral position ( nt = 0) when the maximum mar-\nket variation ϵ occurs, the trading costs being included.\nConsequently, Equation 10 is re-expressed as follows:\nvc\nt+1 ≥−nt+1 pt (1 + ϵ)(1 + C) (13)\nEventually, the RL action spaceAcan be deﬁned as the\ndiscrete set of acceptable values for the quantity of traded\nshares Qt. Derived in detail in Appendix A, the RL action\nspace Ais mathematically expressed as the following:\nA= {Qt ∈Z ∩[Qt, Qt]} (14)\nwhere:\n• Qt = vc\nt\npt (1+C)\n• Qt =\n{ ∆t\npt ϵ(1+C) if ∆t ≥0\n∆t\npt (2C+ϵ(1+C)) if ∆t <0\nwith ∆t = −vc\nt −nt pt (1 + ϵ)(1 + C).\nAction space reduction:\nIn the scope of this scientiﬁc research paper, the ac-\ntion space Ais reduced in order to lower the complexity\nof the algorithmic trading problem. The reduced action", "tags": []}
{"fragment_id": "F_R18_p6_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p6:4", "text": "space is composed of only two RL actions which can be\nmathematically expressed as the following:\nat = Qt ∈{QLong\nt , QShort\nt } (15)\nThe ﬁrst RL action QLong\nt maximises the number of\nshares owned by the trading agent, by converting as much\ncash value vc\nt as possible into share value vs\nt. It can be\nmathematically expressed as follows:\nQLong\nt =\n{⌊\nvc\nt\npt (1+C)\n⌋\nif at−1 ̸= QLong\nt−1 ,\n0 otherwise.\n(16)\n6", "tags": []}
{"fragment_id": "F_R18_p7_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p7:1", "text": "The action QLong\nt is always valid as it is obviously in-\ncluded into the original action space Adeﬁned by Equa-\ntion 14. As a result of this action, the trading agent owns\na number of shares NLong\nt = nt+ QLong\nt . On the contrary,\nthe second RL action, designated byQShort\nt , converts share\nvalue vs\nt into cash value vc\nt, such that the RL agent owns\na number of shares equal to −NLong\nt . This operation can\nbe mathematically expressed as the following:\nˆQShort\nt =\n{\n−2nt −\n⌊\nvc\nt\npt (1+C)\n⌋\nif at−1 ̸= QShort\nt−1 ,\n0 otherwise.\n(17)\nHowever, the action ˆQShort\nt may violate the lower bound\nQt of the action space Awhen the price signiﬁcantly in-\ncreases over time. Eventually, the second RL actionQShort\nt\nis expressed as follows:\nQShort\nt = max\n{ˆQShort\nt , Qt\n}\n(18)\nTo conclude this subsection, it should be mentioned\nthat the two reduced RL actions are actually related to\nthe next trading position of the agent, designated as Pt+1.\nIndeed, the ﬁrst action QLong\nt induces a long trading po-\nsition because the number of owned shares is positive. On\nthe contrary, the second action QShort\nt always results in\na number of shares which is negative, which is generally\nreferred to as a short trading position in ﬁnance.\n3.4.3. RL rewards\nFor this algorithmic trading problem, a natural choice\nfor the RL rewards is the strategy daily returns. Intu-", "tags": []}
{"fragment_id": "F_R18_p7_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p7:2", "text": "itively, it makes sense to favour positive returns which are\nan evidence of a proﬁtable strategy. Moreover, such quan-\ntity has the advantage of being independent of the number\nof shares nt currently owned by the agent. This choice is\nalso motivated by the fact that it allows to avoid a sparse\nreward setup, which is more complex to deal with. The RL\nrewards can be mathematically expressed as the following:\nrt = vt+1 −vt\nvt\n(19)\n3.5. Objective\nObjectively assessing the performance of a trading strat-\negy is a tricky task, due to the numerous quantitative and\nqualitative factors to consider. Indeed, a well-performing\ntrading strategy is not simply expected to generate proﬁt,\nbut also to eﬃciently mitigate the risk associated with\nthe trading activity. The balance between these two goals\nvaries depending on the trading agent proﬁle and its will-\ningness to take extra risks. Although intuitively conve-\nnient, maximising the proﬁt generated by a trading strat-\negy is a necessary but not suﬃcient objective. Instead,\nthe core objective of a trading strategy is the maximisa-\ntion of the Sharpe ratio, a performance indicator widely\nused in the ﬁelds of ﬁnance and algorithmic trading. It\nis particularly well suited for the performance assessment\ntask as it considers both the generated proﬁt and the risk\nassociated with the trading activity. Mathematically, the", "tags": []}
{"fragment_id": "F_R18_p7_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p7:3", "text": "Sharpe ratio Sr is expressed as the following:\nSr = E[Rs −Rf]\nσr\n= E[Rs −Rf]√\nvar[Rs −Rf]\n≃ E[Rs]√\nvar[Rs]\n(20)\nwhere:\n• Rs is the trading strategy return over a certain time\nperiod, modelling its proﬁtability.\n• Rf is the risk-free return, the expected return from\na totally safe investment (negligible).\n• σr is the standard deviation of the trading strategy\nexcess return Rs −Rf, modelling its riskiness.\nIn order to compute the Sharpe ratioSr in practice, the\ndaily returns achieved by the trading strategy are ﬁrstly\ncomputed using the formula ρt = (vt −vt−1)/vt−1. Then,\nthe ratio between the returns mean and standard devia-\ntion is evaluated. Finally, the annualised Sharpe ratio is\nobtained by multiplying this value by the square root of\nthe number of trading days in a year (252).\nMoreover, a well-performing trading strategy should\nideally be capable of achieving acceptable performance on\ndiverse markets presenting very diﬀerent patterns. For in-\nstance, the trading strategy should properly handle both\nbull and bear markets (respectively strong increasing and\ndecreasing price trends), with diﬀerent levels of volatility.\nTherefore, the research paper’s core objective is the devel-\nopment of a novel trading strategy based on DRL tech-\nniques to maximise the average Sharpe ratio computed on\nthe entire set of existing stock markets.", "tags": []}
{"fragment_id": "F_R18_p7_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p7:4", "text": "Despite the fact that the ultimate objective is the max-\nimisation of the Sharpe ratio, the DRL algorithm adopted\nin this scientiﬁc paper actually maximises the expected\ndiscounted sum of rewards (daily returns) over an inﬁnite\ntime horizon. This optimisation criterion, which does not\nexactly corresponds to maximising proﬁts but is very close\nto that, can in fact be seen as a relaxation of the Sharpe ra-\ntio criterion. A future interesting research direction would\nbe to narrow the gap between these two objectives.\n4. Deep reinforcement learning algorithm design\nIn this section, a novel DRL algorithm is designed to\nsolve the algorithmic trading problem previously intro-\nduced. The resulting trading strategy, denominated the\nTrading Deep Q-Network algorithm (TDQN), is inspired\nfrom the successful DQN algorithm presented in Mnih\net al. (2013) and is signiﬁcantly adapted to the speciﬁc\ndecision-making problem at hand. Concerning the train-\ning of the RL agent, artiﬁcial trajectories are generated\nfrom a limited set of stock market historical data.\n7", "tags": []}
{"fragment_id": "F_R18_p8_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p8:1", "text": "Figure 3: Illustration of the DQN algorithm\n4.1. Deep Q-Network algorithm\nThe Deep Q-Network algorithm, generally referred to\nas DQN, is a DRL algorithm capable of successfully learn-\ning control policies from high-dimensional sensory inputs.\nIt is in a way the successor of the popular Q-learning algo-\nrithm introduced in Watkins and Dayan (1992). This DRL\nalgorithm is said to bemodel-free, meaning that a complete\nmodel of the environment is not required and that trajec-\ntories are suﬃcient. Belonging to the Q-learning family of\nalgorithms, it is based on the learning of an approximation\nof the state-action value function, which is represented by a\nDNN. In such context, learning the Q-function amounts to\nlearning the parameters θ of this DNN. Finally, the DQN\nalgorithm is said to be oﬀ-policy as it exploits in batch\nmode previous experiences et = (st,at,rt,st+1) collected\nat any point during training.\nFor the sake of brevity, the DQN algorithm is illus-\ntrated in Figure 3, but is not extensively presented in\nthis paper. Besides the original publications (Mnih et al.\n(2013) and Mnih et al. (2015)), there exists a great sci-\nentiﬁc literature around this algorithm, see for instance\nvan Hasselt et al. (2015), Wang et al. (2015), Schaul et al.\n(2016), Bellemare et al. (2017), Fortunato et al. (2018)\nand Hessel et al. (2017). Concerning DL techniques, inter-", "tags": []}
{"fragment_id": "F_R18_p8_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p8:2", "text": "esting resources are LeCun et al. (2015), Goodfellow et al.\n(2015) and Goodfellow et al. (2016). For more information\nabout RL, the reader can refer to the following textbooks\nand surveys: Sutton and Barto (2018), Szepesvari (2010),\nBusoniu et al. (2010), Arulkumaran et al. (2017) and Shao\net al. (2019).\n4.2. Artiﬁcial trajectories generation\nIn the scope of the algorithmic trading problem, a com-\nplete model of the environment E is not available. The\ntraining of the TDQN algorithm is entirely based on the\ngeneration of artiﬁcial trajectories from a limited set of\nstock market historical daily OHLCV data. A trajectory\nτ is deﬁned as a sequence of observations ot ∈O, actions\nat ∈ Aand rewards rt from an RL agent for a certain\nnumber T of trading time steps t:\nτ =\n(\n{o0,a0,r0},{o1,a1,r1},..., {oT,aT,rT}\n)\nInitially, although the environment Eis unknown, one\ndisposes of a single real trajectory, corresponding to the\nhistorical behaviour of the stock market, i.e. the particular\ncase of the RL agent being inactive. This original trajec-\ntory is composed of the historical prices and volumes to-\ngether with long actions executed by the RL agent with no\nmoney at its disposal, to represent the fact that no shares\nare actually traded. For this algorithmic trading problem,\nnew ﬁctive trajectories are then artiﬁcially generated from", "tags": []}
{"fragment_id": "F_R18_p8_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p8:3", "text": "this unique true trajectory to simulate interactions with\nthe environment E. The historical stock market behaviour\nis simply considered unaﬀected by the new actions per-\nformed by the trading agent. The artiﬁcial trajectories\ngenerated are simply composed of the sequence of histori-\ncal real observations associated with various sequences of\ntrading actions from the RL agent. For such practice to be\nscientiﬁcally acceptable and lead to realistic simulations,\nthe trading agent should not be able to inﬂuence the stock\nmarket behaviour. This assumption generally holds when\nthe number of shares traded by the trading agent is low\nwith respect to the liquidity of the stock market.\nIn addition to the generation of artiﬁcial trajectories\njust described, a trick is employed to slightly improve the\nexploration of the RL agent. It relies on the fact that the\nreduced action space Ais composed of only two actions:\nlong ( QLong\nt ) and short ( QShort\nt ). At each trading time\nstep t, the chosen action at is executed on the trading\nenvironment Eand the opposite action a−\nt is executed on a\ncopy of this environment E−. Although this trick does not\ncompletely solve the challenging exploration/exploitation\ntrade-oﬀ, it enables the RL agent to continuously explore\nat a small extra computational cost.\n4.3. Diverse modiﬁcations and improvements\nThe DQN algorithm was chosen as starting point for", "tags": []}
{"fragment_id": "F_R18_p8_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p8:4", "text": "the novel DRL trading strategy developed, but was signiﬁ-\ncantly adapted to the speciﬁc algorithmic trading decision-\nmaking problem at hand. The diverse modiﬁcations and\nimprovements, which are mainly based on the numerous\nsimulations performed, are summarised hereafter:\n• Deep neural network architecture:The ﬁrst dif-\nference with respect to the classical DQN algorithm\nis the architecture of the DNN approximating the\naction-value function Q(s,a). Due to the diﬀerent\nnature of the input (time-series instead of raw im-\nages), the convolutional neural network (CNN) has\nbeen replaced by a classical feedforward DNN with\nsome leaky rectiﬁed linear unit (Leaky ReLU) acti-\nvation functions.\n8", "tags": []}
{"fragment_id": "F_R18_p9_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p9:1", "text": "• Double DQN: The DQN algorithm suﬀers from\nsubstantial overestimations, this overoptimism harm-\ning the algorithm performance. In order to reduce\nthe impact of this undesired phenomenon, the article\nvan Hasselt et al. (2015) presents the double DQN\nalgorithm which is based on the decomposition of the\ntarget max operation into both action selection and\naction evaluation.\n• ADAM optimiser: The classical DQN algorithm\nimplements the RMSProp optimiser. However, the\nADAM optimiser, introduced in Kingma and Ba (2015),\nexperimentally proves to improve both the training\nstability and the convergence speed of the DRL al-\ngorithm.\n• Huber loss:While the classical DQN algorithm im-\nplements a mean squared error (MSE) loss, the Hu-\nber loss experimentally improves the stability of the\ntraining phase. Such observation is explained by the\nfact that the MSE loss signiﬁcantly penalises large\nerrors, which is generally desired but has a negative\nside-eﬀect for the DQN algorithm because the DNN\nis supposed to predict values that depend on its own\ninput. This DNN should not radically change in a\nsingle training update because this would also lead\nto a signiﬁcant change in the target, which could ac-\ntually result in a larger error. Ideally, the update of\nthe DNN should be performed in a slower and more\nstable manner. On the other hand, the mean ab-", "tags": []}
{"fragment_id": "F_R18_p9_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p9:2", "text": "solute error (MAE) has the drawback of not being\ndiﬀerentiable at 0. A good trade-oﬀ between these\ntwo losses is the Huber loss H:\nH(x) =\n{ 1\n2 x2 if |x|≤ 1,\n|x|− 1\n2 otherwise. (21)\nFigure 4: Comparison of the MSE, MAE and Huber losses\n• Gradient clipping: The gradient clipping tech-\nnique is implemented in the TDQN algorithm to\nsolve the gradient exploding problem which induces\nsigniﬁcant instabilities during the training of the DNN.\n• Xavier initialisation: While the classical DQN\nalgorithm simply initialises the DNN weights ran-\ndomly, the Xavier initialisation is implemented to\nimprove the algorithm convergence. The idea is to\nset the initial weights so that the gradients variance\nremains constant across the DNN layers.\n• Batch normalisation layers:This DL technique,\nintroduced by Ioﬀe and Szegedy (2015), consists in\nnormalising the input layer by adjusting and scaling\nthe activation functions. It brings many beneﬁts in-\ncluding a faster and more robust training phase as\nwell as an improved generalisation.\n• Regularisation techniques:Because a strong ten-\ndency to overﬁt was observed during the ﬁrst exper-\niments with the DRL trading strategy, three regu-\nlarisation techniques are implemented: Dropout, L2\nregularisation and Early Stopping.\n• Preprocessing and normalisation:The training\nloop of the TDQN algorithm is preceded by both a", "tags": []}
{"fragment_id": "F_R18_p9_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p9:3", "text": "preprocessing and a normalisation operation of the\nRL observationsot. Firstly, because the high-frequency\nnoise present in the trading data was experimen-\ntally observed to lower the algorithm generalisation,\na low-pass ﬁltering operation is executed. However,\nsuch a preprocessing operation has a cost as it mod-\niﬁes or even destroys some potentially useful trading\npatterns and introduces a non-negligible lag. Sec-\nondly, the resulting data are transformed in order to\nconvey more meaningful information about market\nmovements. Typically, the daily evolution of prices\nis considered rather than the raw prices. Thirdly,\nthe remaining data are normalised.\n• Data augmentation techniques:A key challenge\nof this algorithmic trading problem is the limited\namount of available data, which are in addition gen-\nerally of poor quality. As a counter to this ma-\njor problem, several data augmentation techniques\nare implemented: signal shifting, signal ﬁltering and\nartiﬁcial noise addition. The application of such\ndata augmentation techniques will artiﬁcially gen-\nerate new trading data which are slightly diﬀerent\nbut which result in the same ﬁnancial phenomena.\nFinally, the algorithm underneath the TDQN trading\nstrategy is depicted in detail in Algorithm 1.\n5. Performance assessment\nAn accurate performance evaluation approach is capi-\ntal in order to produce meaningful results. As previously", "tags": []}
{"fragment_id": "F_R18_p9_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p9:4", "text": "hinted, this procedure is all the more critical because there\nhas been a real lack of a proper performance assessment\nmethodology in the algorithmic trading ﬁeld. In this sec-\ntion, a novel, more reliable methodology is presented to\n9", "tags": []}
{"fragment_id": "F_R18_p10_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p10:1", "text": "Algorithm 1TDQN algorithm\nInitialise the experience replay memory M of capacity C.\nInitialise the main DNN weights θ (Xavier initialisation).\nInitialise the target DNN weights θ−= θ.\nfor episode = 1 to N do\nAcquire the initial observation o1 from the environment E and preprocess it.\nfor t = 1 to T do\nWith probability ϵ, select a random action at from A.\nOtherwise, select at = arg maxa∈AQ(ot,a; θ).\nCopy the environment E−= E.\nInteract with the environment E (action at) and get the new observation ot+1 and reward rt.\nPerform the same operation on E−with the opposite action a−\nt , getting o−\nt+1 and r−\nt .\nPreprocess both new observations ot+1 and o−\nt+1.\nStore both experiences et = (ot,at,rt,ot+1) and e−\nt = (ot,a−\nt ,r−\nt ,o−\nt+1) in M.\nif t % T’ = 0 then\nRandomly sample from M a minibatch of Ne experiences ei = (oi,ai,ri,oi+1).\nSet yi =\n{ ri if the state si+1 is terminal,\nri + γ Q(oi+1,arg maxa∈AQ(oi+1,a; θ); θ−) otherwise.\nCompute and clip the gradients based on the Huber loss H(yi, Q(oi,ai; θ)).\nOptimise the main DNN parameters θ based on these clipped gradients.\nUpdate the target DNN parameters θ−= θ every N−steps.\nend if\nAnneal the ϵ-Greedy exploration parameter ϵ.\nend for\nend for\nobjectively assess the performance of algorithmic trading\nstrategies, including the TDQN algorithm.\n5.1. Testbench\nIn the literature, the performance of a trading strategy", "tags": []}
{"fragment_id": "F_R18_p10_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p10:2", "text": "is generally assessed on a single instrument (stock mar-\nket or others) for a certain period of time. Nevertheless,\nthe analysis resulting from such a basic approach should\nnot be entirely trusted, as the trading data could have\nbeen speciﬁcally selected so that a trading strategy looks\nproﬁtable, even though it is not the case in general. To\neliminate such bias, the performance should ideally be as-\nsessed on multiple instruments presenting diverse patterns.\nAiming to produce trustful conclusions, this research pa-\nper proposes a testbench composed of 30 stocks presenting\ndiverse characteristics (sectors, regions, volatility, liquid-\nity, etc.). The testbench is depicted in Table 2. To avoid\nany confusion, the oﬃcial reference for each stock (ticker)\nis speciﬁed in parentheses. To avoid any ambiguities con-\ncerning the training and evaluation protocols, it should be\nmentioned that a new trading strategy is trained for each\nstock included in the testbench. Nevertheless, for the sake\nof generality, all the algorithm hyperparameters remain\nunchanged over the entire testbench.\nRegarding the trading horizon, the eight years preced-\ning the publication year of the research paper are selected\nto be representative of the current market conditions. Such\na short-time period could be criticised because it may be\ntoo limited to be representative of the entire set of ﬁnan-", "tags": []}
{"fragment_id": "F_R18_p10_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p10:3", "text": "cial phenomena. For instance, the ﬁnancial crisis of 2008 is\nrejected, even though it could be interesting to assess the\nrobustness of trading strategies with respect to such an\nextraordinary event. However, this choice was motivated\nby the fact that a shorter trading horizon is less likely to\ncontain signiﬁcant market regime shifts which would seri-\nously harm the training stability of the trading strategies.\nFinally, the trading horizon of eight years is divided into\nboth training and test sets as follows:\n• Training set:01/01/2012 →31/12/2017.\n• Test set:01/01/2018 →31/12/2019.\nA validation set is also considered as a subset of the\ntraining set for the tuning of the numerous TDQN algo-\nrithm hyperparameters. Note that the RL policy DNN\nparameters θ are ﬁxed during the execution of the trading\nstrategy on the entire test set, meaning that the new ex-\nperiences acquired are not valued for extra training. Nev-\nertheless, such practice constitutes an interesting future\nresearch direction.\nTo end this subsection, it should be noted that the pro-\nposed testbench could be improved thanks to even more\ndiversiﬁcation. The obvious addition would be to include\nmore stocks with diﬀerent ﬁnancial situations and prop-\nerties. Another interesting addition would be to consider\ndiﬀerent training/testing time periods while excluding the", "tags": []}
{"fragment_id": "F_R18_p10_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p10:4", "text": "signiﬁcant market regime shifts. Nevertheless, this last\nidea was discarded in this scientiﬁc article due to the im-\nportant time already required to produce results for the\nproposed testbench.\n10", "tags": []}
{"fragment_id": "F_R18_p11_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p11:1", "text": "Table 2: Performance assessment testbench\nSector Region\nAmerican European Asian\nTrading index\nDow Jones (DIA) FTSE 100 (EZU) Nikkei 225 (EWJ)\nS&P 500 (SPY)\nNASDAQ (QQQ)\nTechnology\nApple (AAPL) Nokia (NOK) Sony (6758.T)\nGoogle (GOOGL) Philips (PHIA.AS) Baidu (BIDU)\nAmazon (AMZN) Siemens (SIE.DE) Tencent (0700.HK)\nFacebook (FB) Alibaba (BABA)\nMicrosoft (MSFT)\nTwitter (TWTR)\nFinancial services JPMorgan Chase (JPM) HSBC (HSBC) CCB (0939.HK)\nEnergy ExxonMobil (XOM) Shell (RDSA.AS) PetroChina (PTR)\nAutomotive Tesla (TSLA) Volkswagen (VOW3.DE) Toyota (7203.T)\nFood Coca Cola (KO) AB InBev (ABI.BR) Kirin (2503.T)\n5.2. Benchmark trading strategies\nIn order to properly assess the strengths and weak-\nnesses of the TDQN algorithm, some benchmark algo-\nrithmic trading strategies were selected for comparison\npurposes. Only the classical trading strategies commonly\nused in practice were considered, excluding for instance\nstrategies based on DL techniques or other advanced ap-\nproaches. Despite the fact that the TDQN algorithm is\nan active trading strategy, both passive and active strate-\ngies are taken into consideration. For the sake of fairness,\nthe strategies share the same input and output spaces pre-\nsented in Section 3.4.2 (Oand A). The following list sum-\nmarises the benchmark strategies selected:\n• Buy and hold (B&H).\n• Sell and hold (S&H).\n• Trend following with moving averages (TF).", "tags": []}
{"fragment_id": "F_R18_p11_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p11:2", "text": "• Mean reversion with moving averages (MR).\nFor the sake of brevity, a detailed description of each\nstrategy is not provided in this research paper. The reader\ncan refer to Chan (2009), Chan (2013) or Narang (2009)\nfor more information. The ﬁrst two benchmark trading\nstrategies (B&H and S&H) are said to be passive, as there\nare no changes in trading position over the trading hori-\nzon. On the contrary, the other two benchmark strategies\n(TF and MR) are active trading strategies, issuing multi-\nple changes in trading positions over the trading horizon.\nOn the one hand, a trend following strategy is concerned\nwith the identiﬁcation and the follow-up of signiﬁcant mar-\nket trends, as depicted in Figure 5. On the other hand, a\nmean reversion strategy, illustrated in Figure 6, is based\non the tendency of a stock market to get back to its previ-\nous average price in the absence of clear trends. By design,\na trend following strategy generally makes a proﬁt when a\nmean reversion strategy does not, the opposite being true\nas well. This is due to the fact that these two families\nof trading strategies adopt opposite positions: a mean re-\nversion strategy always denies and goes against the trends\nwhile a trend following strategy follows the movements.\nFigure 5: Illustration of a typical trend following trading strategy\nFigure 6: Illustration of a typical mean reversion trading strategy", "tags": []}
{"fragment_id": "F_R18_p11_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p11:3", "text": "5.3. Quantitative performance assessment\nThe quantitative performance assessment consists in\ndeﬁning one performance indicator or more to numerically\nquantify the performance of an algorithmic trading strat-\negy. Because the core objective of a trading strategy is\nto be proﬁtable, its performance should be linked to the\namount of money earned. However, such reasoning omits\nto consider the risk associated with the trading activity\nwhich should be eﬃciently mitigated. Generally, a trading\nstrategy achieving a small but stable proﬁt is preferred to\n11", "tags": []}
{"fragment_id": "F_R18_p12_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p12:1", "text": "Table 3: Quantitative performance assessment indicators\nPerformance indicator Description\nSharpe ratio Return of the trading activity compared to its riskiness.\nProﬁt & loss Money gained or lost at the end of the trading activity.\nAnnualised return Annualised return generated during the trading activity.\nAnnualised volatility Modelling of the risk associated with the trading activity.\nProﬁtability ratio Percentage of winning trades made during the trading activity.\nProﬁt and loss ratio Ratio between the trading activity trades average proﬁt and loss.\nSortino ratio Similar to the Sharpe ratio with the negative risk penalised only.\nMaximum drawdown Largest loss from a peak to a trough during the trading activity.\nMaximum drawdown duration Time duration of the trading activity maximum drawdown.\na trading strategy achieving a huge proﬁt in a very unsta-\nble way after suﬀering from multiple losses. It eventually\ndepends on the investor proﬁle and the willingness to take\nextra risks to potentially earn more.\nMultiple performance indicators were selected to accu-\nrately assess the performance of a trading strategy. As\npreviously introduced in Section 3.5, the most important\none is certainly the Sharpe ratio. This performance in-\ndicator, widely used in the ﬁeld of algorithmic trading,\nis particularly informative as it combines both proﬁtabil-", "tags": []}
{"fragment_id": "F_R18_p12_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p12:2", "text": "ity and risk. Besides the Sharpe ratio, this research paper\nconsiders multiple other performance indicators to provide\nextra insights. Table 3 presents the entire set of perfor-\nmance indicators employed to quantify the performance of\na trading strategy.\nComplementarily to the computation of these numer-\nous performance indicators, it is interesting to graphically\nrepresent the trading strategy behaviour. Plotting both\nthe stock market price pt and portfolio value vt evolutions\ntogether with the trading actions at issued by the trading\nstrategy seems appropriate to accurately analyse the trad-\ning policy. Moreover, such visualisation could also provide\nextra insights about the performance, the strengths and\nweaknesses of the strategy analysed.\n6. Results and discussion\nIn this section, the TDQN trading strategy is evalu-\nated following the performance assessment methodology\npreviously described. Firstly, a detailed analysis is per-\nformed for both a case that give good results and a case\nfor which the results were mitigated. This highlights the\nstrengths, weaknesses and limitations of the TDQN algo-\nrithm. Secondly, the performance achieved by the DRL\ntrading strategy on the entire testbench is summarised\nand analysed. Finally, some additional discussions about\nthe discount factor parameter, the trading costs inﬂuence\nand the main challenges faced by the TDQN algorithm", "tags": []}
{"fragment_id": "F_R18_p12_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p12:3", "text": "are provided. The experimental code supporting the re-\nsults presented is publicly available at the following link:\nhttps://github.com/ThibautTheate/An-Applicat\nion-of-Deep-Reinforcement-Learning-to-Algorith\nmic-Trading.\n6.1. Good results - Apple stock\nThe ﬁrst detailed analysis concerns the execution of the\nTDQN trading strategy on the Apple stock, resulting in\npromising results. Similar to many DRL algorithms, the\nTDQN algorithm is subject to a non-negligible variance.\nMultiple training experiments with the exact same initial\nconditions will inevitably lead to slightly diﬀerent trading\nstrategies of varying performance. As a consequence, both\na typical run of the TDQN algorithm and its expected per-\nformance are presented hereafter.\nTypical run: Firstly, Table 4 presents the perfor-\nmance achieved by each trading strategy considered, the\ninitial amount of money being equal to $100,000. The\nTDQN algorithm achieves good results from both an earn-\nings and a risk mitigation point of view, clearly outper-\nforming all the benchmark active and passive trading strate-\ngies. Secondly, Figure 7 plots both the stock market price\npt and RL agent portfolio value vt evolutions, together\nwith the actions at outputted by the TDQN algorithm. It\ncan be observed that the DRL trading strategy is capable\nof accurately detecting and beneﬁting from major trends,", "tags": []}
{"fragment_id": "F_R18_p12_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p12:4", "text": "while being more hesitant during market behavioural shifts\nwhen the volatility increases. It can also be seen that\nthe trading agent generally lags slightly behind the mar-\nket trends, meaning that the TDQN algorithm learned to\nbe more reactive than proactive for this particular stock.\nThis behaviour is expected with such a limited observation\nspace Onot including the reasons for the future market\ndirections (new product announcement, ﬁnancial report,\nmacroeconomics, etc.). However, this does not mean that\nthe policies learned are purely reactive. Indeed, it was ob-\nserved that the RL agent may decide to adapt its trading\nposition before a trend inversion by noticing an increase\nin volatility, therefore anticipating and being proactive.\nExpected performance:In order to estimate the ex-\npected performance as well as the variance of the TDQN\nalgorithm, the same RL trading agent is trained multiple\n12", "tags": []}
{"fragment_id": "F_R18_p13_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p13:1", "text": "Table 4: Performance assessment for the Apple stock\nPerformance indicator B&H S&H TF MR TDQN\nSharpe ratio 1.239 -1.593 1.178 -0.609 1.484\nProﬁt & loss [ $] 79823 -80023 68738 -34630 100288\nAnnualised return [%] 28.86 -100.00 25.97 -19.09 32.81\nAnnualised volatility [%] 26.62 44.39 24.86 28.33 25.69\nProﬁtability ratio [%] 100 0.00 42.31 56.67 52.17\nProﬁt and loss ratio ∞ 0.00 3.182 0.492 2.958\nSortino ratio 1.558 -2.203 1.802 -0.812 1.841\nMax drawdown [%] 38.51 82.48 14.89 51.12 17.31\nMax drawdown duration [days] 62 250 20 204 25\nFigure 7: TDQN algorithm execution for the Apple stock (test set)\ntimes. Figure 8 plots the averaged (over 50 iterations) per-\nformance of the TDQN algorithm for both the training and\ntest sets with respect to the number of training episodes.\nThis expected performance is comparable to the perfor-\nmance achieved during the typical run of the algorithm.\nIt can also be noticed that the overﬁtting tendency of the\nRL agent seems to be properly handled for this speciﬁc\nmarket. Please note that the test set performance being\ntemporarily superior to the training set performance is not\na mistake. It simply indicates an easier to trade and more\nproﬁtable market for the test set trading period for the\nApple stock. This example perfectly illustrates a major\ndiﬃculty of the algorithmic trading problem: the train-", "tags": []}
{"fragment_id": "F_R18_p13_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p13:2", "text": "ing and test sets do not share the same distributions. In-\ndeed, the distribution of the daily returns is continuously\nchanging, which complicates both the training of the DRL\ntrading strategy and its performance evaluation.\n6.2. Mitigated results - Tesla stock\nThe same detailed analysis is performed on the Tesla\nstock, which presents very diﬀerent characteristics com-\npared to the Apple stock, such as a pronounced volatility.\nIn contrast to the promising performance achieved on the\nprevious stock, this case was speciﬁcally selected to high-\nlight the limitations of the TDQN algorithm.\nFigure 8: TDQN algorithm expected performance for the Apple stock\nTypical run: Similar to the previous analysis, Ta-\nble 5 presents the performance achieved by every trad-\ning strategies considered, the initial amount of money be-\ning equal to $100,000. The mitigated results achieved by\nthe benchmark active strategies suggest that the Tesla\nstock is quite diﬃcult to trade, which is partly due to its\nsigniﬁcant volatility. Even though the TDQN algorithm\nachieves a positive Sharpe ratio, almost no proﬁt is gen-\nerated. Moreover, the risk level associated with this trad-\ning activity cannot really be considered acceptable. For\ninstance, the maximum drawdown duration is particularly\nlarge, which would result in a stressful situation for the op-", "tags": []}
{"fragment_id": "F_R18_p13_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p13:3", "text": "erator responsible for the trading strategy. Figure 9, which\nplots both the stock market price pt and RL agent port-\nfolio value vt evolutions together with the actions at out-\nputted by the TDQN algorithm, conﬁrms this observation.\nMoreover, it can be clearly observed that the pronounced\nvolatility of the Tesla stock induces a higher trading fre-\nquency (changes in trading positions, which correspond to\nthe situation where at ̸= at−1) despite the non-negligible\ntrading costs, which increases even more the riskiness of\nthe DRL trading strategy.\n13", "tags": []}
{"fragment_id": "F_R18_p14_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p14:1", "text": "Table 5: Performance assessment for the Tesla stock\nPerformance indicator B&H S&H TF MR TDQN\nSharpe ratio 0.508 -0.154 -0.987 0.358 0.261\nProﬁt & loss [ $] 29847 -29847 -73301 8600 98\nAnnualised return [%] 24.11 -7.38 -100.00 19.02 12.80\nAnnualised volatility [%] 53.14 46.11 52.70 58.05 52.09\nProﬁtability ratio [%] 100 0.00 34.38 67.65 38.18\nProﬁt and loss ratio ∞ 0.00 0.534 0.496 1.621\nSortino ratio 0.741 -0.205 -1.229 0.539 0.359\nMax drawdown [%] 52.83 54.09 79.91 65.31 58.95\nMax drawdown duration [days] 205 144 229 159 331\nFigure 9: TDQN algorithm execution for the Tesla stock (test set)\nExpected performance:Figure 10 plots the expected\nperformance of the TDQN algorithm for both the train-\ning and test sets as a function of the number of training\nepisodes (over 50 iterations). It can be directly noticed\nthat this expected performance is signiﬁcantly better than\nthe performance achieved by the typical run previously\nanalysed, which can therefore be considered as not really\nrepresentative of the average behaviour. This highlights\na key limitation of the TDQN algorithm: the substantial\nvariance which may result in selecting poorly performing\npolicies compared to the expected performance. The sig-\nniﬁcantly higher performance achieved on the training set\nalso suggests that the DRL algorithm is subject to overﬁt-\nting in this speciﬁc case, despite the multiple regularisation", "tags": []}
{"fragment_id": "F_R18_p14_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p14:2", "text": "techniques implemented. This overﬁtting phenomenon can\nbe partially explained by the observation space Owhich is\ntoo limited to eﬃciently apprehend the Tesla stock. Even\nthough this overﬁtting phenomenon does not seem to be\ntoo harmful in this particular case, it may lead to poor\nperformance for other stocks.\n6.3. Global results - Testbench\nAs previously suggested in this research paper, the\nTDQN algorithm is evaluated on the testbench introduced\nFigure 10: TDQN algorithm expected performance for the Tesla\nstock\nin Section 5.1, in order to draw more robust and trust-\nful conclusions. Table 6 presents the expected Sharpe ra-\ntio achieved by both the TDQN and benchmark trading\nstrategies on the entire set of stocks included in this test-\nbench.\nRegarding the performance achieved by the benchmark\ntrading strategies, it is important to diﬀerentiate the pas-\nsive strategies (B&H and S&H) from the active ones (TF\nand MR). Indeed, this second family of trading strategies\nhas more potential at the cost of an extra non-negligible\nrisk: continuous speculation. Because the stock markets\nwere mostly bullish (price pt mainly increasing over time)\nwith some instabilities during the test set trading period,\nit is not surprising to see the buy and hold strategy outper-\nforming the other benchmark trading strategies. In fact,\nneither the trend following nor the mean reversion strat-", "tags": []}
{"fragment_id": "F_R18_p14_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p14:3", "text": "egy managed to generate satisfying results on average on\nthis testbench. It clearly indicates that there is a major\ndiﬃculty to actively trade in such market conditions. This\npoorer performance can also be explained by the fact that\nsuch strategies are generally well suited to exploit speciﬁc\nﬁnancial patterns, but they lack versatility and thus of-\nten fail to achieve good average performance on a large\n14", "tags": []}
{"fragment_id": "F_R18_p15_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p15:1", "text": "Table 6: Performance assessment for the entire testbench\nStock Sharpe Ratio\nB&H S&H TF MR TDQN\nDow Jones (DIA) 0.684 -0.636 -0.325 -0.214 0.684\nS&P 500 (SPY) 0.834 -0.833 -0.309 -0.376 0.834\nNASDAQ 100 (QQQ) 0.845 -0.806 0.264 0.060 0.845\nFTSE 100 (EZU) 0.088 0.026 -0.404 -0.030 0.103\nNikkei 225 (EWJ) 0.128 -0.025 -1.649 0.418 0.019\nGoogle (GOOGL) 0.570 -0.370 0.125 0.555 0.227\nApple (AAPL) 1.239 -1.593 1.178 -0.609 1.424\nFacebook (FB) 0.371 -0.078 0.248 -0.168 0.151\nAmazon (AMZN) 0.559 -0.187 0.161 -1.193 0.419\nMicrosoft (MSFT) 1.364 -1.390 -0.041 -0.416 0.987\nTwitter (TWTR) 0.189 0.314 -0.271 -0.422 0.238\nNokia (NOK) -0.408 0.565 1.088 1.314 -0.094\nPhilips (PHIA.AS) 1.062 -0.672 -0.167 -0.599 0.675\nSiemens (SIE.DE) 0.399 -0.265 0.525 0.526 0.426\nBaidu (BIDU) -0.699 0.866 -1.209 0.167 0.080\nAlibaba (BABA) 0.357 -0.139 -0.068 0.293 0.021\nTencent (0700.HK) -0.013 0.309 0.179 -0.466 -0.198\nSony (6758.T) 0.794 -0.655 -0.352 0.415 0.424\nJPMorgan Chase (JPM) 0.713 -0.743 -1.325 -0.004 0.722\nHSBC (HSBC) -0.518 0.725 -1.061 0.447 0.011\nCCB (0939.HK) 0.026 0.165 -1.163 -0.388 0.202\nExxonMobil (XOM) 0.055 0.132 -0.386 -0.673 0.098\nShell (RDSA.AS) 0.488 -0.238 -0.043 0.742 0.425\nPetroChina (PTR) -0.376 0.514 -0.821 -0.238 0.156\nTesla (TSLA) 0.508 -0.154 -0.987 0.358 0.621\nVolkswagen (VOW3.DE) 0.384 -0.208 -0.361 0.601 0.216\nToyota (7203.T) 0.352 -0.242 -1.108 -0.378 0.304", "tags": []}
{"fragment_id": "F_R18_p15_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p15:2", "text": "Coca Cola (KO) 1.031 -0.871 -0.236 -0.394 1.068\nAB InBev (ABI.BR) -0.058 0.275 0.036 -1.313 0.187\nKirin (2503.T) 0.106 0.156 -1.441 0.313 0.852\nAverage 0.369 -0.202 -0.331 -0.056 0.404\n15", "tags": []}
{"fragment_id": "F_R18_p16_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p16:1", "text": "set of stocks presenting diverse characteristics. Moreover,\nsuch strategies are generally more impacted by the trad-\ning costs due their higher trading frequency (for relatively\nshort moving averages durations, as it is the case in this\nresearch paper).\nConcerning the innovative trading strategy, the TDQN\nalgorithm achieves promising results on the testbench, out-\nperforming the benchmark active trading strategies on av-\nerage. Nevertheless, the DRL trading strategy only barely\nsurpasses the buy and hold strategy on these particular\nbullish markets which are so favourable to this simple pas-\nsive strategy. Interestingly, it should be noted that the\nperformance of the TDQN algorithm is identical or very\nclose to the performance of the passive trading strategies\n(B&H and S&H) for multiple stocks. This is explained by\nthe fact that the DRL strategy eﬃciently learns to tend\ntoward a passive trading strategy when the uncertainty\nassociated to active trading increases. It should also be\nemphasized that the TDQN algorithm is neither a trend\nfollowing nor a mean reversion trading strategy as both\nﬁnancial patterns can be eﬃciently handled in practice.\nThus, the main advantage of the DRL trading strategy is\ncertainly its versatility and its ability to eﬃciently handle\nvarious markets presenting diverse characteristics.\n6.4. Discount factor discussion", "tags": []}
{"fragment_id": "F_R18_p16_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p16:2", "text": "As previously explained in Section 3.4, the discount\nfactor γ is concerned with the importance of future re-\nwards. In the scope of this algorithmic trading problem,\nthe proper tuning of this parameter is not trivial due to\nthe signiﬁcant uncertainty of the future. On the one hand,\nthe desired trading policy should be long-term oriented\n(γ →1), in order to avoid a too high trading frequency\nand being exposed to considerable trading costs. On the\nother hand, it would be unwise to place too much im-\nportance on a stock market future which is particularly\nuncertain (γ →0). Therefore, a trade-oﬀ intuitively exists\nfor the discount factor parameter.\nThis reasoning is validated by the multiple experiments\nperformed to tune the parameter γ. Indeed, it was ob-\nserved that there is an optimal value for the discount\nfactor, which is neither too small nor too large. Addi-\ntionally, these experiments highlighted the hidden link be-\ntween the discount factor and the trading frequency, due\nto the trading costs. From the point of view of the RL\nagent, these costs represent an obstacle to overcome for a\nchange in trading position to occur, due to the immediate\nreduced (and often negative) reward received. It models\nthe fact that the trading agent should be suﬃciently con-\nﬁdent about the future in order to overcome the extra risk\nassociated with the trading costs. The discount factor de-", "tags": []}
{"fragment_id": "F_R18_p16_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p16:3", "text": "termining the importance assigned to the future, a small\nvalue for the parameter γ will inevitably reduce the ten-\ndency of the RL agent to change its trading position, which\ndecreases the trading frequency of the TDQN algorithm.\n6.5. Trading costs discussion\nThe analysis of the trading costs inﬂuence on a trading\nstrategy behaviour and performance is capital, due to the\nfact that such costs represent an extra risk to mitigate. A\nmajor motivation for studying DRL solutions rather than\npure prediction techniques that could also be based on\nDL architectures is related to the trading costs. As pre-\nviously explained in Section 3, the RL formalism enables\nthe consideration of these additional costs directly into the\ndecision-making process. The optimal policy is learned\naccording to the trading costs value. On the contrary,\na purely predictive approach would only output predic-\ntions about the future market direction or prices without\nany indications regarding an appropriate trading strategy\ntaking into account the trading costs. Although this last\napproach oﬀers more ﬂexibility and could certainly lead\nto well-performing trading strategies, it is less eﬃcient by\ndesign.\nIn order to illustrate the ability of the TDQN algo-\nrithm to automatically and eﬃciently adapt to diﬀerent\ntrading costs, Figure 11 presents the behaviour of the DRL", "tags": []}
{"fragment_id": "F_R18_p16_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p16:4", "text": "trading strategy for three diﬀerent costs values, all other\nparameters remaining unchanged. It can clearly be ob-\nserved that the TDQN algorithm eﬀectively reduces its\ntrading frequency when the trading costs increase, as ex-\npected. When these costs become too high, the DRL algo-\nrithm simply stops actively trading and adopts a passive\napproach (buy and hold or sell and hold strategies).\n6.6. Core challenges\nNowadays, the main DRL solutions successfully ap-\nplied to real-life problems concern speciﬁc environments\nwith particular properties such as games (see e.g. the fa-\nmous AlphaGo algorithm developed by Google Deepmind\nSilver et al. (2016)). In this research paper, an entirely\ndiﬀerent environment characterised by a signiﬁcant com-\nplexity and a considerable uncertainty is studied with the\nalgorithmic trading problem. Obviously, multiple chal-\nlenges were faced during the research around the TDQN\nalgorithm, the major ones being summarised hereafter.\nFirstly, the extremely poor observability of the trad-\ning environment is a characteristic that signiﬁcantly lim-\nits the performance of the TDQN algorithm. Indeed, the\namount of information at the disposal of the RL agent\nis really not suﬃcient to accurately explain the ﬁnancial\nphenomena occurring during training, which is necessary\nto eﬃciently learn to trade. Secondly, although the distri-", "tags": []}
{"fragment_id": "F_R18_p16_5", "source_id": "R18", "locator": "2004.06627v3.pdf:p16:5", "text": "bution of the daily returns is continuously changing, the\npast is required to be representative enough of the future\nfor the TDQN algorithm to achieve good results. This\nmakes the DRL trading strategy particularly sensitive to\nsigniﬁcant market regime shifts. Thirdly, the TDQN al-\ngorithm overﬁtting tendency has to be properly handled\nin order to obtain a reliable trading strategy. As sug-\ngested in Zhang et al. (2018), more rigorous evaluation\n16", "tags": []}
{"fragment_id": "F_R18_p17_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p17:1", "text": "(a) Trading costs: 0%\n (b) Trading costs: 0.1%\n (c) Trading costs: 0.2%\nFigure 11: Impact of the trading costs on the TDQN algorithm, for the Apple stock\nprotocols are required in RL due to the strong tendency\nof common DRL techniques to overﬁt. More research on\nthis particular topic is required for DRL techniques to ﬁt\na broader range of real-life applications. Lastly, the sub-\nstantial variance of DRL algorithms such as DQN makes\nit rather diﬃcult to successfully apply these algorithms to\ncertain problems, especially when the training and test sets\ndiﬀer considerably. This is a key limitation of the TDQN\nalgorithm which was previously highlighted for the Tesla\nstock.\n7. Conclusion\nThis scientiﬁc research paper presents the Trading Deep\nQ-Network algorithm (TDQN), a deep reinforcement learn-\ning (DRL) solution to the algorithmic trading problem\nof determining the optimal trading position at any point\nin time during a trading activity in stock markets. Fol-\nlowing a rigorous performance assessment, this innova-\ntive trading strategy achieves promising results, surpassing\non average the benchmark trading strategies. Moreover,\nthe TDQN algorithm demonstrates multiple beneﬁts com-\npared to more classical approaches, such as an appreciable\nversatility and a remarkable robustness to diverse trading\ncosts. Additionally, such data-driven approach presents", "tags": []}
{"fragment_id": "F_R18_p17_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p17:2", "text": "the major advantage of suppressing the complex task of\ndeﬁning explicit rules suited to the particular ﬁnancial\nmarkets considered.\nNevertheless, the performance of the TDQN algorithm\ncould still be improved, from both a generalisation and a\nreproducibility point of view, to cite a few. Several re-\nsearch directions are suggested to upgrade the DRL solu-\ntion, such as the use of LSTM layers into the deep neural\nnetwork which should help to better process the ﬁnancial\ntime-series data, see e.g. Hausknecht and Stone (2015).\nAnother example is the consideration of the numerous im-\nprovements implemented in the Rainbow algorithm, which\nare detailed in Sutton and Barto (2018), van Hasselt et al.\n(2015), Wang et al. (2015), Schaul et al. (2016), Bellemare\net al. (2017), Fortunato et al. (2018) and Hessel et al.\n(2017). Another interesting research direction is the com-\nparison of the TDQN algorithm with Policy Optimisation\nDRL algorithms such as the Proximal Policy Optimisation\n(PPO - Schulman et al. (2017)) algorithm.\nThe last major research direction suggested concerns\nthe formalisation of the algorithmic trading problem into a\nreinforcement learning one. Firstly, the observation space\nOshould be extended to enhance the observability of the\ntrading environment. Similarly, some constraints about\nthe action space Acould be relaxed in order to enable", "tags": []}
{"fragment_id": "F_R18_p17_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p17:3", "text": "new trading possibilities. Secondly, advanced RL reward\nengineering should be performed to narrow the gap be-\ntween the RL objective and the Sharpe ratio maximisation\nobjective. Finally, an interesting and promising research\ndirection is the consideration of distributions instead of ex-\npected values in the TDQN algorithm in order to encom-\npass the notion of risk and to better handle uncertainty.\nAcknowledgements\nThibaut Th´ eate is a Research Fellow of the F.R.S.-\nFNRS, of which he acknowledges the ﬁnancial support.\nReferences\nAr´ evalo, A., Ni˜ no, J., Hern´ andez, G., and Sandoval, J. (2016). High-\nFrequency Trading Strategy Based on Deep Neural Networks.\nICIC.\nArulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath,\nA. A. (2017). A Brief Survey of Deep Reinforcement Learning.\nCoRR, abs/1708.05866.\nBailey, D. H., Borwein, J. M., de Prado, M. L., and Zhu, Q. J. (2014).\nPseudo-Mathematics and Financial Charlatanism: The Eﬀects of\nBacktest Overﬁtting on Out-of-Sample Performance. Notice of\nthe American Mathematical Society, pages 458–471.\nBao, W. N., Yue, J., and Rao, Y. (2017). A Deep Learning Frame-\nwork for Financial Time Series using Stacked Autoencoders and\nLong-Short Term Memory. PloS one, 12.\nBellemare, M. G., Dabney, W., and Munos, R. (2017). A Dis-\ntributional Perspective on Reinforcement Learning. CoRR,\nabs/1707.06887.", "tags": []}
{"fragment_id": "F_R18_p17_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p17:4", "text": "Bollen, J., Mao, H., and jun Zeng, X. (2011). Twitter Mood Predicts\nthe Stock Market. J. Comput. Science , 2:1–8.\n17", "tags": []}
{"fragment_id": "F_R18_p18_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p18:1", "text": "Boukas, I., Ernst, D., Th´ eate, T., Bolland, A., Huynen, A., Buch-\nwald, M., Wynants, C., and Corn´ elusse, B. (2020). A Deep Rein-\nforcement Learning Framework for Continuous Intraday Market\nBidding. ArXiv, abs/2004.05940.\nBusoniu, L., Babuska, R., De Schutter, B., and Ernst, D. (2010). Re-\ninforcement Learning and Dynamic Programming using Function\nApproximators. CRC Press.\nCarapu¸ co, J., Neves, R. F., and Horta, N. (2018). Reinforcement\nLearning applied to Forex Trading. Appl. Soft Comput. , 73:783–\n794.\nChan, E. P. (2009). Quantitative Trading: How to Build Your Own\nAlgorithmic Trading Business. Wiley.\nChan, E. P. (2013). Algorithmic Trading: Winning Strategies and\nTheir Rationale. Wiley.\nDempster, M. A. H. and Leemans, V. (2006). An Automated FX\nTrading System using Adaptive Reinforcement Learning. Expert\nSyst. Appl., 30:543–552.\nDeng, Y., Bao, F., Kong, Y., Ren, Z., and Dai, Q. (2017). Deep\nDirect Reinforcement Learning for Financial Signal Representa-\ntion and Trading. IEEE Transactions on Neural Networks and\nLearning Systems, 28:653–664.\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Hessel, M., Osband,\nI., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O.,\nBlundell, C., and Legg, S. (2018). Noisy Networks for Exploration.\nCoRR, abs/1706.10295.\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning.\nMIT Press.", "tags": []}
{"fragment_id": "F_R18_p18_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p18:2", "text": "Goodfellow, I. J., Bengio, Y., and Courville, A. C. (2015). Deep\nLearning. Nature, 521:436–444.\nHausknecht, M. J. and Stone, P. (2015). Deep Recurrent Q-Learning\nfor Partially Observable MDPs. CoRR, abs/1507.06527.\nHendershott, T., Jones, C. M., and Menkveld, A. J. (2011). Does\nAlgorithmic Trading Improve Liquidity? Journal of Finance ,\n66:1–33.\nHessel, M., Modayil, J., van Hasselt, H. P., Schaul, T., Ostrovski,\nG., Dabney, W., Horgan, D., Piot, B., Azar, M. G., and Silver, D.\n(2017). Rainbow: Combining Improvements in Deep Reinforce-\nment Learning. CoRR, abs/1710.02298.\nIoannidis, J. P. A. (2005). Why Most Published Research Findings\nAre False. PLoS Med, 2:124.\nIoﬀe, S. and Szegedy, C. (2015). Batch Normalization: Accelerat-\ning Deep Network Training by Reducing Internal Covariate Shift.\nCoRR, abs/1502.03167.\nKingma, D. P. and Ba, J. (2015). Adam: A Method for Stochastic\nOptimization. CoRR, abs/1412.6980.\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep Learning. Na-\nture, 521.\nLeinweber, D. and Sisk, J. (2011). Event-Driven Trading and the\n“New News”. The Journal of Portfolio Management , 38:110–124.\nLi, Y. (2017). Deep Reinforcement Learning: An Overview. CoRR,\nabs/1701.07274.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,\nWierstra, D., and Riedmiller, M. A. (2013). Playing Atari with\nDeep Reinforcement Learning. CoRR, abs/1312.5602.", "tags": []}
{"fragment_id": "F_R18_p18_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p18:3", "text": "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\nBellemare, M. G., Graves, A., Riedmiller, M. A., Fidjeland, A.,\nOstrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou,\nI., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis,\nD. (2015). Human-Level Control through Deep Reinforcement\nLearning. Nature, 518:529–533.\nMoody, J. E. and Saﬀell, M. (2001). Learning to Trade via Direct\nReinforcement. IEEE transactions on neural networks , 12 4:875–\n89.\nNarang, R. K. (2009). Inside the Black Box . Wiley.\nNuij, W., Milea, V., Hogenboom, F., Frasincar, F., and Kaymak, U.\n(2014). An Automated Framework for Incorporating News into\nStock Trading Strategies. IEEE Transactions on Knowledge and\nData Engineering, 26:823–835.\nNuti, G., Mirghaemi, M., Treleaven, P. C., and Yingsaeree, C.\n(2011). Algorithmic Trading. Computer, 44:61–69.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Priori-\ntized Experience Replay. CoRR, abs/1511.05952.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov,\nO. (2017). Proximal Policy Optimization Algorithms. CoRR,\nabs/1707.06347.\nShao, K., Tang, Z., Zhu, Y., Li, N., and Zhao, D. (2019). A Sur-\nvey of Deep Reinforcement Learning in Video Games. ArXiv,\nabs/1912.10944.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den\nDriessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,", "tags": []}
{"fragment_id": "F_R18_p18_4", "source_id": "R18", "locator": "2004.06627v3.pdf:p18:4", "text": "V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbren-\nner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu,\nK., Graepel, T., and Hassabis, D. (2016). Mastering the Game of\nGo with Deep Neural Networks and Tree Search.Nature, 529:484–\n489.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An\nIntroduction. The MIT Press, second edition.\nSzepesvari, C. (2010). Algorithms for Reinforcement Learning. Mor-\ngan and Claypool Publishers.\nTreleaven, P. C., Galas, M., and Lalchand, V. (2013). Algorithmic\nTrading Review. Commun. ACM, 56:76–85.\nvan Hasselt, H. P., Guez, A., and Silver, D. (2015). Deep Reinforce-\nment Learning with Double Q-Learning. CoRR, abs/1509.06461.\nWang, Z., de Freitas, N., and Lanctot, M. (2015). Dueling Net-\nwork Architectures for Deep Reinforcement Learning. CoRR,\nabs/1511.06581.\nWatkins, C. J. C. H. and Dayan, P. (1992). Technical Note: Q-\nLearning. Machine Learning, 8:279–292.\nZhang, C., Vinyals, O., Munos, R., and Bengio, S. (2018). A\nStudy on Overﬁtting in Deep Reinforcement Learning. CoRR,\nabs/1804.06893.\n18", "tags": []}
{"fragment_id": "F_R18_p19_1", "source_id": "R18", "locator": "2004.06627v3.pdf:p19:1", "text": "Appendix A. Derivation of action spaceA\nTheorem 1. The RL action space A admits an upper\nbound Qt such that:\nQt = vc\nt\npt (1 + C)\nProof. The upper bound of the RL action space Ais de-\nrived from the fact that the cash value vc\nt has to remain\npositive over the entire trading horizon (Equation 9). Mak-\ning the hypothesis that vc\nt ≥0, the number of shares Qt\ntraded by the RL agent at time step t has to be set such\nthat vc\nt+1 ≥ 0 as well. Introducing this condition into\nEquation 12 expressing the update of the cash value, the\nfollowing expression is obtained:\nvc\nt −Qt pt −C |Qt|pt ≥0\nTwo cases arise depending on the value of Qt:\nCase of Qt <0: The previous expression becomes\nvc\nt −Qt pt + C Qt pt ≥0.\n⇔Qt ≤ vc\nt\npt (1−C) .\nThe expression on the right side of the inequality is al-\nways positive due to the hypothesis that vc\nt ≥0. Because\nQt is negative in this case, the condition is always satisﬁed.\nCase of Qt ≥0: The previous expression becomes\nvc\nt −Qt pt −C Qt pt ≥0.\n⇔Qt ≤ vc\nt\npt (1+C) .\nThis condition represents the upper bound (positive) of\nthe RL action space A.\nTheorem 2.The RL action space Aadmits a lower bound\nQt such that:\nQt =\n{ ∆t\npt ϵ(1+C) if ∆t ≥0\n∆t\npt (2C+ϵ(1+C)) if ∆t <0\nwith ∆t = −vc\nt −nt pt (1 + ϵ)(1 + C).\nProof. The lower bound of the RL action space Ais de-\nrived from the fact that the cash value vc\nt has to be suﬃ-", "tags": []}
{"fragment_id": "F_R18_p19_2", "source_id": "R18", "locator": "2004.06627v3.pdf:p19:2", "text": "cient to get back to a neutral position (nt = 0) over the en-\ntire trading horizon (Equation 13). Making the hypothesis\nthat this condition is satisﬁed at time step t, the number\nof shares Qt traded by the RL agent should be such that\nthis condition remains true at the next time step t+1. In-\ntroducing this constraint into Equation 12, the following\ninequality is obtained:\nvc\nt −Qt pt −C |Qt|pt ≥−(nt + Qt) pt (1 + C)(1 + ϵ)\nTwo cases arise depending on the value of Qt:\nCase of Qt ≥0: The previous expression becomes\nvc\nt −Qt pt −C Qt pt ≥−(nt + Qt) pt (1 + C)(1 + ϵ)\n⇔vc\nt ≥−nt pt (1 + C)(1 + ϵ) −Qt pt ϵ (1 + C)\n⇔Qt ≥−vc\nt −nt pt (1+C)(1+ϵ)\npt ϵ (1+C)\nThe expression on the right side of the inequality repre-\nsents the ﬁrst lower bound for the RL action space A.\nCase of Qt <0: The previous expression becomes\nvc\nt −Qt pt + C Qt pt ≥−(nt + Qt) pt (1 + C)(1 + ϵ)\n⇔vc\nt ≥−nt pt (1 + C)(1 + ϵ) −Qt pt (2C+ ϵ+ ϵC)\n⇔Qt ≥−vc\nt −nt pt (1+C)(1+ϵ)\npt (2C+ϵ(1+C))\nThe expression on the right side of the inequality repre-\nsents the second lower bound for the RL action space A.\nBoth lower bounds previously derived have the same\nnumerator, which is denoted ∆t from now on. This quan-\ntity represents the diﬀerence between the maximum as-\nsumed cost to get back to a neutral position at the next\ntime step t+ 1 and the current cash value of the agent vc\nt.", "tags": []}
{"fragment_id": "F_R18_p19_3", "source_id": "R18", "locator": "2004.06627v3.pdf:p19:3", "text": "The expression tests whether the agent can pay its debt\nin the worst assumed case or not at the next time step, if\nnothing is done at the current time step ( Qt = 0). Two\ncases arise depending on the sign of the quantity ∆ t:\nCase of ∆t <0: The trading agent has no problem paying\nits debt in the situation previously described. This is al-\nways true when the agent owns a positive number of shares\n(nt ≥0). This is also always true when the agent owns\na negative number of shares ( nt <0) and when the price\ndecreases ( pt < pt−1) due to the hypothesis that Equa-\ntion 13 was veriﬁed for time step t. In this case, the most\nconstraining lower bound of the two is the following:\nQt = ∆t\npt (2C+ ϵ(1 + C))\nCase of ∆t ≥0: The trading agent may have problem pay-\ning its debt in the situation previously described. Follow-\ning a similar reasoning than for the previous case, the most\nconstraining lower bound of the two is the following:\nQt = ∆t\npt ϵ(1 + C)\n19", "tags": []}