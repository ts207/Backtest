## 1) What this project is (verified from code structure)

This repository is a **spec-first, run-id–scoped research pipeline** for **crypto perpetuals** that:

1. **Ingests** Binance UM perp market data (OHLCV + funding; optional OI + liquidations; optional spot OHLCV for cross-venue work).
2. **Cleans + aligns** those feeds into a point-in-time “lake” layout (time-partitioned Parquet).
3. **Builds features** (a compact microstructure/state feature set).
4. **Detects event windows** (Phase 1 analyzers per “event family”, e.g., liquidity vacuum, vol shock relaxation).
5. **Normalizes events** into a unified **Event Registry** (events + event_flags per symbol and timestamp).
6. **Generates and executes Phase 2 “candidates”** (rule templates + conditional filters), producing per-candidate expectancy stats and multiplicity control fields (BH/FDR).
7. Applies a **bridge gating** step (cost/tradability heuristics) and a **promotion** step.
8. **Compiles promoted candidates into deterministic Strategy Blueprints** (JSONL), which a **DSL interpreter strategy** can execute.
9. Runs a **cost-aware backtest engine** over blueprints, then builds a **report**.

The single orchestrator that wires “start → finish” is:

* `project/pipelines/run_all.py`

and it writes artifacts under a run id:

* `data/runs/<run_id>/...` (manifests + logs)
* `data/reports/<stage>/<run_id>/...`
* `data/lake/...` and `data/lake/run_scoped/<run_id>/...` (data products)

(`BACKTEST_DATA_ROOT` controls the root; default is `<repo>/data`.)

---

## 2) How the “start → finish” execution is actually wired

### Entry point and stage chain

`project/pipelines/run_all.py` constructs a stage list in order. The early part is:

* `ingest_binance_um_ohlcv_5m.py`
* `ingest_binance_um_funding.py`
* (optional) spot OHLCV, liquidation snapshot, open-interest history
* `clean/build_cleaned_5m.py`
* `features/build_features_v1.py`
* `features/build_market_context.py`
* then an event chain `PHASE2_EVENT_CHAIN` (Phase 1 analyzers per event type)
* then Phase 2 + bridge + promotion + compilation + stress + backtest + report

The “event chain” is explicitly enumerated in `PHASE2_EVENT_CHAIN` inside `run_all.py`.

### Run-id scoping

Every stage receives `--run_id` and writes:

* a **manifest JSON** (per stage) in `data/runs/<run_id>/<stage>.json`
* optionally a stage log in `data/runs/<run_id>/<stage>.log` (only if the script supports `--log_path`)
* main outputs under `data/reports/.../<run_id>/...` and/or `data/lake/run_scoped/<run_id>/...`

The manifest helper is in:

* `project/pipelines/_lib/run_manifest.py`

---

## 3) Data model: the “lake”, “reports”, and “registry” contracts

### Lake layout (what the engine and research stages consume)

The core lake areas (as implemented by scripts and `io_utils.py`) are:

* **Raw** (from ingest): `data/lake/raw/...`
* **Cleaned bars**: `data/lake/cleaned/perp/<symbol>/<timeframe>/bars_<timeframe>/...`
* **Features**: `data/lake/features/perp/<symbol>/5m/features_v1/...`
* **Market context**: `data/lake/context/market/<symbol>/5m/market_context_v1/...`
* **Event registry outputs**:

  * `data/events/<run_id>/events.parquet`
  * `data/events/<run_id>/event_flags/<symbol>.parquet`

The engine loads cleaned bars using a path pattern that depends on timeframe:

* `project/engine/runner.py` loads from `.../cleaned/perp/<symbol>/<timeframe>/bars_<timeframe>/...`

### Reports layout (what promotion/compile consumes)

Key “reports” products used later:

* Phase 2 candidates: `data/reports/phase2/<run_id>/phase2_candidates.csv`
* Bridge-scored candidates: `data/reports/bridge_eval/<run_id>/phase2_candidates_bridge_scored.csv`
* Edge candidates normalized: `data/reports/edge_candidates/<run_id>/edge_candidates_normalized.csv`
* Promotions report: `data/reports/promotions/<run_id>/promotion_report.json`
* Blueprints: `data/reports/strategy_blueprints/<run_id>/blueprints.jsonl`
* Backtest outputs (strategy returns, metrics, traces) under `data/reports/backtest/...` (written by `backtest_strategies.py`)
* Final report: produced by `project/pipelines/report/make_report.py`

---

## 4) Core objects and the “logic spine” end-to-end

### A) Ingest (Binance UM)

Examples:

* `project/pipelines/ingest/ingest_binance_um_ohlcv_5m.py`
* `project/pipelines/ingest/ingest_binance_um_funding.py`

Ingest pulls monthly/daily archives (binance.vision style) and writes per-symbol, time-partitioned Parquet.

**Important**: Ingest scripts include their own completeness checks and timestamp parsing; later cleaning assumes UTC timestamps and monotonic time.

### B) Clean + Align (perp bars + funding + OI + liquidations)

* `project/pipelines/clean/build_cleaned_5m.py`

This stage:

* builds a full 5-minute index for the requested time range,
* reindexes OHLCV into a contiguous grid,
* aligns funding to bars (merge_asof backward),
* optionally aligns OI/liquidation snapshots,
* writes cleaned bars and metadata.

### C) Features (v1)

* `project/pipelines/features/build_features_v1.py`

This builds a compact feature set including:

* returns (`logret_1`)
* realized volatility (`rv_96`) and long-window percentile (`rv_pct_17280`)
* rolling high/low (`high_96`, `low_96`) and derived range (`range_96`)
* basis and liquidation/OI deltas if present
* funding features

It validates required columns against:

* `project/schemas/feature_schema_v1.json` via `validate_feature_schema_columns(...)` in `run_manifest.py`

### D) Market context

* `project/pipelines/features/build_market_context.py`

This is intended to build higher-level context states such as:

* `vol_regime`
* `compression_state`
* `funding_regime`
* plus trend measures

This context is referenced by candidate planning (Phase 2) when conditions require state variables like `vol_regime`.

### E) Phase 1 events (per event family analyzers)

Examples:

* `project/pipelines/research/analyze_liquidity_vacuum.py`
* `project/pipelines/research/analyze_vol_shock_relaxation.py`
* etc.

Each analyzer:

* loads features/bars,
* detects event windows,
* writes an event CSV + summary JSON under `data/reports/<event_type>/<run_id>/...`

### F) Event Registry normalization

* `project/events/registry.py`

This unifies phase-1 event outputs into:

* `events.parquet` (canonical event rows)
* `event_flags/<symbol>.parquet` (timestamp-aligned boolean/int flags per event type)

The DSL strategy and/or Phase 2 candidate builders rely on these flags to know “event fired at t”.

### G) Candidate planning (Atlas loop)

* `project/pipelines/research/generate_candidate_templates.py`
* `project/pipelines/research/generate_candidate_plan.py`
* `project/pipelines/research/verify_atlas_claims.py`

This layer exists to prevent “LLM-ish idea generation” from scheduling non-executable claims:

* it produces candidate templates and a plan JSONL,
* it records blocked conditions (excluded claims) which `generate_candidate_plan.py` consults on subsequent runs.

### H) Phase 2 candidate discovery (expectancy + multiplicity + gating)

* `project/pipelines/research/phase2_candidate_discovery.py`

This is where “research hypotheses” become rows with:

* expectancy per trade
* t-stats / p-values (normal-approx in `analyze_conditional_expectancy.py`; Phase 2 has its own stats path)
* BH-adjusted q-values
* a “final gate” boolean (`gate_phase2_final`), plus additional structural gates

### I) Bridge gating (tradability proxy)

* `project/pipelines/research/bridge_evaluate_phase2.py`

This adds columns like:

* `cost_ratio`
* `after_cost_expectancy_per_trade`
* `stressed_after_cost_expectancy_per_trade`
* `gate_bridge_tradable`

This is a *heuristic tradability filter* based on expectancy vs cost and turnover proxies (not a full microstructure sim).

### J) Promotion and checklist gating

* `project/pipelines/research/promote_blueprints.py`
* `project/pipelines/research/generate_recommendations_checklist.py`

Checklist output:

* `data/runs/<run_id>/research_checklist/checklist.json`
  and `compile_strategy_blueprints.py` will refuse to compile unless the decision is `PROMOTE` (unless overridden).

### K) Blueprint compilation

* `project/pipelines/research/compile_strategy_blueprints.py`

This turns candidate rows into deterministic **Blueprint** objects, including:

* entry condition normalized into condition nodes (via `project/strategy_dsl/contract_v1.py`)
* stop/target/time stops derived from policy
* overlays and confirmations
* lineage metadata, selection logs, evidence hashes

Outputs:

* `data/reports/strategy_blueprints/<run_id>/blueprints.jsonl`

### L) Stress test (parameter perturbation)

* `project/pipelines/research/stress_test_blueprints.py`

Builds perturbed blueprint variants (±10% style), runs a quick engine pass, and checks that expectancy remains acceptable under stress.

### M) Backtest engine

Main execution is in:

* `project/engine/runner.py`
* `project/engine/pnl.py`
* `project/engine/execution_model.py`
* `project/engine/risk_allocator.py`
* strategy adapters including `project/strategies/dsl_interpreter_v1.py`

The DSL interpreter:

* reads `blueprints.jsonl`
* loads event flags from the registry
* constructs eligibility masks from triggers + condition nodes
* emits positions and optional overlay actions

The engine:

* aligns bars + features + context,
* applies execution cost model (spread/impact/fees),
* computes PnL components (gross, trading_cost, funding_pnl, borrow_cost),
* outputs per-strategy time series plus diagnostics/traces.

### N) Final report

* `project/pipelines/report/make_report.py`

Aggregates blueprint metadata, engine outputs, and produces a markdown summary report.

---

## 5) Structural correctness issues found by direct inspection (these are not “opinions”; they are concrete breakpoints or contradictions)

### (1) Import/package layout is inconsistent (“project.” prefix is not supported by current sys.path pattern)

Most pipeline scripts do:

* `PROJECT_ROOT = .../project` and `sys.path.insert(0, str(PROJECT_ROOT))`

That makes imports like `from pipelines._lib...` work (modules are “top-level” inside `project/`).

But at least one critical script uses `from project.features...` which will fail because there is no `project/` package under that sys.path:

* `project/pipelines/features/build_market_context.py` imports `project.features...` (will raise `ModuleNotFoundError` in the default execution setup).

**Implication**: the market context stage is currently broken unless the environment differs from what the scripts set up.

### (2) `run_walkforward.py` has a hard IndentationError (walkforward eval cannot run)

`python -m compileall project` reports:

* `project/pipelines/eval/run_walkforward.py` IndentationError around line ~1018.

`run_all.py` can execute walkforward if `--run_walkforward_eval 1`, so enabling it will fail.

### (3) Duplicate “runner.py” exists, one copy contains a SyntaxError

`python -m compileall` also reports:

* `project/pipelines/engine/runner.py` SyntaxError (a stray escaped string key `\"dsl\"`).

This file is **not** the engine used by `backtest_strategies.py` (that uses `project/engine/runner.py`), but it increases maintenance risk and will break any pipeline path that imports it.

### (4) Timeframe is internally inconsistent (5m vs 15m residue)

Observed facts:

* Ingest and clean stages are named and implemented as **5m** (e.g., `ingest_binance_um_ohlcv_5m.py`, `build_cleaned_5m.py`).
* Engine default timeframe constant is **5m** (`project/engine/runner.py`).
* Schema key name is **features_v1_15m_v1** while the features are stored under `.../5m/features_v1/` (`project/schemas/feature_schema_v1.json` + `build_features_v1.py`).

This won’t necessarily crash, but it causes confusion and increases the chance of silent mismatches (especially if someone switches engine timeframe expecting 15m availability).

### (5) Funding-per-bar scaling in cleaning is almost certainly wrong for 5m bars

In `build_cleaned_5m.py`, funding is aligned to bars and then scaled by:

* `bars_per_event = int((interval_hours * 60) / 1)` → 480 for 8 hours

But 8 hours of **5m** bars is 96, not 480.

**Implication**: if `funding_rate_scaled` is meant to be “per-bar funding contribution”, this under-allocates funding impact by ~5×.

### (6) Ingest completeness expectation for 5m OHLCV uses a 15-minute interval

`ingest_binance_um_ohlcv_5m.py` computes expected bars using:

* `bar_seconds = 15 * 60`

even though it ingests 5m bars.

**Implication**: completeness checks can incorrectly accept partial months/days as “complete enough”.

### (7) Phase 2 → blueprint compile has a known logical contradiction around fallback quality

`compile_strategy_blueprints.py` uses quality floors:

* strict/fallback floors from `spec/gates.yaml` (`quality_floor_fallback` is 0.66)

But `phase2_candidate_discovery.py` assigns:

* `robustness_score = 1.0 if gate_phase2_final else 0.5` (observed in code)

So “fallback” candidates that fail the final (BH/FDR) gate are structurally unable to meet a 0.66 floor if robustness is capped at 0.5.

**Implication**: fallback compilation can become impossible or nearly impossible unless other fields override the quality score, which defeats the intent of having an exploratory fallback track.

---

## 6) The fastest way to “fully understand” it (operational reading order)

If the goal is deep understanding with minimal thrash, the shortest path is:

1. **Orchestration contract**

   * `project/pipelines/run_all.py`
   * focus: stage order, inputs/flags, where artifacts land

2. **Data contracts + manifesting**

   * `project/pipelines/_lib/io_utils.py`
   * `project/pipelines/_lib/run_manifest.py`
   * `project/schemas/feature_schema_v1.json`

3. **Clean → Features → Context**

   * `project/pipelines/clean/build_cleaned_5m.py`
   * `project/pipelines/features/build_features_v1.py`
   * `project/pipelines/features/build_market_context.py`

4. **Event pipeline**

   * one analyzer (e.g. `analyze_liquidity_vacuum.py`) to see the pattern
   * `project/events/registry.py` (this is the “join point” for all event families)

5. **Phase 2 discovery + bridge**

   * `project/pipelines/research/phase2_candidate_discovery.py`
   * `project/pipelines/research/bridge_evaluate_phase2.py`

6. **Promotion + compile**

   * `project/pipelines/research/promote_blueprints.py`
   * `project/pipelines/research/compile_strategy_blueprints.py`
   * `project/strategy_dsl/contract_v1.py`

7. **Execution**

   * `project/strategies/dsl_interpreter_v1.py`
   * `project/engine/runner.py`
   * `project/engine/execution_model.py`
   * `project/engine/pnl.py`

8. **Reporting**

   * `project/pipelines/report/make_report.py`

This order matches the actual dependency chain and prevents “reading dead code first”.

---

## 7) Next logical step (to convert understanding into a working, inspectable run)

Before any deeper strategy research, make the pipeline *runnable end-to-end* on a tiny slice (1–2 symbols, 2–4 weeks):

1. Fix the **hard failures**:

   * `build_market_context.py` imports (`project.features` → `features`)
   * `run_walkforward.py` IndentationError (if you intend to use walkforward)
   * remove or fix `project/pipelines/engine/runner.py` SyntaxError (or delete if legacy)
2. Fix the two **silent data integrity issues**:

   * 5m OHLCV expected bars uses 15m
   * funding scaling uses 1m divisor instead of 5m
