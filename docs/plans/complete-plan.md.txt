## Complete plan: use the Knowledge Atlas + the Framework to guide end-to-end strategy + portfolio development

This plan assumes you start with:

* `knowledge_atlas.json`, `fragments.jsonl`, `paper_cards.json`, `atlas_links.json`
* Your target configuration (C3): perps + spot basis + top-of-book features
* Goal: produce a small set of robust, tradable strategies and a portfolio, with gates and tests

---

# A) Operating model

## A1) The role of the atlas

The atlas is not “reference material”; it is a **work queue generator** and a **specification source**.

You will use it to:

1. Generate a backlog of operational candidates (event/feature/label/execution hypotheses)
2. Generate executable specs (YAML + code) from claims + evidence fragments
3. Track contradictions as mandatory experiments
4. Maintain provenance (what claim led to what test/result)

## A2) The role of the framework

The framework is your **state machine**:

* Discovery → Validation → Bridge → Blueprint → Strategy → Walkforward → Portfolio
  Each stage has:
* required inputs
* required outputs
* fail-closed gates

---

# B) Artifacts and repositories (what you will produce)

## B1) Specs (human-readable + versioned)

* `event_registry.yaml` (versioned event definitions)
* `feature_specs.yaml` (PIT feature definitions)
* `label_specs.yaml` (forward label definitions)
* `cost_model.yaml` (fee/slippage/impact defaults + sweeps)
* `candidate_family.yaml` (how candidates are generated + multiplicity family)
* `gates.yaml` (thresholds for V1/Bridge/Walkforward/Portfolio)

## B2) Code modules (executable)

* `ingest_*` (perp bars, spot bars, funding, mark/index, OI, ToB)
* `build_clean_*` (alignment + QA)
* `build_features_*`
* `build_labels_*`
* `generate_events_*`
* `phase2_validate_*` (event studies + slices + FDR + cost/capacity sweeps)
* `bridge_sim_*` (executed-like)
* `backtest_*`, `walkforward_*`
* `portfolio_*` (allocation + execution simulation)

## B3) Outputs (machine-readable results)

* `events.parquet`
* `features_pt.parquet`
* `labels_fwd.parquet`
* `phase2_candidates.csv` (ranked with q-values + stability + after-cost)
* `bridge_results.csv` (executed score + fail reasons)
* `blueprints.jsonl`
* `backtest_returns.parquet`
* `walkforward_summary.json`
* `portfolio_report.json`

## B4) Research backlog (atlas-driven)

* `research_backlog.csv` with one row per “operational candidate”
* `tie_break_tests/` for contradictions

---

# C) One-time setup: convert atlas into a research backlog

## C1) Backlog schema (required)

Each row:

* `paper_card_id`
* `claim_id`
* `concept_id`
* `candidate_type`: `event | feature | label | evaluation | execution`
* `operationalizable`: `Y/N`
* `missing`: `{data,label,split,costs,capacity,PIT,definition}`
* `next_artifact`: `{event_registry, feature_specs, label_specs, cost_model, tests}`
* `priority`: (Impact × Feasibility × Novelty) as 1–5 each
* `target_gate`: `D0 | V1 | Bridge | S | P`
* `evidence_locator`: from fragments (page/locator)
* `notes`

## C2) How to fill it fast (repeatable)

For each `concept_id` you target:

1. Use `atlas_links.concept_id_to_paper_card_ids[concept_id]`
2. For each `paper_card_id`, take its `key_claims` (policy A: 25)
3. For each `claim_id`:

   * classify as event/feature/label/eval/execution
   * check if it contains (or can be mapped to) A–E below

### The operationalization checklist (A–E)

A claim is operationalizable only if you can define:
A) **Event trigger** (or feature definition) at t₀
B) **Feature inputs** and PIT windowing
C) **Label/horizon**
D) **Test protocol** (slices + multiplicity family)
E) **Cost/capacity model** (or at least a default sweep)

If not, mark `missing = {...}`.

## C3) Contradictions become mandatory tests

For each `contradiction_id`:

* create a backlog row with `candidate_type=tie_break`
* the output is a tie-break experiment spec:

  * what data slice
  * what metric
  * what result supports each side

---

# D) Build plan: vertical slices with gates (end-to-end)

You build in slices; each slice ends in a runnable artifact and a gate decision.

## Slice 1 — Data layer (must be correct before research)

**Goal:** build stable, replayable datasets.

### Required datasets

* Perp: 1m bars, funding, mark/index, OI
* Spot: 1m bars (for basis)
* ToB snapshots: bid/ask px+size at 1s (2–5s acceptable initially)

### Deliverables

* `bars_1m_perp`, `bars_1m_spot`
* `tob_snapshots` + `tob_1m_agg` (spread_bps, depth_usd, imbalance stats)
* `basis_state` (perp_mid vs spot_mid)
* QA report: gaps, missingness, outliers, timestamp monotonicity

### Gate D0 (data integrity)

Fail if:

* time alignment inconsistent
* missingness above threshold (choose a strict default)
* ToB aggregation does not match bar timestamps deterministically

---

## Slice 2 — PIT features + forward labels

**Goal:** define the state vector and outcomes.

### Features (C3 core)

Microstructure:

* spread level/percentiles, spread changes
* depth level/percentiles, depth collapse flags
* imbalance statistics
* liquidity stress composite

Derivatives:

* funding level/z-score/persistence
* basis level/z-score/mean-reversion proxy
* mark-index anomalies
* OI deltas

Vol regime:

* realized vol / ATR percentiles
* compression/expansion flags

### Labels

* fwd returns at: 5m, 15m, 1h, 4h, 24h
* MAE/MFE at the same horizons

### Gate PIT-1 (leakage)

Fail if any feature uses data with timestamp > t₀.

---

## Slice 3 — Event registry and event generation

**Goal:** produce a versioned `events` table.

### Event families (C3 priority)

* Liquidity stress (spread widen, depth collapse, stress spikes)
* Shock proxies (wick/vol shock + liquidity stress)
* Funding/basis (extremes, persistence, snapback, funding window)
* OI shocks (spike/drop conditioned on returns)

### Gate E-1 (event quality)

Fail if:

* events are degenerate (too rare, too frequent, or mostly duplicates)
* events do not join cleanly to features/labels

---

## Slice 4 — Validation (Phase2): candidate generation + testing + multiplicity

**Goal:** transform events into candidate edges with stability and after-cost results.

### Candidate generation (standard form)

Candidate = event_type × context bucket × horizon × rule template

Rule templates (minimal set):

* directional: long/short conditional on context
* mean reversion: fade extension post-stress
* carry/basis: conditional hold during funding regime
* breakout: compression → expansion conditional

### Tests per candidate

* event study forward return curve
* slice stability:

  * time slices
  * vol regime slices
  * liquidity regime slices
  * cross-symbol slices
* cost sweeps:

  * fee sweep
  * slippage/impact sweep
  * stress multipliers when liquidity stress active
* capacity sweeps:

  * participation cap vs 1m volume

### Multiplicity control

* BH-FDR across the candidate family (define family explicitly)

### Gate V1 (validation)

Pass only if:

* after-cost effect positive under conservative cost sweep
* sign stable across key slices
* q-value below threshold (choose default q=0.1 initially, tighten later)

---

## Slice 5 — Bridge: executed-like tradability

**Goal:** eliminate “paper alpha”.

### Bridge simulator requirements

* fill at bid/ask with slippage
* latency model (1–5s)
* cost sweep applied
* fill degradation under stress regimes
* turnover and capacity penalty

### Gate B (bridge)

Pass only if:

* executed-like expectancy positive under conservative assumptions
* turnover acceptable
* capacity not violated

---

## Slice 6 — Strategy backtests + sweeps + stress

**Goal:** compile survivors into strategies and test robustness.

### Strategy build

* blueprint per survivor (executable spec)
* parameter neighborhood sweeps
* stress tests:

  * higher costs
  * worse liquidity
  * delayed fills
  * missing data edges

### Gate S (strategy robustness)

Pass only if:

* OOS performance is stable (walkforward)
* no razor-thin parameter optimum
* drawdown/tail risk acceptable
* stress sweeps do not kill expectancy

---

## Slice 7 — Portfolio construction + portfolio execution simulation

**Goal:** build a portfolio that remains tradable under combined impact.

### Portfolio steps

* cluster by event family + horizon + correlation
* allocate with caps and turnover penalties
* simulate portfolio execution shortfall under participation limits

### Gate P (portfolio viability)

Pass only if:

* portfolio after-cost+impact remains positive
* diversification holds under stress
* combined capacity constraints satisfied

---

# E) Work splitting (parallel tracks)

## Track A — Data engineering

* ingest + QA + ToB aggregation + basis construction

## Track B — Research primitives

* features, labels, event registry

## Track C — Validation + statistics

* event studies, slice framework, FDR, reporting

## Track D — Execution realism

* cost model + bridge sim + portfolio execution sim

## Track E — Test/determinism

* PIT tests, schema tests, manifest/provenance tests, regression harness

Interfaces between tracks are the parquet tables + spec YAMLs.

---

# F) Coding agent usage: the exact workflow

## F1) Ticketing (agent tasks must be closed and testable)

Every ticket includes:

* Objective
* Inputs (tables + spec files)
* Outputs (exact files)
* Tests to add
* Commands to run
* DoD (assertions + artifacts produced)

## F2) Agent cadence (recommended)

* 1 ticket = 1 module = 1 PR/commit set
* Always include:

  * unit tests for transforms
  * a small smoke run (2 symbols × 7 days)
  * manifest output (hashes)

## F3) What agents should not do

* no cross-cutting refactors
* no schema changes without migration note and updated tests
* no silent fallbacks (fail closed)

---

# G) Recommended timeline (realistic sequence)

## Week 1

* Build Slice 1 (data) + QA + determinism hooks

## Week 2

* Slice 2 (features/labels) + PIT tests

## Week 3

* Slice 3 (events) + event quality gate

## Week 4

* Slice 4 (validation) + FDR + cost/capacity sweeps → first candidate list

## Week 5

* Slice 5 (bridge) → tradable survivors

## Week 6–7

* Slice 6 (strategy + walkforward) → robust strategies

## Week 8

* Slice 7 (portfolio) → portfolio-level tradability

This can compress if you reduce scope (fewer event types, fewer symbols, fewer horizons).

---

# H) How the atlas guides daily work (the practical loop)

Daily loop:

1. Pick one `concept_id`
2. Pull `paper_card_id`s via `atlas_links`
3. Convert 10–20 `key_claims` into backlog rows
4. Choose top 1–3 operational backlog rows
5. Produce one executable artifact per row:

   * event definition OR feature set OR test spec
6. Run the next pipeline gate impacted (validation/bridge/etc.)
7. Store result back as status on the backlog row

Weekly loop:

* add event families and rerun Phase2 + bridge
* promote survivors; archive falsified claims (do not keep retesting)

---

# I) Next action (concrete)

To make this fully operational, you start with **three files**:

1. `research_backlog.csv` (generated from atlas)
2. `event_registry.yaml` v1 (15–30 events)
3. `gates.yaml` (D0, PIT-1, E-1, V1, B, S, P thresholds)

If you want, specify:

* ToB interval: 1s or 5s
* initial universe size: 5 / 10 / 20
* horizons: keep the full set or restrict to (5m, 1h, 24h) initially

and the plan can be reduced to an “MVP path” that reaches a first bridge-pass strategy set faster.
Use the knowledge atlas as a **control plane** for the framework: it should generate (1) your research backlog, (2) your executable specs (events/features/labels/tests), and (3) your promotion decisions. The framework tells you *what stages exist and what gates must pass*; the atlas tells you *what to test, how to define it, and what evidence supports it*.

## 1) Map atlas objects to framework phases

### Discovery (Phase 1)

**Atlas inputs**

* `concepts[]` → research domains (microstructure, efficiency, ML models, funding/basis, vol regimes)
* `claims[]` (especially mechanistic/heuristic) → candidate event definitions + context deltas
* `fragments.jsonl` → exact definitions, caveats, data requirements

**Framework outputs**

* `event_registry.yaml`
* `features_pt` spec
* `invariants` (sanity checks)

### Validation (Phase 2)

**Atlas inputs**

* `claims[].operationalization` (features/threshold hints)
* `contradictions[]` → “tie-break tests” you must run

**Framework outputs**

* event studies, slice stability tables
* after-cost expectancy + capacity sweeps
* multiplicity control (FDR / reality-check)

### Bridge (tradability)

**Atlas inputs**

* claims mentioning costs, execution, microstructure frictions
* microstructure concepts/claims to parameterize slippage/impact stress

**Framework outputs**

* executed-like PnL and failure reasons (turnover, stress degradation)

### Strategy / Walkforward

**Atlas inputs**

* ML-related claims become blueprint options (labels, splits, metrics)
* market-efficiency claims become regime conditioning hypotheses

**Framework outputs**

* blueprint specs
* backtests + sweeps + stress
* walkforward pass/fail evidence

### Portfolio

**Atlas inputs**

* claims about correlation, regime dependence, liquidity constraints
* contradictions about evaluation methodology become portfolio-level robustness tests

**Framework outputs**

* allocation + correlation control
* portfolio execution simulation

---

## 2) Convert atlas → an actionable backlog (the key step)

Create a “Research Backlog” table where each row is **one operational candidate**:

**Row schema (recommended)**

* `paper_card_id`
* `claim_id`
* `concept_id`
* `candidate_type`: {event, feature, label, evaluation, execution}
* `operationalizable`: Y/N
* `missing`: {data, label, split, costs, capacity, PIT}
* `next_artifact`: {event_registry entry, feature spec, label spec, test spec}
* `priority`: impact × feasibility × novelty
* `gate_target`: {D0, V1, Bridge, S, P}

How to fill it quickly:

1. Start from `paper_cards.json` (one paper at a time)
2. Take the `key_claims` (top 25, policy A)
3. For each claim, mark whether it can be translated into:

   * event trigger definition, or
   * feature definition, or
   * label/evaluation requirement, or
   * execution/cost requirement
4. If missing pieces exist, link them to required data and add a “missing” tag.

Result: you stop “reading papers”; you produce a queue of buildable units.

---

## 3) The “Operationalization” recipe (claim → executable spec)

For any claim you intend to use, force it into this canonical form:

### (A) Event definition

* `event_type`
* `trigger`: exact boolean condition at time t₀
* `cooldown`: bars/time
* `required_fields`: which tables/columns
* `metadata`: severity, regime tags

### (B) Context delta / features

* `features_pt[]`: each with lookback window, aggregation, normalization
* “PIT proof”: all inputs require `ts ≤ t₀`

### (C) Label definition

* horizon H, label type (return, direction, MAE/MFE)
* explicit formula and sampling rule

### (D) Test definition

* event study horizons
* slices (time/regime/symbol)
* multiplicity family definition (what set is corrected together)

### (E) Execution/cost model

* fees + slippage + impact proxy
* stress multipliers under liquidity stress
* participation caps / capacity assumptions

If any part (A–E) cannot be specified, the claim remains **non-operational** and stays in backlog until resolved.

---

## 4) Use contradictions as mandatory experiments (prevents narrative drift)

From `knowledge_atlas.json`:

* Each `contradiction_id` becomes a **tie-break test** ticket.
* You do not “decide” which claim is correct; you run the discriminating test.

Implementation pattern:

* Create `tests/tie_break/T_<contradiction_id>.yaml` with:

  * scope restrictions (venue/timeframe/horizon)
  * metrics and pass criteria
  * expected outcomes that would support either side

This keeps the system from quietly absorbing incompatible assumptions.

---

## 5) Recommended weekly operating cadence (high throughput, low chaos)

### Weekly cycle (repeat)

1. **Select 1–2 concepts** to focus (e.g., microstructure metrics + funding/basis)
2. **Pull papers via atlas_links**:

   * concept → paper_cards
3. **Backlog expansion**:

   * add 10–30 new candidate rows from key_claims
4. **Build 3–5 operational specs**:

   * update event_registry/features/labels/tests
5. **Run Phase2 validation**:

   * event studies + cost/capacity sweeps + FDR
6. **Bridge evaluate survivors**
7. If survivors exist: blueprint/backtest/walkforward; else: adjust hypotheses or add data features.

Key rule: Every week ends with either:

* (a) survivors promoted to blueprints, or
* (b) explicit falsification notes saved to atlas (claims marked “unsupported in your environment”).

---

## 6) How to use a coding agent with the atlas (most effective pattern)

Give the agent **bounded build tickets** generated directly from the backlog row(s).

### Ticket template (use exactly)

* **Goal:** Implement candidate X as executable artifact(s)
* **Inputs:** claim_id(s), evidence locator(s), required tables
* **Outputs:** specific files (event_registry entry, feature function, label function, tests)
* **Constraints:** PIT requirements; no schema drift; deterministic outputs; must add unit tests
* **DoD:** tests pass + smoke run produces expected artifact files

The atlas provides the references:

* agent reads `paper_cards.json` → selects claim_ids
* agent uses `fragments.jsonl` locators to copy exact definitions into code comments/spec fields
* agent updates the YAML/spec and adds tests

---

## 7) Minimal “guide” you should run next

Pick one concept to start (recommended: `C_MICROSTRUCTURE_METRICS` or `C_MARKET_EFFICIENCY`), then do:

1. `atlas_links.json`: concept → paper_card_ids
2. open those cards: collect their `key_claims`
3. create 20-row backlog
4. operationalize 3 rows into:

   * 1 event definition
   * 1 feature set
   * 1 validation test suite (slices + costs + FDR)

That becomes your first repeatable loop.

---

## Next step

Choose one starting concept (one of these is enough):

* `C_MICROSTRUCTURE_METRICS`
* `C_MARKET_EFFICIENCY`
* `C_ML_TRADING_MODELS`

If you pick one, I will produce a concrete “Week 1 plan” in terms of: which paper cards to pull, which claim_ids to operationalize first, and the exact artifact list (event_registry entries + feature/label specs + tests).
## Plan from scratch (C3): what you need, how to split work, how to use coding agents effectively

### Guiding principle

Build in **vertical slices** that end in a measurable artifact (dataset → events → validation → bridge → blueprint → backtest → walkforward → portfolio sim). Do not build “infrastructure first” beyond what each slice needs.

---

## 1) What you need (minimum viable stack)

### Data + storage

* **Binance USDT-M perps**: 1m OHLCV, funding schedule+rates, mark/index, open interest.
* **Binance spot**: 1m OHLCV (for basis).
* **Top-of-book (ToB)** snapshots: bid/ask price+size at **1s** (2–5s acceptable to start).
* Storage:

  * **Raw**: parquet (partitioned by venue/symbol/date).
  * **Curated**: parquet + manifest hashes.
  * Optional: SQLite for metadata/manifests; parquet for time series.

### Compute

* A single workstation is fine initially.
* One reproducible environment:

  * Python, uv/poetry, pinned deps
  * deterministic runs (seed, config hash)

### Core code modules (must exist)

1. `ingest/` (download + normalize + partition)
2. `clean/` (align timestamps, resample, QA)
3. `features/` (PIT feature generation)
4. `labels/` (forward returns + MAE/MFE)
5. `events/` (event registry + detection)
6. `phase2_validate/` (event studies + slices + FDR + cost/capacity sweeps)
7. `bridge/` (executed-like simulation)
8. `strategy/` (blueprints + backtest + sweeps + stress)
9. `walkforward/`
10. `portfolio/` (allocation + portfolio execution sim)
11. `tests/` (PIT, schema, invariants, regression tests)
12. `run/` (CLI to run stages + manifests)

---

## 2) Split work into phases with “done” criteria (recommended)

### Phase 0 — Project skeleton + invariants (1–2 days)

**Goal:** enforce correctness early.

Deliverables:

* repo layout + CLI runner + config system
* manifest writer (dataset hash, code hash, config hash)
* PIT guard utilities (time window helpers)
* schema definitions (pydantic/dataclasses)

Done criteria:

* `run ingest_smoke` produces a manifest and a tiny dataset sample.

### Phase 1 — Data layer (vertical slice #1)

**Goal:** produce reliable `bars_1m`, `tob_1s`, `funding`, `oi`, `spot_1m`, `basis_state`.

Deliverables:

* ingestion scripts (perp bars, spot bars, funding, mark/index, OI)
* ToB collector/loader (either websocket capture or historical source you already have)
* aligners:

  * ToB → 1m aggregates (spread_bps, depth_usd, imbalance stats)
  * spot/perp sync and basis_bps

Done criteria:

* For 2 symbols and 7 days you can build:

  * `bars_1m`, `tob_1m_agg`, `funding`, `oi`, `basis_state`
  * QA report: missingness, timestamp continuity, outliers.

### Phase 2 — Features + labels (vertical slice #2)

**Goal:** generate PIT `features_pt` and forward `labels_fwd`.

Deliverables:

* feature families:

  * microstructure: spread/depth/imbalance/stress composite
  * derivatives: funding/basis/mark-index/OI deltas
  * vol regime: realized vol / ATR percentiles
* labels:

  * fwd returns at 5m/15m/1h/4h/24h
  * MAE/MFE

Done criteria:

* PIT tests pass (no forward leakage).
* labels match forward price moves precisely (spot-check).

### Phase 3 — Event registry + event generation (vertical slice #3)

**Goal:** build `events` table from versioned YAML.

Deliverables:

* `event_registry.yaml` with 15–30 event types (C3 priority: liquidity stress, funding/basis, vol transitions, OI shocks)
* detector producing `events` with cooldown/dedup and metadata

Done criteria:

* Each event type has prevalence stats and is not degenerate.
* Events join cleanly to `features_pt` and `labels_fwd` at t₀.

### Phase 4 — Phase2 validation + multiplicity + costs (vertical slice #4)

**Goal:** turn events into candidate edges and quantify after-cost stability.

Deliverables:

* candidate generator: (event × context bucket × horizon × rule template)
* event studies:

  * forward return curves
  * slice stability (time/regime/symbol)
* FDR (BH) correction across candidate families
* cost + capacity sweeps (fees/slippage/impact + participation cap)

Done criteria:

* outputs a ranked candidate list with:

  * after-cost expectancy
  * stability score
  * q-values
  * capacity flags

### Phase 5 — Bridge (executed-like) (vertical slice #5)

**Goal:** remove “paper alpha” before strategy building.

Deliverables:

* minimal execution simulator (bid/ask fills + slippage)
* latency model (1–5s)
* executed PnL stats + turnover + degradation in stress regimes

Done criteria:

* produces “bridge_pass” set with conservative assumptions.

### Phase 6 — Strategy + walkforward + portfolio (final)

**Goal:** production-grade selection.

Deliverables:

* blueprint compiler (YAML specs)
* backtest engine (single strategy)
* parameter sweeps + stress tests
* walkforward runner
* portfolio allocator + portfolio execution sim

Done criteria:

* small portfolio of strategies survives walkforward and portfolio sim under costs/capacity.

---

## 3) How to split work (parallelization)

You can parallelize by “interfaces”:

### Track A — Data engineering

* ingest + QA + ToB aggregation
* ensures stable tables + hashes

### Track B — Research logic

* features + labels + event registry
* validation metrics + slicing + FDR

### Track C — Execution + simulation

* cost model
* bridge simulator
* portfolio execution sim

### Track D — Orchestration + testing

* runner, manifests, config, unit tests, PIT tests, regression tests

Rule: each track must deliver artifacts matching schemas so others can progress without waiting.

---

## 4) Using a coding agent effectively (high leverage pattern)

### 4.1 Give the agent “closed tasks”

Agents perform best when each task has:

* scope boundaries
* exact file targets
* expected CLI/test commands
* acceptance criteria

**Template you should use:**

* Objective (1–2 lines)
* Files to edit/create (explicit paths)
* Inputs/outputs (schemas)
* Tests to add/update
* Commands to run
* Definition of done

### 4.2 Agent workflow (recommended)

1. **You** write spec + acceptance tests first (even minimal).
2. **Agent** implements to satisfy tests.
3. **You** review diffs + run tests.
4. Lock into manifests; never “patch by conversation” without commits.

### 4.3 Always force the agent into small commits

* One PR per module per slice:

  * `tob_aggregator`
  * `features_microstructure`
  * `event_registry_v1`
  * `phase2_fdr`
  * `bridge_sim_v1`

### 4.4 Guardrails that prevent agent drift

* No refactors unless required.
* No new dependencies unless justified in the task spec.
* Must write unit tests for every transformation step.
* Must not change schemas without migration notes.

---

## 5) Recommended day-to-day workflow (high throughput)

### Daily loop

1. Pick **one vertical slice milestone** for the day (e.g., “ToB 1m aggregation”).
2. Write a **spec** + **tests** for it.
3. Run agent to implement.
4. Validate with:

   * tests
   * 1–2 “golden” data spot-check notebooks
5. Produce an artifact:

   * parquet snapshot + manifest
   * report JSON/HTML
6. Only then move to the next slice.

### Weekly loop

* Add 2–3 new event types and re-run Phase2.
* Promote/demote candidates based on bridge + walkforward.
* Expand universe by 2–5 symbols only after QA.

---

## 6) Concrete “starting from scratch” work plan (first 2 weeks)

### Week 1: data + PIT correctness

* Day 1–2: skeleton + runner + manifests + QA scaffolding
* Day 3–5: ingest perps/spot/funding/OI + ToB ingestion
* Day 6–7: ToB → 1m aggregates + basis_state + QA report

### Week 2: features/labels/events + first validation run

* Day 8–9: features_pt + labels_fwd + PIT tests
* Day 10–11: event_registry.yaml v1 + events generator
* Day 12–14: phase2 validation (event studies + cost sweep + FDR) + first ranked candidates

Bridge + backtests start in week 3.

---

## 7) Next step (to make this operational immediately)

Answer these three parameters (one line each), and I’ll produce the exact task list (slice-by-slice) plus the first 5 agent tickets:

1. ToB sampling you will use now: **1s** or **5s**
2. Storage choice: **parquet-only** or **parquet + sqlite manifests**
3. Initial symbol count: **5**, **10**, or **20**
